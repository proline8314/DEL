nohup: ignoring input
usage: test_refnet_transfer.py [-h] [--name NAME] [--seed SEED]
                               [--device DEVICE] [--batch_size BATCH_SIZE]
                               [--epochs EPOCHS] [--lr LR]
                               [--log_interval LOG_INTERVAL]
                               [--save_interval SAVE_INTERVAL]
                               [--save_path SAVE_PATH] [--load_path LOAD_PATH]
                               [--target_name TARGET_NAME] [--fp_size FP_SIZE]
                               [--transfer_learning]
                               [--transfer_learning_ratio TRANSFER_LEARNING_RATIO]
                               [--num_workers NUM_WORKERS]
                               [--enc_node_feat_dim ENC_NODE_FEAT_DIM]
                               [--enc_edge_feat_dim ENC_EDGE_FEAT_DIM]
                               [--enc_node_embedding_size ENC_NODE_EMBEDDING_SIZE]
                               [--enc_edge_embedding_size ENC_EDGE_EMBEDDING_SIZE]
                               [--enc_n_layers ENC_N_LAYERS]
                               [--enc_gat_n_heads ENC_GAT_N_HEADS]
                               [--enc_gat_ffn_ratio ENC_GAT_FFN_RATIO]
                               [--enc_fp_embedding_size ENC_FP_EMBEDDING_SIZE]
                               [--enc_fp_ffn_size ENC_FP_FFN_SIZE]
                               [--enc_fp_gated]
                               [--enc_fp_n_heads ENC_FP_N_HEADS]
                               [--enc_fp_size ENC_FP_SIZE]
                               [--enc_fp_to_gat_feedback ENC_FP_TO_GAT_FEEDBACK]
                               [--enc_gat_to_fp_pooling ENC_GAT_TO_FP_POOLING]
                               [--dec_node_input_size DEC_NODE_INPUT_SIZE]
                               [--dec_node_emb_size DEC_NODE_EMB_SIZE]
                               [--dec_fp_input_size DEC_FP_INPUT_SIZE]
                               [--dec_fp_emb_size DEC_FP_EMB_SIZE]
                               [--dec_output_size DEC_OUTPUT_SIZE]
                               [--reg_input_size REG_INPUT_SIZE]
                               [--reg_hidden_size REG_HIDDEN_SIZE]
                               [--reg_output_size REG_OUTPUT_SIZE]
                               [--record_path RECORD_PATH]
test_refnet_transfer.py: error: unrecognized arguments: --enc_with_fp --dec_with_fp --dec_with_dist
nohup: ignoring input
INFO:__main__:name: ca9_05
INFO:__main__:seed: 4
INFO:__main__:device: cuda:2
INFO:__main__:batch_size: 2048
INFO:__main__:epochs: 100
INFO:__main__:lr: 0.0003
INFO:__main__:log_interval: 1
INFO:__main__:save_interval: 5
INFO:__main__:save_path: /data02/gtguo/DEL/data/weights/refnet_ca9_transfer/
INFO:__main__:load_path: /data02/gtguo/DEL/data/weights/refnet/
INFO:__main__:target_name: ca9
INFO:__main__:fp_size: 2048
INFO:__main__:transfer_learning: True
INFO:__main__:transfer_learning_ratio: 0.05
INFO:__main__:num_workers: 8
INFO:__main__:enc_node_feat_dim: 19
INFO:__main__:enc_edge_feat_dim: 2
INFO:__main__:enc_node_embedding_size: 64
INFO:__main__:enc_edge_embedding_size: 64
INFO:__main__:enc_n_layers: 5
INFO:__main__:enc_gat_n_heads: 4
INFO:__main__:enc_gat_ffn_ratio: 4
INFO:__main__:enc_fp_embedding_size: 32
INFO:__main__:enc_fp_ffn_size: 128
INFO:__main__:enc_fp_gated: False
INFO:__main__:enc_fp_n_heads: 4
INFO:__main__:enc_fp_size: 256
INFO:__main__:enc_fp_to_gat_feedback: add
INFO:__main__:enc_gat_to_fp_pooling: mean
INFO:__main__:dec_node_input_size: 64
INFO:__main__:dec_node_emb_size: 64
INFO:__main__:dec_fp_input_size: 32
INFO:__main__:dec_fp_emb_size: 64
INFO:__main__:dec_output_size: 2
INFO:__main__:reg_input_size: 64
INFO:__main__:reg_hidden_size: 64
INFO:__main__:reg_output_size: 1
INFO:__main__:record_path: /data02/gtguo/DEL/data/records/refnet_ca9_transfer/
INFO:__main__:Model has 408772 parameters
INFO:__main__:Head has 4225 parameters
Splitting dataset:   0%|          | 0/2372674 [00:00<?, ?it/s]Splitting dataset:   1%|          | 17573/2372674 [00:00<00:13, 175720.11it/s]Splitting dataset:   2%|▏         | 36358/2372674 [00:00<00:12, 182849.38it/s]Splitting dataset:   2%|▏         | 54643/2372674 [00:00<00:12, 182596.89it/s]Splitting dataset:   3%|▎         | 73388/2372674 [00:00<00:12, 184507.63it/s]Splitting dataset:   4%|▍         | 92277/2372674 [00:00<00:12, 186083.29it/s]Splitting dataset:   5%|▍         | 110886/2372674 [00:00<00:12, 182717.59it/s]Splitting dataset:   5%|▌         | 129170/2372674 [00:00<00:12, 180114.12it/s]Splitting dataset:   6%|▌         | 147374/2372674 [00:00<00:12, 180713.16it/s]Splitting dataset:   7%|▋         | 166254/2372674 [00:00<00:12, 183204.21it/s]Splitting dataset:   8%|▊         | 184636/2372674 [00:01<00:11, 183389.48it/s]Splitting dataset:   9%|▊         | 202982/2372674 [00:01<00:11, 183091.79it/s]Splitting dataset:   9%|▉         | 222296/2372674 [00:01<00:11, 186129.34it/s]Splitting dataset:  10%|█         | 240914/2372674 [00:01<00:11, 185941.07it/s]Splitting dataset:  11%|█         | 260878/2372674 [00:01<00:11, 190061.28it/s]Splitting dataset:  12%|█▏        | 279888/2372674 [00:01<00:11, 189367.02it/s]Splitting dataset:  13%|█▎        | 299451/2372674 [00:01<00:10, 191241.40it/s]Splitting dataset:  13%|█▎        | 319537/2372674 [00:01<00:10, 194118.10it/s]Splitting dataset:  14%|█▍        | 338952/2372674 [00:01<00:10, 190888.06it/s]Splitting dataset:  15%|█▌        | 358183/2372674 [00:01<00:10, 191306.56it/s]Splitting dataset:  16%|█▌        | 377478/2372674 [00:02<00:10, 191793.55it/s]Splitting dataset:  17%|█▋        | 396665/2372674 [00:02<00:10, 191084.13it/s]Splitting dataset:  18%|█▊        | 416131/2372674 [00:02<00:10, 192145.77it/s]Splitting dataset:  18%|█▊        | 435351/2372674 [00:02<00:10, 190208.19it/s]Splitting dataset:  19%|█▉        | 454379/2372674 [00:02<00:10, 188822.89it/s]Splitting dataset:  20%|█▉        | 473267/2372674 [00:02<00:10, 188202.75it/s]Splitting dataset:  21%|██        | 493828/2372674 [00:02<00:09, 193362.98it/s]Splitting dataset:  22%|██▏       | 513173/2372674 [00:02<00:09, 190497.75it/s]Splitting dataset:  22%|██▏       | 532237/2372674 [00:02<00:09, 188910.65it/s]Splitting dataset:  23%|██▎       | 551748/2372674 [00:02<00:09, 190735.15it/s]Splitting dataset:  24%|██▍       | 571122/2372674 [00:03<00:09, 191621.67it/s]Splitting dataset:  25%|██▍       | 590293/2372674 [00:03<00:09, 191631.96it/s]Splitting dataset:  26%|██▌       | 611040/2372674 [00:03<00:08, 196350.38it/s]Splitting dataset:  27%|██▋       | 630682/2372674 [00:03<00:08, 195751.53it/s]Splitting dataset:  27%|██▋       | 650366/2372674 [00:03<00:08, 196073.92it/s]Splitting dataset:  28%|██▊       | 670234/2372674 [00:03<00:08, 196849.16it/s]Splitting dataset:  29%|██▉       | 689922/2372674 [00:03<00:08, 196292.13it/s]Splitting dataset:  30%|██▉       | 710170/2372674 [00:03<00:08, 198138.50it/s]Splitting dataset:  31%|███       | 729987/2372674 [00:03<00:08, 196904.78it/s]Splitting dataset:  32%|███▏      | 750188/2372674 [00:03<00:08, 198424.53it/s]Splitting dataset:  32%|███▏      | 770290/2372674 [00:04<00:08, 199195.26it/s]Splitting dataset:  33%|███▎      | 790213/2372674 [00:04<00:07, 198429.93it/s]Splitting dataset:  34%|███▍      | 810395/2372674 [00:04<00:07, 199438.76it/s]Splitting dataset:  35%|███▍      | 830395/2372674 [00:04<00:07, 199602.77it/s]Splitting dataset:  36%|███▌      | 850586/2372674 [00:04<00:07, 200290.31it/s]Splitting dataset:  37%|███▋      | 870617/2372674 [00:04<00:07, 197919.81it/s]Splitting dataset:  38%|███▊      | 890544/2372674 [00:04<00:07, 198317.41it/s]Splitting dataset:  38%|███▊      | 910620/2372674 [00:04<00:07, 199043.50it/s]Splitting dataset:  39%|███▉      | 930581/2372674 [00:04<00:07, 199210.15it/s]Splitting dataset:  40%|████      | 950505/2372674 [00:04<00:07, 198824.81it/s]Splitting dataset:  41%|████      | 970715/2372674 [00:05<00:07, 199800.80it/s]Splitting dataset:  42%|████▏     | 990697/2372674 [00:05<00:06, 199628.16it/s]Splitting dataset:  43%|████▎     | 1010662/2372674 [00:05<00:06, 197760.20it/s]Splitting dataset:  43%|████▎     | 1030443/2372674 [00:05<00:06, 193435.88it/s]Splitting dataset:  44%|████▍     | 1050274/2372674 [00:05<00:06, 194863.03it/s]Splitting dataset:  45%|████▌     | 1071496/2372674 [00:05<00:06, 199989.54it/s]Splitting dataset:  46%|████▌     | 1092159/2372674 [00:05<00:06, 201957.17it/s]Splitting dataset:  47%|████▋     | 1112371/2372674 [00:05<00:06, 200922.46it/s]Splitting dataset:  48%|████▊     | 1132476/2372674 [00:05<00:06, 199455.39it/s]Splitting dataset:  49%|████▊     | 1153094/2372674 [00:05<00:06, 201446.04it/s]Splitting dataset:  49%|████▉     | 1173427/2372674 [00:06<00:05, 202003.98it/s]Splitting dataset:  50%|█████     | 1193635/2372674 [00:06<00:05, 199795.74it/s]Splitting dataset:  51%|█████     | 1214665/2372674 [00:06<00:05, 202906.38it/s]Splitting dataset:  52%|█████▏    | 1234966/2372674 [00:06<00:05, 202322.58it/s]Splitting dataset:  53%|█████▎    | 1255206/2372674 [00:06<00:05, 195187.32it/s]Splitting dataset:  54%|█████▍    | 1275837/2372674 [00:06<00:05, 198420.22it/s]Splitting dataset:  55%|█████▍    | 1295728/2372674 [00:06<00:05, 197280.18it/s]Splitting dataset:  55%|█████▌    | 1316279/2372674 [00:06<00:05, 199697.09it/s]Splitting dataset:  56%|█████▋    | 1336277/2372674 [00:06<00:05, 199376.12it/s]Splitting dataset:  57%|█████▋    | 1356512/2372674 [00:06<00:05, 200252.01it/s]Splitting dataset:  58%|█████▊    | 1376552/2372674 [00:07<00:04, 199433.10it/s]Splitting dataset:  59%|█████▉    | 1396506/2372674 [00:07<00:04, 198809.36it/s]Splitting dataset:  60%|█████▉    | 1416395/2372674 [00:07<00:04, 198087.58it/s]Splitting dataset:  61%|██████    | 1437053/2372674 [00:07<00:04, 200610.90it/s]Splitting dataset:  61%|██████▏   | 1457120/2372674 [00:07<00:04, 200193.02it/s]Splitting dataset:  62%|██████▏   | 1477157/2372674 [00:07<00:04, 200243.59it/s]Splitting dataset:  63%|██████▎   | 1497185/2372674 [00:07<00:04, 200147.59it/s]Splitting dataset:  64%|██████▍   | 1518107/2372674 [00:07<00:04, 202858.29it/s]Splitting dataset:  65%|██████▍   | 1538696/2372674 [00:07<00:04, 203762.06it/s]Splitting dataset:  66%|██████▌   | 1559974/2372674 [00:07<00:03, 206458.68it/s]Splitting dataset:  67%|██████▋   | 1581396/2372674 [00:08<00:03, 208782.15it/s]Splitting dataset:  68%|██████▊   | 1602276/2372674 [00:08<00:03, 205629.21it/s]Splitting dataset:  68%|██████▊   | 1623173/2372674 [00:08<00:03, 206617.96it/s]Splitting dataset:  69%|██████▉   | 1644328/2372674 [00:08<00:03, 208082.89it/s]Splitting dataset:  70%|███████   | 1665841/2372674 [00:08<00:03, 210181.36it/s]Splitting dataset:  71%|███████   | 1687120/2372674 [00:08<00:03, 210956.31it/s]Splitting dataset:  72%|███████▏  | 1708221/2372674 [00:08<00:03, 207963.37it/s]Splitting dataset:  73%|███████▎  | 1729030/2372674 [00:08<00:03, 203420.59it/s]Splitting dataset:  74%|███████▎  | 1749685/2372674 [00:08<00:03, 204332.82it/s]Splitting dataset:  75%|███████▍  | 1770139/2372674 [00:09<00:02, 204278.92it/s]Splitting dataset:  75%|███████▌  | 1790858/2372674 [00:09<00:02, 205139.31it/s]Splitting dataset:  76%|███████▋  | 1811383/2372674 [00:09<00:02, 204709.81it/s]Splitting dataset:  77%|███████▋  | 1831862/2372674 [00:09<00:02, 197978.94it/s]Splitting dataset:  78%|███████▊  | 1851709/2372674 [00:09<00:02, 194121.64it/s]Splitting dataset:  79%|███████▉  | 1871704/2372674 [00:09<00:02, 195807.09it/s]Splitting dataset:  80%|███████▉  | 1891320/2372674 [00:09<00:02, 195895.69it/s]Splitting dataset:  81%|████████  | 1910935/2372674 [00:09<00:02, 193682.71it/s]Splitting dataset:  81%|████████▏ | 1930323/2372674 [00:09<00:02, 191603.73it/s]Splitting dataset:  82%|████████▏ | 1949499/2372674 [00:09<00:02, 191559.05it/s]Splitting dataset:  83%|████████▎ | 1968666/2372674 [00:10<00:02, 190875.39it/s]Splitting dataset:  84%|████████▍ | 1987761/2372674 [00:10<00:02, 190694.65it/s]Splitting dataset:  85%|████████▍ | 2007002/2372674 [00:10<00:01, 191199.92it/s]Splitting dataset:  85%|████████▌ | 2026430/2372674 [00:10<00:01, 192113.71it/s]Splitting dataset:  86%|████████▌ | 2045731/2372674 [00:10<00:01, 192377.03it/s]Splitting dataset:  87%|████████▋ | 2065074/2372674 [00:10<00:01, 192688.81it/s]Splitting dataset:  88%|████████▊ | 2084345/2372674 [00:10<00:01, 191301.46it/s]Splitting dataset:  89%|████████▊ | 2103479/2372674 [00:10<00:01, 188354.96it/s]Splitting dataset:  89%|████████▉ | 2122326/2372674 [00:10<00:01, 187252.28it/s]Splitting dataset:  90%|█████████ | 2141059/2372674 [00:10<00:01, 181250.23it/s]Splitting dataset:  91%|█████████ | 2159484/2372674 [00:11<00:01, 182116.22it/s]Splitting dataset:  92%|█████████▏| 2179538/2372674 [00:11<00:01, 187510.60it/s]Splitting dataset:  93%|█████████▎| 2199645/2372674 [00:11<00:00, 191507.79it/s]Splitting dataset:  94%|█████████▎| 2220326/2372674 [00:11<00:00, 196042.72it/s]Splitting dataset:  94%|█████████▍| 2240912/2372674 [00:11<00:00, 198960.77it/s]Splitting dataset:  95%|█████████▌| 2261983/2372674 [00:11<00:00, 202461.76it/s]Splitting dataset:  96%|█████████▌| 2282731/2372674 [00:11<00:00, 203957.82it/s]Splitting dataset:  97%|█████████▋| 2303643/2372674 [00:11<00:00, 205498.88it/s]Splitting dataset:  98%|█████████▊| 2324203/2372674 [00:11<00:00, 193272.77it/s]Splitting dataset:  99%|█████████▉| 2343685/2372674 [00:11<00:00, 190174.07it/s]Splitting dataset: 100%|█████████▉| 2362811/2372674 [00:12<00:00, 189702.36it/s]Splitting dataset: 100%|██████████| 2372674/2372674 [00:12<00:00, 195548.84it/s]
INFO:root:processing dataset...
  0%|          | 0/5453 [00:00<?, ?it/s]  1%|          | 52/5453 [00:00<00:10, 512.48it/s]  2%|▏         | 106/5453 [00:00<00:10, 522.97it/s]  3%|▎         | 169/5453 [00:00<00:09, 570.62it/s]  4%|▍         | 227/5453 [00:00<00:09, 540.14it/s]/data02/gtguo/DEL/pkg/utils/mol_feat.py:71: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matricesor `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484808560/work/aten/src/ATen/native/TensorShape.cpp:2981.)
  return torch.tensor(coo, dtype=torch.long).T
  5%|▌         | 297/5453 [00:00<00:08, 593.23it/s]  7%|▋         | 357/5453 [00:00<00:09, 554.39it/s]  8%|▊         | 423/5453 [00:00<00:08, 584.70it/s]  9%|▉         | 483/5453 [00:00<00:08, 553.86it/s] 10%|▉         | 540/5453 [00:00<00:09, 543.54it/s] 11%|█         | 599/5453 [00:01<00:08, 556.06it/s] 12%|█▏        | 655/5453 [00:01<00:08, 540.35it/s] 13%|█▎        | 710/5453 [00:01<00:08, 531.03it/s] 14%|█▍        | 768/5453 [00:01<00:08, 544.51it/s] 15%|█▌        | 823/5453 [00:01<00:08, 535.50it/s] 16%|█▌        | 877/5453 [00:01<00:08, 530.09it/s] 17%|█▋        | 931/5453 [00:01<00:08, 529.94it/s] 18%|█▊        | 990/5453 [00:01<00:08, 545.68it/s] 19%|█▉        | 1045/5453 [00:01<00:08, 536.17it/s] 20%|██        | 1105/5453 [00:02<00:07, 552.50it/s] 21%|██▏       | 1163/5453 [00:02<00:07, 560.27it/s] 22%|██▏       | 1220/5453 [00:02<00:07, 551.02it/s] 23%|██▎       | 1276/5453 [00:02<00:07, 539.47it/s] 24%|██▍       | 1331/5453 [00:02<00:08, 513.76it/s] 26%|██▌       | 1395/5453 [00:02<00:07, 547.61it/s] 27%|██▋       | 1451/5453 [00:02<00:07, 550.08it/s] 28%|██▊       | 1507/5453 [00:02<00:07, 546.92it/s] 29%|██▊       | 1566/5453 [00:02<00:06, 559.04it/s] 30%|██▉       | 1623/5453 [00:02<00:06, 554.21it/s] 31%|███       | 1679/5453 [00:03<00:06, 549.93it/s] 32%|███▏      | 1740/5453 [00:03<00:06, 565.37it/s] 33%|███▎      | 1797/5453 [00:03<00:06, 536.93it/s] 34%|███▍      | 1852/5453 [00:03<00:06, 522.87it/s] 35%|███▍      | 1905/5453 [00:03<00:07, 489.60it/s] 36%|███▌      | 1956/5453 [00:03<00:07, 493.81it/s] 37%|███▋      | 2012/5453 [00:03<00:06, 510.46it/s] 38%|███▊      | 2066/5453 [00:03<00:06, 517.74it/s] 39%|███▉      | 2121/5453 [00:03<00:06, 526.74it/s] 40%|███▉      | 2176/5453 [00:04<00:06, 531.47it/s] 41%|████      | 2232/5453 [00:04<00:05, 539.79it/s] 42%|████▏     | 2287/5453 [00:04<00:06, 517.42it/s] 43%|████▎     | 2341/5453 [00:04<00:05, 523.34it/s] 44%|████▍     | 2399/5453 [00:04<00:05, 538.39it/s] 45%|████▌     | 2455/5453 [00:04<00:05, 541.91it/s] 46%|████▌     | 2510/5453 [00:04<00:05, 533.08it/s] 47%|████▋     | 2564/5453 [00:04<00:05, 511.47it/s] 48%|████▊     | 2621/5453 [00:04<00:05, 526.39it/s] 49%|████▉     | 2674/5453 [00:05<00:05, 486.70it/s] 50%|████▉     | 2724/5453 [00:05<00:05, 488.17it/s] 51%|█████     | 2780/5453 [00:05<00:05, 507.90it/s] 52%|█████▏    | 2834/5453 [00:05<00:05, 516.12it/s] 53%|█████▎    | 2886/5453 [00:05<00:05, 506.80it/s] 54%|█████▍    | 2937/5453 [00:05<00:05, 448.72it/s] 55%|█████▍    | 2990/5453 [00:05<00:05, 467.31it/s] 56%|█████▌    | 3038/5453 [00:05<00:05, 469.09it/s] 57%|█████▋    | 3088/5453 [00:05<00:04, 475.20it/s] 58%|█████▊    | 3139/5453 [00:05<00:04, 482.99it/s] 58%|█████▊    | 3189/5453 [00:06<00:04, 486.52it/s] 59%|█████▉    | 3240/5453 [00:06<00:04, 491.16it/s] 60%|██████    | 3292/5453 [00:06<00:04, 498.02it/s] 61%|██████▏   | 3344/5453 [00:06<00:04, 502.22it/s] 62%|██████▏   | 3396/5453 [00:06<00:04, 506.75it/s] 63%|██████▎   | 3447/5453 [00:06<00:03, 503.12it/s] 64%|██████▍   | 3500/5453 [00:06<00:03, 509.41it/s] 65%|██████▌   | 3553/5453 [00:06<00:03, 514.24it/s] 66%|██████▌   | 3605/5453 [00:06<00:03, 513.55it/s] 67%|██████▋   | 3657/5453 [00:06<00:03, 507.80it/s] 68%|██████▊   | 3708/5453 [00:07<00:03, 506.49it/s] 69%|██████▉   | 3759/5453 [00:07<00:05, 335.23it/s] 70%|██████▉   | 3809/5453 [00:07<00:04, 370.65it/s] 71%|███████   | 3857/5453 [00:07<00:04, 395.93it/s] 72%|███████▏  | 3909/5453 [00:07<00:03, 426.43it/s] 73%|███████▎  | 3959/5453 [00:07<00:03, 443.71it/s] 74%|███████▎  | 4013/5453 [00:07<00:03, 467.10it/s] 75%|███████▍  | 4064/5453 [00:07<00:02, 477.18it/s] 75%|███████▌  | 4117/5453 [00:08<00:02, 488.37it/s] 76%|███████▋  | 4169/5453 [00:08<00:02, 495.02it/s] 77%|███████▋  | 4220/5453 [00:08<00:02, 496.11it/s] 78%|███████▊  | 4271/5453 [00:08<00:02, 495.42it/s] 79%|███████▉  | 4321/5453 [00:08<00:02, 495.40it/s] 80%|████████  | 4373/5453 [00:08<00:02, 499.35it/s] 81%|████████  | 4424/5453 [00:08<00:02, 489.47it/s] 82%|████████▏ | 4474/5453 [00:08<00:02, 484.11it/s] 83%|████████▎ | 4523/5453 [00:08<00:01, 484.26it/s] 84%|████████▍ | 4572/5453 [00:09<00:01, 475.48it/s] 85%|████████▍ | 4621/5453 [00:09<00:01, 476.97it/s] 86%|████████▌ | 4669/5453 [00:09<00:01, 475.81it/s] 87%|████████▋ | 4717/5453 [00:09<00:01, 472.63it/s] 87%|████████▋ | 4765/5453 [00:09<00:01, 474.03it/s] 88%|████████▊ | 4814/5453 [00:09<00:01, 478.34it/s] 89%|████████▉ | 4862/5453 [00:09<00:01, 475.34it/s] 90%|█████████ | 4911/5453 [00:09<00:01, 478.42it/s] 91%|█████████ | 4961/5453 [00:09<00:01, 482.38it/s] 92%|█████████▏| 5012/5453 [00:09<00:00, 488.67it/s] 93%|█████████▎| 5062/5453 [00:10<00:00, 490.13it/s] 94%|█████████▍| 5113/5453 [00:10<00:00, 493.29it/s] 95%|█████████▍| 5163/5453 [00:10<00:00, 493.30it/s] 96%|█████████▌| 5215/5453 [00:10<00:00, 498.72it/s] 97%|█████████▋| 5265/5453 [00:10<00:00, 490.95it/s] 97%|█████████▋| 5315/5453 [00:10<00:00, 487.90it/s] 98%|█████████▊| 5364/5453 [00:10<00:00, 486.68it/s] 99%|█████████▉| 5413/5453 [00:10<00:00, 482.51it/s]100%|██████████| 5453/5453 [00:10<00:00, 502.15it/s]
INFO:__main__:Dataset has 5453 samples
INFO:__main__:Dataset data example: {'pyg_data': Data(x=[22, 19], edge_index=[2, 50], edge_attr=[50, 2]), 'activity': 5.028260409112222}
Traceback (most recent call last):
  File "/data02/gtguo/DEL/pkg/test_refnet_transfer.py", line 186, in <module>
    torch.load(os.path.join(args.load_path, args.name, "model.pt")), strict=False
  File "/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch/serialization.py", line 699, in load
    with _open_file_like(f, 'rb') as opened_file:
  File "/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch/serialization.py", line 230, in _open_file_like
    return _open_file(name_or_buffer, mode)
  File "/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch/serialization.py", line 211, in __init__
    super(_open_file, self).__init__(open(name, mode))
FileNotFoundError: [Errno 2] No such file or directory: '/data02/gtguo/DEL/data/weights/refnet/ca9_05/model.pt'
nohup: ignoring input
INFO:__main__:name: ca9_05
INFO:__main__:base_fname: ca9_large
INFO:__main__:seed: 4
INFO:__main__:device: cuda:0
INFO:__main__:batch_size: 2048
INFO:__main__:epochs: 100
INFO:__main__:lr: 0.0003
INFO:__main__:log_interval: 1
INFO:__main__:save_interval: 5
INFO:__main__:save_path: /data02/gtguo/DEL/data/weights/refnet_ca9_transfer/
INFO:__main__:load_path: /data02/gtguo/DEL/data/weights/refnet/
INFO:__main__:target_name: ca9
INFO:__main__:fp_size: 2048
INFO:__main__:transfer_learning: True
INFO:__main__:transfer_learning_ratio: 0.05
INFO:__main__:num_workers: 8
INFO:__main__:enc_node_feat_dim: 19
INFO:__main__:enc_edge_feat_dim: 2
INFO:__main__:enc_node_embedding_size: 64
INFO:__main__:enc_edge_embedding_size: 64
INFO:__main__:enc_n_layers: 5
INFO:__main__:enc_gat_n_heads: 4
INFO:__main__:enc_gat_ffn_ratio: 4
INFO:__main__:enc_fp_embedding_size: 32
INFO:__main__:enc_fp_ffn_size: 128
INFO:__main__:enc_fp_gated: False
INFO:__main__:enc_fp_n_heads: 4
INFO:__main__:enc_fp_size: 256
INFO:__main__:enc_fp_to_gat_feedback: add
INFO:__main__:enc_gat_to_fp_pooling: mean
INFO:__main__:dec_node_input_size: 64
INFO:__main__:dec_node_emb_size: 64
INFO:__main__:dec_fp_input_size: 32
INFO:__main__:dec_fp_emb_size: 64
INFO:__main__:dec_output_size: 2
INFO:__main__:reg_input_size: 64
INFO:__main__:reg_hidden_size: 64
INFO:__main__:reg_output_size: 1
INFO:__main__:record_path: /data02/gtguo/DEL/data/records/refnet_ca9_transfer/
INFO:__main__:Model has 408772 parameters
INFO:__main__:Head has 4225 parameters
Splitting dataset:   0%|          | 0/2372674 [00:00<?, ?it/s]Splitting dataset:   1%|          | 17933/2372674 [00:00<00:13, 179276.74it/s]Splitting dataset:   2%|▏         | 36466/2372674 [00:00<00:12, 182813.31it/s]Splitting dataset:   2%|▏         | 55643/2372674 [00:00<00:12, 186897.76it/s]Splitting dataset:   3%|▎         | 74333/2372674 [00:00<00:12, 184471.89it/s]Splitting dataset:   4%|▍         | 93997/2372674 [00:00<00:12, 188820.09it/s]Splitting dataset:   5%|▍         | 112886/2372674 [00:00<00:12, 188311.13it/s]Splitting dataset:   6%|▌         | 132282/2372674 [00:00<00:11, 190143.52it/s]Splitting dataset:   6%|▋         | 151301/2372674 [00:00<00:11, 188242.62it/s]Splitting dataset:   7%|▋         | 170550/2372674 [00:00<00:11, 189555.06it/s]Splitting dataset:   8%|▊         | 189511/2372674 [00:01<00:11, 188778.19it/s]Splitting dataset:   9%|▉         | 208878/2372674 [00:01<00:11, 190259.42it/s]Splitting dataset:  10%|▉         | 228087/2372674 [00:01<00:11, 190811.36it/s]Splitting dataset:  10%|█         | 248643/2372674 [00:01<00:10, 195258.74it/s]Splitting dataset:  11%|█▏        | 268681/2372674 [00:01<00:10, 196797.27it/s]Splitting dataset:  12%|█▏        | 288364/2372674 [00:01<00:10, 196385.39it/s]Splitting dataset:  13%|█▎        | 309397/2372674 [00:01<00:10, 200569.55it/s]Splitting dataset:  14%|█▍        | 329457/2372674 [00:01<00:10, 197190.21it/s]Splitting dataset:  15%|█▍        | 349625/2372674 [00:01<00:10, 198518.46it/s]Splitting dataset:  16%|█▌        | 369489/2372674 [00:01<00:10, 198192.64it/s]Splitting dataset:  16%|█▋        | 389317/2372674 [00:02<00:10, 194754.83it/s]Splitting dataset:  17%|█▋        | 408810/2372674 [00:02<00:10, 194177.40it/s]Splitting dataset:  18%|█▊        | 428239/2372674 [00:02<00:10, 193900.58it/s]Splitting dataset:  19%|█▉        | 448204/2372674 [00:02<00:09, 195602.98it/s]Splitting dataset:  20%|█▉        | 467772/2372674 [00:02<00:09, 194423.43it/s]Splitting dataset:  21%|██        | 488384/2372674 [00:02<00:09, 197894.92it/s]Splitting dataset:  21%|██▏       | 508765/2372674 [00:02<00:09, 199653.84it/s]Splitting dataset:  22%|██▏       | 528956/2372674 [00:02<00:09, 200323.33it/s]Splitting dataset:  23%|██▎       | 548994/2372674 [00:02<00:09, 198637.27it/s]Splitting dataset:  24%|██▍       | 569238/2372674 [00:02<00:09, 199765.08it/s]Splitting dataset:  25%|██▍       | 589696/2372674 [00:03<00:08, 201199.12it/s]Splitting dataset:  26%|██▌       | 611256/2372674 [00:03<00:08, 205498.70it/s]Splitting dataset:  27%|██▋       | 631811/2372674 [00:03<00:08, 196252.04it/s]Splitting dataset:  27%|██▋       | 651526/2372674 [00:03<00:08, 195788.17it/s]Splitting dataset:  28%|██▊       | 671943/2372674 [00:03<00:08, 198232.23it/s]Splitting dataset:  29%|██▉       | 691939/2372674 [00:03<00:08, 198738.94it/s]Splitting dataset:  30%|███       | 711849/2372674 [00:03<00:08, 198347.36it/s]Splitting dataset:  31%|███       | 731709/2372674 [00:03<00:08, 197214.84it/s]Splitting dataset:  32%|███▏      | 751449/2372674 [00:03<00:08, 194765.63it/s]Splitting dataset:  33%|███▎      | 771894/2372674 [00:03<00:08, 197618.64it/s]Splitting dataset:  33%|███▎      | 791673/2372674 [00:04<00:08, 197297.00it/s]Splitting dataset:  34%|███▍      | 811415/2372674 [00:04<00:07, 197217.95it/s]Splitting dataset:  35%|███▌      | 831145/2372674 [00:04<00:07, 196320.52it/s]Splitting dataset:  36%|███▌      | 851494/2372674 [00:04<00:07, 198449.94it/s]Splitting dataset:  37%|███▋      | 871345/2372674 [00:04<00:07, 197836.74it/s]Splitting dataset:  38%|███▊      | 891133/2372674 [00:04<00:07, 196970.30it/s]Splitting dataset:  38%|███▊      | 911172/2372674 [00:04<00:07, 197984.98it/s]Splitting dataset:  39%|███▉      | 931679/2372674 [00:04<00:07, 200097.37it/s]Splitting dataset:  40%|████      | 952747/2372674 [00:04<00:06, 203257.58it/s]Splitting dataset:  41%|████      | 973454/2372674 [00:04<00:06, 204395.73it/s]Splitting dataset:  42%|████▏     | 993897/2372674 [00:05<00:06, 202852.25it/s]Splitting dataset:  43%|████▎     | 1014187/2372674 [00:05<00:06, 201630.19it/s]Splitting dataset:  44%|████▎     | 1034354/2372674 [00:05<00:06, 198501.48it/s]Splitting dataset:  44%|████▍     | 1054216/2372674 [00:05<00:06, 197058.94it/s]Splitting dataset:  45%|████▌     | 1074889/2372674 [00:05<00:06, 199906.01it/s]Splitting dataset:  46%|████▌     | 1095223/2372674 [00:05<00:06, 200920.33it/s]Splitting dataset:  47%|████▋     | 1115324/2372674 [00:05<00:06, 197712.11it/s]Splitting dataset:  48%|████▊     | 1135111/2372674 [00:05<00:06, 197360.86it/s]Splitting dataset:  49%|████▊     | 1155149/2372674 [00:05<00:06, 198252.27it/s]Splitting dataset:  50%|████▉     | 1174983/2372674 [00:05<00:06, 197320.15it/s]Splitting dataset:  50%|█████     | 1194721/2372674 [00:06<00:06, 196042.62it/s]Splitting dataset:  51%|█████▏    | 1216038/2372674 [00:06<00:05, 201116.59it/s]Splitting dataset:  52%|█████▏    | 1236500/2372674 [00:06<00:05, 202157.04it/s]Splitting dataset:  53%|█████▎    | 1256724/2372674 [00:06<00:05, 201962.24it/s]Splitting dataset:  54%|█████▍    | 1277627/2372674 [00:06<00:05, 204069.25it/s]Splitting dataset:  55%|█████▍    | 1298039/2372674 [00:06<00:05, 203141.35it/s]Splitting dataset:  56%|█████▌    | 1318901/2372674 [00:06<00:05, 204772.89it/s]Splitting dataset:  56%|█████▋    | 1339382/2372674 [00:06<00:05, 202445.06it/s]Splitting dataset:  57%|█████▋    | 1359912/2372674 [00:06<00:04, 203289.50it/s]Splitting dataset:  58%|█████▊    | 1380287/2372674 [00:06<00:04, 203423.92it/s]Splitting dataset:  59%|█████▉    | 1400634/2372674 [00:07<00:04, 202122.82it/s]Splitting dataset:  60%|█████▉    | 1420995/2372674 [00:07<00:04, 202561.75it/s]Splitting dataset:  61%|██████    | 1441820/2372674 [00:07<00:04, 204256.06it/s]Splitting dataset:  62%|██████▏   | 1462250/2372674 [00:07<00:04, 202761.77it/s]Splitting dataset:  62%|██████▏   | 1482584/2372674 [00:07<00:04, 202931.03it/s]Splitting dataset:  63%|██████▎   | 1502881/2372674 [00:07<00:04, 202912.98it/s]Splitting dataset:  64%|██████▍   | 1524715/2372674 [00:07<00:04, 207518.52it/s]Splitting dataset:  65%|██████▌   | 1546096/2372674 [00:07<00:03, 209396.30it/s]Splitting dataset:  66%|██████▌   | 1567877/2372674 [00:07<00:03, 211910.69it/s]Splitting dataset:  67%|██████▋   | 1589071/2372674 [00:07<00:03, 209988.67it/s]Splitting dataset:  68%|██████▊   | 1610076/2372674 [00:08<00:03, 207641.07it/s]Splitting dataset:  69%|██████▉   | 1632125/2372674 [00:08<00:03, 211443.32it/s]Splitting dataset:  70%|██████▉   | 1654127/2372674 [00:08<00:03, 213989.70it/s]Splitting dataset:  71%|███████   | 1676103/2372674 [00:08<00:03, 215707.51it/s]Splitting dataset:  72%|███████▏  | 1697682/2372674 [00:08<00:03, 214956.24it/s]Splitting dataset:  72%|███████▏  | 1719184/2372674 [00:08<00:03, 209896.56it/s]Splitting dataset:  73%|███████▎  | 1740202/2372674 [00:08<00:03, 209309.68it/s]Splitting dataset:  74%|███████▍  | 1761152/2372674 [00:08<00:03, 200852.62it/s]Splitting dataset:  75%|███████▌  | 1782413/2372674 [00:08<00:02, 204233.50it/s]Splitting dataset:  76%|███████▌  | 1803088/2372674 [00:09<00:02, 204962.19it/s]Splitting dataset:  77%|███████▋  | 1823634/2372674 [00:09<00:02, 203221.58it/s]Splitting dataset:  78%|███████▊  | 1843991/2372674 [00:09<00:02, 197382.69it/s]Splitting dataset:  79%|███████▊  | 1863781/2372674 [00:09<00:02, 197252.86it/s]Splitting dataset:  79%|███████▉  | 1884275/2372674 [00:09<00:02, 199497.33it/s]Splitting dataset:  80%|████████  | 1904256/2372674 [00:09<00:02, 199175.74it/s]Splitting dataset:  81%|████████  | 1924195/2372674 [00:09<00:02, 198074.41it/s]Splitting dataset:  82%|████████▏ | 1944018/2372674 [00:09<00:02, 197186.74it/s]Splitting dataset:  83%|████████▎ | 1963747/2372674 [00:09<00:02, 196209.99it/s]Splitting dataset:  84%|████████▎ | 1983375/2372674 [00:09<00:01, 195359.68it/s]Splitting dataset:  84%|████████▍ | 2003063/2372674 [00:10<00:01, 195807.54it/s]Splitting dataset:  85%|████████▌ | 2023266/2372674 [00:10<00:01, 197655.50it/s]Splitting dataset:  86%|████████▌ | 2043036/2372674 [00:10<00:01, 197343.45it/s]Splitting dataset:  87%|████████▋ | 2063277/2372674 [00:10<00:01, 198853.37it/s]Splitting dataset:  88%|████████▊ | 2083166/2372674 [00:10<00:01, 198198.33it/s]Splitting dataset:  89%|████████▊ | 2102989/2372674 [00:10<00:01, 191844.75it/s]Splitting dataset:  89%|████████▉ | 2122217/2372674 [00:10<00:01, 187557.95it/s]Splitting dataset:  90%|█████████ | 2141596/2372674 [00:10<00:01, 189359.90it/s]Splitting dataset:  91%|█████████ | 2160568/2372674 [00:10<00:01, 185779.97it/s]Splitting dataset:  92%|█████████▏| 2181128/2372674 [00:10<00:01, 191541.15it/s]Splitting dataset:  93%|█████████▎| 2202809/2372674 [00:11<00:00, 198952.76it/s]Splitting dataset:  94%|█████████▍| 2224868/2372674 [00:11<00:00, 205341.00it/s]Splitting dataset:  95%|█████████▍| 2245643/2372674 [00:11<00:00, 206051.14it/s]Splitting dataset:  96%|█████████▌| 2267221/2372674 [00:11<00:00, 208942.94it/s]Splitting dataset:  96%|█████████▋| 2289467/2372674 [00:11<00:00, 212973.11it/s]Splitting dataset:  97%|█████████▋| 2310783/2372674 [00:11<00:00, 211589.16it/s]Splitting dataset:  98%|█████████▊| 2331957/2372674 [00:11<00:00, 204303.61it/s]Splitting dataset:  99%|█████████▉| 2352448/2372674 [00:11<00:00, 201525.17it/s]Splitting dataset: 100%|█████████▉| 2372644/2372674 [00:11<00:00, 199296.94it/s]Splitting dataset: 100%|██████████| 2372674/2372674 [00:11<00:00, 199456.09it/s]
INFO:root:processing dataset...
  0%|          | 0/5453 [00:00<?, ?it/s]  1%|          | 51/5453 [00:00<00:10, 508.16it/s]  2%|▏         | 106/5453 [00:00<00:10, 530.44it/s]  3%|▎         | 166/5453 [00:00<00:09, 560.99it/s]  4%|▍         | 223/5453 [00:00<00:09, 532.21it/s]/data02/gtguo/DEL/pkg/utils/mol_feat.py:71: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matricesor `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484808560/work/aten/src/ATen/native/TensorShape.cpp:2981.)
  return torch.tensor(coo, dtype=torch.long).T
  5%|▌         | 288/5453 [00:00<00:09, 571.54it/s]  6%|▋         | 346/5453 [00:00<00:09, 543.13it/s]  8%|▊         | 412/5453 [00:00<00:08, 576.43it/s]  9%|▊         | 471/5453 [00:00<00:08, 557.36it/s] 10%|▉         | 528/5453 [00:00<00:08, 550.82it/s] 11%|█         | 586/5453 [00:01<00:08, 559.18it/s] 12%|█▏        | 643/5453 [00:01<00:08, 554.30it/s] 13%|█▎        | 699/5453 [00:01<00:08, 537.93it/s] 14%|█▍        | 754/5453 [00:01<00:08, 538.88it/s] 15%|█▍        | 809/5453 [00:01<00:08, 538.40it/s] 16%|█▌        | 863/5453 [00:01<00:08, 528.46it/s] 17%|█▋        | 916/5453 [00:01<00:08, 523.97it/s] 18%|█▊        | 969/5453 [00:01<00:08, 500.79it/s] 19%|█▉        | 1027/5453 [00:01<00:08, 520.66it/s] 20%|█▉        | 1080/5453 [00:02<00:08, 502.06it/s] 21%|██        | 1131/5453 [00:02<00:08, 496.50it/s] 22%|██▏       | 1188/5453 [00:02<00:08, 515.12it/s] 23%|██▎       | 1243/5453 [00:02<00:08, 523.68it/s] 24%|██▍       | 1296/5453 [00:02<00:08, 501.67it/s] 25%|██▍       | 1351/5453 [00:02<00:07, 513.39it/s] 26%|██▌       | 1410/5453 [00:02<00:07, 533.49it/s] 27%|██▋       | 1466/5453 [00:02<00:07, 539.40it/s] 28%|██▊       | 1523/5453 [00:02<00:07, 545.59it/s] 29%|██▉       | 1583/5453 [00:02<00:06, 559.30it/s] 30%|███       | 1640/5453 [00:03<00:06, 548.20it/s] 31%|███       | 1701/5453 [00:03<00:06, 565.88it/s] 32%|███▏      | 1758/5453 [00:03<00:06, 565.79it/s] 33%|███▎      | 1815/5453 [00:03<00:06, 551.92it/s] 34%|███▍      | 1871/5453 [00:03<00:06, 523.94it/s] 35%|███▌      | 1924/5453 [00:03<00:07, 487.24it/s] 36%|███▋      | 1977/5453 [00:03<00:06, 497.70it/s] 37%|███▋      | 2033/5453 [00:03<00:06, 513.71it/s] 38%|███▊      | 2086/5453 [00:03<00:06, 516.55it/s] 39%|███▉      | 2139/5453 [00:04<00:06, 510.33it/s] 40%|████      | 2196/5453 [00:04<00:06, 524.83it/s] 41%|████      | 2249/5453 [00:04<00:06, 520.39it/s] 42%|████▏     | 2302/5453 [00:04<00:06, 519.44it/s] 43%|████▎     | 2355/5453 [00:04<00:06, 507.25it/s] 44%|████▍     | 2418/5453 [00:04<00:05, 540.13it/s] 45%|████▌     | 2473/5453 [00:04<00:05, 531.66it/s] 46%|████▋     | 2527/5453 [00:04<00:05, 516.63it/s] 47%|████▋     | 2579/5453 [00:04<00:05, 508.95it/s] 48%|████▊     | 2636/5453 [00:04<00:05, 524.34it/s] 49%|████▉     | 2689/5453 [00:05<00:05, 500.94it/s] 50%|█████     | 2740/5453 [00:05<00:05, 500.06it/s] 51%|█████▏    | 2795/5453 [00:05<00:05, 512.08it/s] 52%|█████▏    | 2850/5453 [00:05<00:04, 520.80it/s] 53%|█████▎    | 2903/5453 [00:05<00:05, 490.86it/s] 54%|█████▍    | 2953/5453 [00:05<00:05, 460.44it/s] 55%|█████▌    | 3007/5453 [00:05<00:05, 479.99it/s] 56%|█████▌    | 3058/5453 [00:05<00:04, 486.50it/s] 57%|█████▋    | 3108/5453 [00:05<00:04, 490.11it/s] 58%|█████▊    | 3158/5453 [00:06<00:04, 484.07it/s] 59%|█████▉    | 3207/5453 [00:06<00:04, 473.31it/s] 60%|█████▉    | 3258/5453 [00:06<00:04, 481.61it/s] 61%|██████    | 3309/5453 [00:06<00:04, 488.69it/s] 62%|██████▏   | 3360/5453 [00:06<00:04, 493.85it/s] 63%|██████▎   | 3411/5453 [00:06<00:04, 496.58it/s] 63%|██████▎   | 3461/5453 [00:06<00:04, 497.32it/s] 64%|██████▍   | 3514/5453 [00:06<00:03, 505.07it/s] 65%|██████▌   | 3566/5453 [00:06<00:03, 508.70it/s] 66%|██████▋   | 3617/5453 [00:06<00:03, 503.10it/s] 67%|██████▋   | 3668/5453 [00:07<00:03, 503.89it/s] 68%|██████▊   | 3719/5453 [00:07<00:03, 496.61it/s] 69%|██████▉   | 3769/5453 [00:07<00:05, 308.37it/s] 70%|███████   | 3820/5453 [00:07<00:04, 349.41it/s] 71%|███████   | 3870/5453 [00:07<00:04, 382.85it/s] 72%|███████▏  | 3922/5453 [00:07<00:03, 415.56it/s] 73%|███████▎  | 3969/5453 [00:07<00:03, 427.15it/s] 74%|███████▎  | 4021/5453 [00:07<00:03, 450.30it/s] 75%|███████▍  | 4071/5453 [00:08<00:02, 463.49it/s] 76%|███████▌  | 4120/5453 [00:08<00:02, 462.90it/s] 76%|███████▋  | 4168/5453 [00:08<00:02, 463.77it/s] 77%|███████▋  | 4216/5453 [00:08<00:02, 463.52it/s] 78%|███████▊  | 4264/5453 [00:08<00:02, 463.94it/s] 79%|███████▉  | 4313/5453 [00:08<00:02, 469.47it/s] 80%|███████▉  | 4361/5453 [00:08<00:02, 469.09it/s] 81%|████████  | 4409/5453 [00:08<00:02, 469.12it/s] 82%|████████▏ | 4457/5453 [00:08<00:02, 464.87it/s] 83%|████████▎ | 4506/5453 [00:09<00:02, 470.78it/s] 84%|████████▎ | 4554/5453 [00:09<00:01, 468.03it/s] 84%|████████▍ | 4602/5453 [00:09<00:01, 470.31it/s] 85%|████████▌ | 4650/5453 [00:09<00:01, 466.63it/s] 86%|████████▌ | 4697/5453 [00:09<00:01, 463.15it/s] 87%|████████▋ | 4744/5453 [00:09<00:01, 464.91it/s] 88%|████████▊ | 4793/5453 [00:09<00:01, 470.19it/s] 89%|████████▉ | 4842/5453 [00:09<00:01, 475.22it/s] 90%|████████▉ | 4890/5453 [00:09<00:01, 471.02it/s] 91%|█████████ | 4938/5453 [00:09<00:01, 469.76it/s] 91%|█████████▏| 4988/5453 [00:10<00:00, 476.19it/s] 92%|█████████▏| 5038/5453 [00:10<00:00, 480.54it/s] 93%|█████████▎| 5087/5453 [00:10<00:00, 479.54it/s] 94%|█████████▍| 5135/5453 [00:10<00:00, 476.39it/s] 95%|█████████▌| 5183/5453 [00:10<00:00, 474.75it/s] 96%|█████████▌| 5231/5453 [00:10<00:00, 476.05it/s] 97%|█████████▋| 5279/5453 [00:10<00:00, 470.99it/s] 98%|█████████▊| 5329/5453 [00:10<00:00, 478.94it/s] 99%|█████████▊| 5377/5453 [00:10<00:00, 471.33it/s] 99%|█████████▉| 5425/5453 [00:10<00:00, 471.47it/s]100%|██████████| 5453/5453 [00:11<00:00, 492.64it/s]
INFO:__main__:Dataset has 5453 samples
INFO:__main__:Dataset data example: {'pyg_data': Data(x=[22, 19], edge_index=[2, 50], edge_attr=[50, 2]), 'activity': 5.028260409112222}
/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([136])) that is different to the input size (torch.Size([136, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:__main__:Epoch 0, Train Loss 56.39084243774414
INFO:__main__:Epoch 0, Valid Loss 55.23600769042969
INFO:__main__:Saving model at epoch 0
INFO:__main__:Epoch 1, Train Loss 56.016719818115234
INFO:__main__:Epoch 1, Valid Loss 54.88306427001953
INFO:__main__:Epoch 2, Train Loss 55.65998458862305
INFO:__main__:Epoch 2, Valid Loss 54.54835510253906
INFO:__main__:Epoch 3, Train Loss 55.3216667175293
INFO:__main__:Epoch 3, Valid Loss 54.240779876708984
INFO:__main__:Epoch 4, Train Loss 55.010746002197266
INFO:__main__:Epoch 4, Valid Loss 53.94607925415039
INFO:__main__:Epoch 5, Train Loss 54.71283721923828
INFO:__main__:Epoch 5, Valid Loss 53.66215896606445
INFO:__main__:Saving model at epoch 5
INFO:__main__:Epoch 6, Train Loss 54.42582702636719
INFO:__main__:Epoch 6, Valid Loss 53.39445114135742
INFO:__main__:Epoch 7, Train Loss 54.15518569946289
INFO:__main__:Epoch 7, Valid Loss 53.131813049316406
INFO:__main__:Epoch 8, Train Loss 53.88966751098633
INFO:__main__:Epoch 8, Valid Loss 52.8891716003418
INFO:__main__:Epoch 9, Train Loss 53.64445114135742
INFO:__main__:Epoch 9, Valid Loss 52.67858123779297
INFO:__main__:Epoch 10, Train Loss 53.4315299987793
INFO:__main__:Epoch 10, Valid Loss 52.490264892578125
INFO:__main__:Saving model at epoch 10
INFO:__main__:Epoch 11, Train Loss 53.24116897583008
INFO:__main__:Epoch 11, Valid Loss 52.31437301635742
INFO:__main__:Epoch 12, Train Loss 53.06330490112305
INFO:__main__:Epoch 12, Valid Loss 52.148502349853516
INFO:__main__:Epoch 13, Train Loss 52.8956298828125
INFO:__main__:Epoch 13, Valid Loss 51.986663818359375
INFO:__main__:Epoch 14, Train Loss 52.732017517089844
INFO:__main__:Epoch 14, Valid Loss 51.82139587402344
INFO:__main__:Epoch 15, Train Loss 52.56491470336914
INFO:__main__:Epoch 15, Valid Loss 51.65856170654297
INFO:__main__:Saving model at epoch 15
INFO:__main__:Epoch 16, Train Loss 52.40028381347656
INFO:__main__:Epoch 16, Valid Loss 51.5072021484375
INFO:__main__:Epoch 17, Train Loss 52.247230529785156
INFO:__main__:Epoch 17, Valid Loss 51.354248046875
INFO:__main__:Epoch 18, Train Loss 52.09257507324219
INFO:__main__:Epoch 18, Valid Loss 51.19316864013672
INFO:__main__:Epoch 19, Train Loss 51.929710388183594
INFO:__main__:Epoch 19, Valid Loss 51.023223876953125
INFO:__main__:Epoch 20, Train Loss 51.757869720458984
INFO:__main__:Epoch 20, Valid Loss 50.845314025878906
INFO:__main__:Saving model at epoch 20
INFO:__main__:Epoch 21, Train Loss 51.577972412109375
INFO:__main__:Epoch 21, Valid Loss 50.658447265625
INFO:__main__:Epoch 22, Train Loss 51.3890380859375
INFO:__main__:Epoch 22, Valid Loss 50.46146011352539
INFO:__main__:Epoch 23, Train Loss 51.1898193359375
INFO:__main__:Epoch 23, Valid Loss 50.25360107421875
INFO:__main__:Epoch 24, Train Loss 50.9796028137207
INFO:__main__:Epoch 24, Valid Loss 50.03179931640625
INFO:__main__:Epoch 25, Train Loss 50.755271911621094
INFO:__main__:Epoch 25, Valid Loss 49.79931640625
INFO:__main__:Saving model at epoch 25
INFO:__main__:Epoch 26, Train Loss 50.52019119262695
INFO:__main__:Epoch 26, Valid Loss 49.56482696533203
INFO:__main__:Epoch 27, Train Loss 50.28305435180664
INFO:__main__:Epoch 27, Valid Loss 49.32741165161133
INFO:__main__:Epoch 28, Train Loss 50.042964935302734
INFO:__main__:Epoch 28, Valid Loss 49.084938049316406
INFO:__main__:Epoch 29, Train Loss 49.7977409362793
INFO:__main__:Epoch 29, Valid Loss 48.83715057373047
INFO:__main__:Epoch 30, Train Loss 49.547119140625
INFO:__main__:Epoch 30, Valid Loss 48.576759338378906
INFO:__main__:Saving model at epoch 30
INFO:__main__:Epoch 31, Train Loss 49.283748626708984
INFO:__main__:Epoch 31, Valid Loss 48.30720138549805
INFO:__main__:Epoch 32, Train Loss 49.011085510253906
INFO:__main__:Epoch 32, Valid Loss 48.027156829833984
INFO:__main__:Epoch 33, Train Loss 48.727821350097656
INFO:__main__:Epoch 33, Valid Loss 47.73183059692383
INFO:__main__:Epoch 34, Train Loss 48.4290885925293
INFO:__main__:Epoch 34, Valid Loss 47.42438507080078
INFO:__main__:Epoch 35, Train Loss 48.118080139160156
INFO:__main__:Epoch 35, Valid Loss 47.10786819458008
INFO:__main__:Saving model at epoch 35
INFO:__main__:Epoch 36, Train Loss 47.79789352416992
INFO:__main__:Epoch 36, Valid Loss 46.78763961791992
INFO:__main__:Epoch 37, Train Loss 47.473941802978516
INFO:__main__:Epoch 37, Valid Loss 46.460880279541016
INFO:__main__:Epoch 38, Train Loss 47.14337158203125
INFO:__main__:Epoch 38, Valid Loss 46.11709213256836
INFO:__main__:Epoch 39, Train Loss 46.7955436706543
INFO:__main__:Epoch 39, Valid Loss 45.759124755859375
INFO:__main__:Epoch 40, Train Loss 46.43336486816406
INFO:__main__:Epoch 40, Valid Loss 45.39278793334961
INFO:__main__:Saving model at epoch 40
INFO:__main__:Epoch 41, Train Loss 46.06269454956055
INFO:__main__:Epoch 41, Valid Loss 45.003936767578125
INFO:__main__:Epoch 42, Train Loss 45.66920852661133
INFO:__main__:Epoch 42, Valid Loss 44.59950256347656
INFO:__main__:Epoch 43, Train Loss 45.25992965698242
INFO:__main__:Epoch 43, Valid Loss 44.183258056640625
INFO:__main__:Epoch 44, Train Loss 44.83867645263672
INFO:__main__:Epoch 44, Valid Loss 43.75950622558594
INFO:__main__:Epoch 45, Train Loss 44.409812927246094
INFO:__main__:Epoch 45, Valid Loss 43.31743621826172
INFO:__main__:Saving model at epoch 45
INFO:__main__:Epoch 46, Train Loss 43.962379455566406
INFO:__main__:Epoch 46, Valid Loss 42.85389709472656
INFO:__main__:Epoch 47, Train Loss 43.49319839477539
INFO:__main__:Epoch 47, Valid Loss 42.369850158691406
INFO:__main__:Epoch 48, Train Loss 43.00321578979492
INFO:__main__:Epoch 48, Valid Loss 41.86708068847656
INFO:__main__:Epoch 49, Train Loss 42.4942512512207
INFO:__main__:Epoch 49, Valid Loss 41.34552001953125
INFO:__main__:Epoch 50, Train Loss 41.96622085571289
INFO:__main__:Epoch 50, Valid Loss 40.80565643310547
INFO:__main__:Saving model at epoch 50
INFO:__main__:Epoch 51, Train Loss 41.419612884521484
INFO:__main__:Epoch 51, Valid Loss 40.243202209472656
INFO:__main__:Epoch 52, Train Loss 40.85008239746094
INFO:__main__:Epoch 52, Valid Loss 39.6536750793457
INFO:__main__:Epoch 53, Train Loss 40.253108978271484
INFO:__main__:Epoch 53, Valid Loss 39.040279388427734
INFO:__main__:Epoch 54, Train Loss 39.63188934326172
INFO:__main__:Epoch 54, Valid Loss 38.40521240234375
INFO:__main__:Epoch 55, Train Loss 38.988651275634766
INFO:__main__:Epoch 55, Valid Loss 37.747379302978516
INFO:__main__:Saving model at epoch 55
INFO:__main__:Epoch 56, Train Loss 38.322269439697266
INFO:__main__:Epoch 56, Valid Loss 37.07095718383789
INFO:__main__:Epoch 57, Train Loss 37.63699722290039
INFO:__main__:Epoch 57, Valid Loss 36.369056701660156
INFO:__main__:Epoch 58, Train Loss 36.92580795288086
INFO:__main__:Epoch 58, Valid Loss 35.64456558227539
INFO:__main__:Epoch 59, Train Loss 36.1916389465332
INFO:__main__:Epoch 59, Valid Loss 34.896480560302734
INFO:__main__:Epoch 60, Train Loss 35.43344497680664
INFO:__main__:Epoch 60, Valid Loss 34.123783111572266
INFO:__main__:Saving model at epoch 60
INFO:__main__:Epoch 61, Train Loss 34.65018844604492
INFO:__main__:Epoch 61, Valid Loss 33.3271598815918
INFO:__main__:Epoch 62, Train Loss 33.842533111572266
INFO:__main__:Epoch 62, Valid Loss 32.506004333496094
INFO:__main__:Epoch 63, Train Loss 33.0098876953125
INFO:__main__:Epoch 63, Valid Loss 31.661972045898438
INFO:__main__:Epoch 64, Train Loss 32.153865814208984
INFO:__main__:Epoch 64, Valid Loss 30.795923233032227
INFO:__main__:Epoch 65, Train Loss 31.275346755981445
INFO:__main__:Epoch 65, Valid Loss 29.90778923034668
INFO:__main__:Saving model at epoch 65
INFO:__main__:Epoch 66, Train Loss 30.374244689941406
INFO:__main__:Epoch 66, Valid Loss 28.996044158935547
INFO:__main__:Epoch 67, Train Loss 29.4489688873291
INFO:__main__:Epoch 67, Valid Loss 28.064123153686523
INFO:__main__:Epoch 68, Train Loss 28.503002166748047
INFO:__main__:Epoch 68, Valid Loss 27.115917205810547
INFO:__main__:Epoch 69, Train Loss 27.540225982666016
INFO:__main__:Epoch 69, Valid Loss 26.14636993408203
INFO:__main__:Epoch 70, Train Loss 26.55551528930664
INFO:__main__:Epoch 70, Valid Loss 25.156112670898438
INFO:__main__:Saving model at epoch 70
INFO:__main__:Epoch 71, Train Loss 25.549463272094727
INFO:__main__:Epoch 71, Valid Loss 24.147722244262695
INFO:__main__:Epoch 72, Train Loss 24.52465057373047
INFO:__main__:Epoch 72, Valid Loss 23.125608444213867
INFO:__main__:Epoch 73, Train Loss 23.48551368713379
INFO:__main__:Epoch 73, Valid Loss 22.092601776123047
INFO:__main__:Epoch 74, Train Loss 22.43488883972168
INFO:__main__:Epoch 74, Valid Loss 21.0504207611084
INFO:__main__:Epoch 75, Train Loss 21.374500274658203
INFO:__main__:Epoch 75, Valid Loss 19.995746612548828
INFO:__main__:Saving model at epoch 75
INFO:__main__:Epoch 76, Train Loss 20.30086898803711
INFO:__main__:Epoch 76, Valid Loss 18.93706512451172
INFO:__main__:Epoch 77, Train Loss 19.222625732421875
INFO:__main__:Epoch 77, Valid Loss 17.883094787597656
INFO:__main__:Epoch 78, Train Loss 18.148597717285156
INFO:__main__:Epoch 78, Valid Loss 16.83247184753418
INFO:__main__:Epoch 79, Train Loss 17.077327728271484
INFO:__main__:Epoch 79, Valid Loss 15.78408432006836
INFO:__main__:Epoch 80, Train Loss 16.007640838623047
INFO:__main__:Epoch 80, Valid Loss 14.74267292022705
INFO:__main__:Saving model at epoch 80
INFO:__main__:Epoch 81, Train Loss 14.944297790527344
INFO:__main__:Epoch 81, Valid Loss 13.711578369140625
INFO:__main__:Epoch 82, Train Loss 13.89063549041748
INFO:__main__:Epoch 82, Valid Loss 12.694762229919434
INFO:__main__:Epoch 83, Train Loss 12.85062313079834
INFO:__main__:Epoch 83, Valid Loss 11.696045875549316
INFO:__main__:Epoch 84, Train Loss 11.828102111816406
INFO:__main__:Epoch 84, Valid Loss 10.716447830200195
INFO:__main__:Epoch 85, Train Loss 10.824018478393555
INFO:__main__:Epoch 85, Valid Loss 9.763075828552246
INFO:__main__:Saving model at epoch 85
INFO:__main__:Epoch 86, Train Loss 9.845558166503906
INFO:__main__:Epoch 86, Valid Loss 8.840493202209473
INFO:__main__:Epoch 87, Train Loss 8.897315979003906
INFO:__main__:Epoch 87, Valid Loss 7.954120635986328
INFO:__main__:Epoch 88, Train Loss 7.984767436981201
INFO:__main__:Epoch 88, Valid Loss 7.108133316040039
INFO:__main__:Epoch 89, Train Loss 7.112122535705566
INFO:__main__:Epoch 89, Valid Loss 6.307391166687012
INFO:__main__:Epoch 90, Train Loss 6.2843017578125
INFO:__main__:Epoch 90, Valid Loss 5.557705879211426
INFO:__main__:Saving model at epoch 90
INFO:__main__:Epoch 91, Train Loss 5.50723123550415
INFO:__main__:Epoch 91, Valid Loss 4.860478401184082
INFO:__main__:Epoch 92, Train Loss 4.782289028167725
INFO:__main__:Epoch 92, Valid Loss 4.221794128417969
INFO:__main__:Epoch 93, Train Loss 4.115733623504639
INFO:__main__:Epoch 93, Valid Loss 3.6440515518188477
INFO:__main__:Epoch 94, Train Loss 3.5100364685058594
INFO:__main__:Epoch 94, Valid Loss 3.1297595500946045
INFO:__main__:Epoch 95, Train Loss 2.967818260192871
INFO:__main__:Epoch 95, Valid Loss 2.6806509494781494
INFO:__main__:Saving model at epoch 95
INFO:__main__:Epoch 96, Train Loss 2.490938901901245
INFO:__main__:Epoch 96, Valid Loss 2.2974929809570312
INFO:__main__:Epoch 97, Train Loss 2.0803065299987793
INFO:__main__:Epoch 97, Valid Loss 1.9800643920898438
INFO:__main__:Epoch 98, Train Loss 1.7358616590499878
INFO:__main__:Epoch 98, Valid Loss 1.726747989654541
INFO:__main__:Epoch 99, Train Loss 1.4561506509780884
INFO:__main__:Epoch 99, Valid Loss 1.5346438884735107
INFO:__main__:R2: 0.19081892826484836
INFO:__main__:R2: 0.050336535276626404
nohup: ignoring input
INFO:__main__:name: ca9_05
INFO:__main__:base_fname: ca9_large
INFO:__main__:seed: 4
INFO:__main__:device: cuda:0
INFO:__main__:batch_size: 2048
INFO:__main__:epochs: 500
INFO:__main__:lr: 0.0003
INFO:__main__:log_interval: 1
INFO:__main__:save_interval: 5
INFO:__main__:save_path: /data02/gtguo/DEL/data/weights/refnet_ca9_transfer/
INFO:__main__:load_path: /data02/gtguo/DEL/data/weights/refnet/
INFO:__main__:target_name: ca9
INFO:__main__:fp_size: 2048
INFO:__main__:transfer_learning: True
INFO:__main__:transfer_learning_ratio: 0.1
INFO:__main__:num_workers: 8
INFO:__main__:enc_node_feat_dim: 19
INFO:__main__:enc_edge_feat_dim: 2
INFO:__main__:enc_node_embedding_size: 64
INFO:__main__:enc_edge_embedding_size: 64
INFO:__main__:enc_n_layers: 5
INFO:__main__:enc_gat_n_heads: 4
INFO:__main__:enc_gat_ffn_ratio: 4
INFO:__main__:enc_fp_embedding_size: 32
INFO:__main__:enc_fp_ffn_size: 128
INFO:__main__:enc_fp_gated: False
INFO:__main__:enc_fp_n_heads: 4
INFO:__main__:enc_fp_size: 256
INFO:__main__:enc_fp_to_gat_feedback: add
INFO:__main__:enc_gat_to_fp_pooling: mean
INFO:__main__:dec_node_input_size: 64
INFO:__main__:dec_node_emb_size: 64
INFO:__main__:dec_fp_input_size: 32
INFO:__main__:dec_fp_emb_size: 64
INFO:__main__:dec_output_size: 2
INFO:__main__:reg_input_size: 64
INFO:__main__:reg_hidden_size: 64
INFO:__main__:reg_output_size: 1
INFO:__main__:record_path: /data02/gtguo/DEL/data/records/refnet_ca9_transfer/
INFO:__main__:Model has 408772 parameters
INFO:__main__:Head has 4225 parameters
Splitting dataset:   0%|          | 0/2372674 [00:00<?, ?it/s]Splitting dataset:   1%|          | 17978/2372674 [00:00<00:13, 179768.60it/s]Splitting dataset:   2%|▏         | 36647/2372674 [00:00<00:12, 183814.23it/s]Splitting dataset:   2%|▏         | 55473/2372674 [00:00<00:12, 185840.72it/s]Splitting dataset:   3%|▎         | 74058/2372674 [00:00<00:12, 185359.46it/s]Splitting dataset:   4%|▍         | 93202/2372674 [00:00<00:12, 187541.70it/s]Splitting dataset:   5%|▍         | 112132/2372674 [00:00<00:12, 188133.41it/s]Splitting dataset:   6%|▌         | 131491/2372674 [00:00<00:11, 189913.13it/s]Splitting dataset:   6%|▋         | 150836/2372674 [00:00<00:11, 191034.04it/s]Splitting dataset:   7%|▋         | 169940/2372674 [00:00<00:11, 189628.95it/s]Splitting dataset:   8%|▊         | 188906/2372674 [00:01<00:12, 181976.98it/s]Splitting dataset:   9%|▉         | 207729/2372674 [00:01<00:11, 183827.21it/s]Splitting dataset:  10%|▉         | 226739/2372674 [00:01<00:11, 185690.27it/s]Splitting dataset:  10%|█         | 246889/2372674 [00:01<00:11, 190399.79it/s]Splitting dataset:  11%|█         | 266640/2372674 [00:01<00:10, 192521.69it/s]Splitting dataset:  12%|█▏        | 286313/2372674 [00:01<00:10, 193778.02it/s]Splitting dataset:  13%|█▎        | 306812/2372674 [00:01<00:10, 197130.34it/s]Splitting dataset:  14%|█▍        | 326541/2372674 [00:01<00:10, 196200.05it/s]Splitting dataset:  15%|█▍        | 346173/2372674 [00:01<00:10, 193443.93it/s]Splitting dataset:  15%|█▌        | 365533/2372674 [00:01<00:10, 191800.11it/s]Splitting dataset:  16%|█▌        | 384725/2372674 [00:02<00:10, 189620.86it/s]Splitting dataset:  17%|█▋        | 404287/2372674 [00:02<00:10, 191380.55it/s]Splitting dataset:  18%|█▊        | 423879/2372674 [00:02<00:10, 192722.29it/s]Splitting dataset:  19%|█▊        | 444305/2372674 [00:02<00:09, 196149.49it/s]Splitting dataset:  20%|█▉        | 463930/2372674 [00:02<00:09, 196169.04it/s]Splitting dataset:  20%|██        | 485252/2372674 [00:02<00:09, 201258.48it/s]Splitting dataset:  21%|██▏       | 505386/2372674 [00:02<00:09, 200189.25it/s]Splitting dataset:  22%|██▏       | 525412/2372674 [00:02<00:09, 200140.69it/s]Splitting dataset:  23%|██▎       | 545431/2372674 [00:02<00:09, 197364.71it/s]Splitting dataset:  24%|██▍       | 565179/2372674 [00:02<00:09, 195602.98it/s]Splitting dataset:  25%|██▍       | 584749/2372674 [00:03<00:09, 190308.83it/s]Splitting dataset:  26%|██▌       | 605805/2372674 [00:03<00:09, 196199.17it/s]Splitting dataset:  26%|██▋       | 625465/2372674 [00:03<00:08, 194815.37it/s]Splitting dataset:  27%|██▋       | 644974/2372674 [00:03<00:08, 194177.12it/s]Splitting dataset:  28%|██▊       | 664632/2372674 [00:03<00:08, 194880.98it/s]Splitting dataset:  29%|██▉       | 684590/2372674 [00:03<00:08, 196271.00it/s]Splitting dataset:  30%|██▉       | 705510/2372674 [00:03<00:08, 200114.03it/s]Splitting dataset:  31%|███       | 726110/2372674 [00:03<00:08, 201867.38it/s]Splitting dataset:  31%|███▏      | 746820/2372674 [00:03<00:07, 203426.71it/s]Splitting dataset:  32%|███▏      | 767630/2372674 [00:03<00:07, 204814.90it/s]Splitting dataset:  33%|███▎      | 788481/2372674 [00:04<00:07, 205917.38it/s]Splitting dataset:  34%|███▍      | 809077/2372674 [00:04<00:07, 205397.40it/s]Splitting dataset:  35%|███▍      | 829718/2372674 [00:04<00:07, 205697.89it/s]Splitting dataset:  36%|███▌      | 850711/2372674 [00:04<00:07, 206959.73it/s]Splitting dataset:  37%|███▋      | 871409/2372674 [00:04<00:07, 206631.29it/s]Splitting dataset:  38%|███▊      | 892105/2372674 [00:04<00:07, 206726.21it/s]Splitting dataset:  38%|███▊      | 912910/2372674 [00:04<00:07, 207119.11it/s]Splitting dataset:  39%|███▉      | 934101/2372674 [00:04<00:06, 208552.23it/s]Splitting dataset:  40%|████      | 954957/2372674 [00:04<00:06, 207589.19it/s]Splitting dataset:  41%|████      | 975718/2372674 [00:04<00:06, 206138.36it/s]Splitting dataset:  42%|████▏     | 996335/2372674 [00:05<00:06, 202970.19it/s]Splitting dataset:  43%|████▎     | 1016643/2372674 [00:05<00:06, 197130.61it/s]Splitting dataset:  44%|████▎     | 1036393/2372674 [00:05<00:06, 195597.47it/s]Splitting dataset:  45%|████▍     | 1056309/2372674 [00:05<00:06, 196632.00it/s]Splitting dataset:  45%|████▌     | 1077153/2372674 [00:05<00:06, 200099.63it/s]Splitting dataset:  46%|████▌     | 1097184/2372674 [00:05<00:06, 199309.97it/s]Splitting dataset:  47%|████▋     | 1117130/2372674 [00:05<00:06, 196370.57it/s]Splitting dataset:  48%|████▊     | 1137990/2372674 [00:05<00:06, 199965.92it/s]Splitting dataset:  49%|████▉     | 1158998/2372674 [00:05<00:05, 202956.01it/s]Splitting dataset:  50%|████▉     | 1179311/2372674 [00:05<00:05, 202328.57it/s]Splitting dataset:  51%|█████     | 1199556/2372674 [00:06<00:05, 202063.84it/s]Splitting dataset:  51%|█████▏    | 1220837/2372674 [00:06<00:05, 205260.44it/s]Splitting dataset:  52%|█████▏    | 1241372/2372674 [00:06<00:05, 202248.55it/s]Splitting dataset:  53%|█████▎    | 1261612/2372674 [00:06<00:05, 200486.65it/s]Splitting dataset:  54%|█████▍    | 1282134/2372674 [00:06<00:05, 201880.85it/s]Splitting dataset:  55%|█████▍    | 1302333/2372674 [00:06<00:05, 201746.79it/s]Splitting dataset:  56%|█████▌    | 1322694/2372674 [00:06<00:05, 202298.03it/s]Splitting dataset:  57%|█████▋    | 1342930/2372674 [00:06<00:05, 200477.05it/s]Splitting dataset:  57%|█████▋    | 1363243/2372674 [00:06<00:05, 201261.02it/s]Splitting dataset:  58%|█████▊    | 1383624/2372674 [00:06<00:04, 202016.21it/s]Splitting dataset:  59%|█████▉    | 1403830/2372674 [00:07<00:04, 201212.00it/s]Splitting dataset:  60%|██████    | 1423955/2372674 [00:07<00:04, 191845.32it/s]Splitting dataset:  61%|██████    | 1443855/2372674 [00:07<00:04, 193908.37it/s]Splitting dataset:  62%|██████▏   | 1464235/2372674 [00:07<00:04, 196794.11it/s]Splitting dataset:  63%|██████▎   | 1483972/2372674 [00:07<00:04, 196914.90it/s]Splitting dataset:  63%|██████▎   | 1504551/2372674 [00:07<00:04, 199539.58it/s]Splitting dataset:  64%|██████▍   | 1526208/2372674 [00:07<00:04, 204597.31it/s]Splitting dataset:  65%|██████▌   | 1547709/2372674 [00:07<00:03, 207698.57it/s]Splitting dataset:  66%|██████▌   | 1569551/2372674 [00:07<00:03, 210896.49it/s]Splitting dataset:  67%|██████▋   | 1590657/2372674 [00:08<00:03, 210794.71it/s]Splitting dataset:  68%|██████▊   | 1611748/2372674 [00:08<00:03, 208045.32it/s]Splitting dataset:  69%|██████▉   | 1633401/2372674 [00:08<00:03, 210558.19it/s]Splitting dataset:  70%|██████▉   | 1655082/2372674 [00:08<00:03, 212416.42it/s]Splitting dataset:  71%|███████   | 1676621/2372674 [00:08<00:03, 213300.57it/s]Splitting dataset:  72%|███████▏  | 1697960/2372674 [00:08<00:03, 210783.97it/s]Splitting dataset:  72%|███████▏  | 1719051/2372674 [00:08<00:03, 203751.59it/s]Splitting dataset:  73%|███████▎  | 1739479/2372674 [00:08<00:03, 201137.61it/s]Splitting dataset:  74%|███████▍  | 1759630/2372674 [00:08<00:03, 199904.96it/s]Splitting dataset:  75%|███████▌  | 1779645/2372674 [00:08<00:02, 198741.89it/s]Splitting dataset:  76%|███████▌  | 1799535/2372674 [00:09<00:02, 195214.70it/s]Splitting dataset:  77%|███████▋  | 1819074/2372674 [00:09<00:02, 193426.12it/s]Splitting dataset:  77%|███████▋  | 1838428/2372674 [00:09<00:02, 188243.15it/s]Splitting dataset:  78%|███████▊  | 1857279/2372674 [00:09<00:02, 180256.19it/s]Splitting dataset:  79%|███████▉  | 1877462/2372674 [00:09<00:02, 186358.01it/s]Splitting dataset:  80%|███████▉  | 1896907/2372674 [00:09<00:02, 188686.08it/s]Splitting dataset:  81%|████████  | 1916109/2372674 [00:09<00:02, 189655.58it/s]Splitting dataset:  82%|████████▏ | 1935125/2372674 [00:09<00:02, 189359.35it/s]Splitting dataset:  82%|████████▏ | 1954556/2372674 [00:09<00:02, 190817.55it/s]Splitting dataset:  83%|████████▎ | 1973665/2372674 [00:09<00:02, 190117.46it/s]Splitting dataset:  84%|████████▍ | 1993193/2372674 [00:10<00:01, 191648.08it/s]Splitting dataset:  85%|████████▍ | 2013232/2372674 [00:10<00:01, 194248.78it/s]Splitting dataset:  86%|████████▌ | 2033067/2372674 [00:10<00:01, 195468.81it/s]Splitting dataset:  87%|████████▋ | 2052771/2372674 [00:10<00:01, 195934.42it/s]Splitting dataset:  87%|████████▋ | 2072657/2372674 [00:10<00:01, 196807.89it/s]Splitting dataset:  88%|████████▊ | 2092343/2372674 [00:10<00:01, 195272.44it/s]Splitting dataset:  89%|████████▉ | 2111877/2372674 [00:10<00:01, 193594.46it/s]Splitting dataset:  90%|████████▉ | 2131243/2372674 [00:10<00:01, 193155.21it/s]Splitting dataset:  91%|█████████ | 2150563/2372674 [00:10<00:01, 193100.30it/s]Splitting dataset:  91%|█████████▏| 2170518/2372674 [00:10<00:01, 195018.16it/s]Splitting dataset:  92%|█████████▏| 2192191/2372674 [00:11<00:00, 201498.63it/s]Splitting dataset:  93%|█████████▎| 2213959/2372674 [00:11<00:00, 206334.08it/s]Splitting dataset:  94%|█████████▍| 2235875/2372674 [00:11<00:00, 210168.52it/s]Splitting dataset:  95%|█████████▌| 2256897/2372674 [00:11<00:00, 208954.97it/s]Splitting dataset:  96%|█████████▌| 2277798/2372674 [00:11<00:00, 205326.40it/s]Splitting dataset:  97%|█████████▋| 2299439/2372674 [00:11<00:00, 208594.29it/s]Splitting dataset:  98%|█████████▊| 2320315/2372674 [00:11<00:00, 203345.20it/s]Splitting dataset:  99%|█████████▊| 2340686/2372674 [00:11<00:00, 197139.83it/s]Splitting dataset:  99%|█████████▉| 2360454/2372674 [00:11<00:00, 194063.51it/s]Splitting dataset: 100%|██████████| 2372674/2372674 [00:11<00:00, 198008.57it/s]
INFO:root:processing dataset...
  0%|          | 0/5453 [00:00<?, ?it/s]  1%|          | 53/5453 [00:00<00:10, 528.55it/s]  2%|▏         | 106/5453 [00:00<00:10, 523.52it/s]  3%|▎         | 169/5453 [00:00<00:09, 567.75it/s]  4%|▍         | 226/5453 [00:00<00:09, 546.56it/s]/data02/gtguo/DEL/pkg/utils/mol_feat.py:71: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matricesor `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484808560/work/aten/src/ATen/native/TensorShape.cpp:2981.)
  return torch.tensor(coo, dtype=torch.long).T
  5%|▌         | 297/5453 [00:00<00:08, 599.41it/s]  7%|▋         | 358/5453 [00:00<00:08, 569.14it/s]  8%|▊         | 424/5453 [00:00<00:08, 595.37it/s]  9%|▉         | 484/5453 [00:00<00:08, 562.75it/s] 10%|▉         | 541/5453 [00:00<00:08, 559.79it/s] 11%|█         | 601/5453 [00:01<00:08, 570.12it/s] 12%|█▏        | 659/5453 [00:01<00:08, 547.39it/s] 13%|█▎        | 715/5453 [00:01<00:09, 525.14it/s] 14%|█▍        | 768/5453 [00:01<00:09, 512.72it/s] 15%|█▌        | 820/5453 [00:01<00:09, 501.93it/s] 16%|█▌        | 871/5453 [00:01<00:09, 489.67it/s] 17%|█▋        | 922/5453 [00:01<00:09, 493.86it/s] 18%|█▊        | 972/5453 [00:01<00:09, 491.63it/s] 19%|█▉        | 1030/5453 [00:01<00:08, 513.61it/s] 20%|█▉        | 1082/5453 [00:02<00:08, 513.40it/s] 21%|██        | 1137/5453 [00:02<00:08, 523.46it/s] 22%|██▏       | 1193/5453 [00:02<00:07, 532.78it/s] 23%|██▎       | 1247/5453 [00:02<00:07, 527.56it/s] 24%|██▍       | 1300/5453 [00:02<00:08, 496.14it/s] 25%|██▍       | 1357/5453 [00:02<00:07, 516.27it/s] 26%|██▌       | 1413/5453 [00:02<00:07, 528.33it/s] 27%|██▋       | 1468/5453 [00:02<00:07, 533.93it/s] 28%|██▊       | 1522/5453 [00:02<00:07, 530.03it/s] 29%|██▉       | 1582/5453 [00:02<00:07, 550.32it/s] 30%|███       | 1638/5453 [00:03<00:07, 544.39it/s] 31%|███       | 1695/5453 [00:03<00:06, 550.80it/s] 32%|███▏      | 1752/5453 [00:03<00:06, 553.89it/s] 33%|███▎      | 1808/5453 [00:03<00:06, 536.80it/s] 34%|███▍      | 1862/5453 [00:03<00:06, 516.78it/s] 35%|███▌      | 1914/5453 [00:03<00:07, 480.17it/s] 36%|███▌      | 1968/5453 [00:03<00:07, 494.45it/s] 37%|███▋      | 2024/5453 [00:03<00:06, 510.84it/s] 38%|███▊      | 2077/5453 [00:03<00:06, 516.24it/s] 39%|███▉      | 2130/5453 [00:04<00:06, 518.00it/s] 40%|████      | 2189/5453 [00:04<00:06, 538.48it/s] 41%|████      | 2244/5453 [00:04<00:06, 532.57it/s] 42%|████▏     | 2298/5453 [00:04<00:06, 524.21it/s] 43%|████▎     | 2351/5453 [00:04<00:05, 517.40it/s] 44%|████▍     | 2409/5453 [00:04<00:05, 534.52it/s] 45%|████▌     | 2463/5453 [00:04<00:05, 530.49it/s] 46%|████▌     | 2517/5453 [00:04<00:05, 513.18it/s] 47%|████▋     | 2569/5453 [00:04<00:05, 502.55it/s] 48%|████▊     | 2627/5453 [00:04<00:05, 523.99it/s] 49%|████▉     | 2680/5453 [00:05<00:05, 499.51it/s] 50%|█████     | 2731/5453 [00:05<00:05, 502.23it/s] 51%|█████     | 2787/5453 [00:05<00:05, 517.79it/s] 52%|█████▏    | 2843/5453 [00:05<00:04, 527.79it/s] 53%|█████▎    | 2896/5453 [00:05<00:05, 508.03it/s] 54%|█████▍    | 2948/5453 [00:05<00:05, 466.26it/s] 55%|█████▌    | 3001/5453 [00:05<00:05, 481.82it/s] 56%|█████▌    | 3052/5453 [00:05<00:04, 488.39it/s] 57%|█████▋    | 3103/5453 [00:05<00:04, 494.09it/s] 58%|█████▊    | 3154/5453 [00:06<00:04, 497.01it/s] 59%|█████▉    | 3204/5453 [00:06<00:04, 493.94it/s] 60%|█████▉    | 3254/5453 [00:06<00:04, 483.31it/s] 61%|██████    | 3306/5453 [00:06<00:04, 493.69it/s] 62%|██████▏   | 3358/5453 [00:06<00:04, 500.07it/s] 63%|██████▎   | 3410/5453 [00:06<00:04, 503.22it/s] 63%|██████▎   | 3461/5453 [00:06<00:03, 503.28it/s] 64%|██████▍   | 3514/5453 [00:06<00:03, 508.81it/s] 65%|██████▌   | 3567/5453 [00:06<00:03, 513.00it/s] 66%|██████▋   | 3619/5453 [00:06<00:03, 506.49it/s] 67%|██████▋   | 3671/5453 [00:07<00:03, 509.06it/s] 68%|██████▊   | 3722/5453 [00:07<00:03, 506.67it/s] 69%|██████▉   | 3773/5453 [00:07<00:05, 319.40it/s] 70%|███████   | 3821/5453 [00:07<00:04, 350.47it/s] 71%|███████   | 3866/5453 [00:07<00:04, 371.48it/s] 72%|███████▏  | 3915/5453 [00:07<00:03, 399.97it/s] 73%|███████▎  | 3961/5453 [00:07<00:03, 414.27it/s] 74%|███████▎  | 4009/5453 [00:07<00:03, 431.20it/s] 74%|███████▍  | 4055/5453 [00:08<00:03, 438.56it/s] 75%|███████▌  | 4102/5453 [00:08<00:03, 446.77it/s] 76%|███████▌  | 4149/5453 [00:08<00:02, 440.24it/s] 77%|███████▋  | 4194/5453 [00:08<00:02, 428.81it/s] 78%|███████▊  | 4242/5453 [00:08<00:02, 440.74it/s] 79%|███████▊  | 4293/5453 [00:08<00:02, 460.46it/s] 80%|███████▉  | 4342/5453 [00:08<00:02, 468.27it/s] 81%|████████  | 4393/5453 [00:08<00:02, 477.48it/s] 81%|████████▏ | 4442/5453 [00:08<00:02, 476.11it/s] 82%|████████▏ | 4490/5453 [00:09<00:02, 473.27it/s] 83%|████████▎ | 4538/5453 [00:09<00:01, 472.66it/s] 84%|████████▍ | 4586/5453 [00:09<00:01, 462.76it/s] 85%|████████▍ | 4634/5453 [00:09<00:01, 467.73it/s] 86%|████████▌ | 4681/5453 [00:09<00:01, 466.91it/s] 87%|████████▋ | 4728/5453 [00:09<00:01, 466.68it/s] 88%|████████▊ | 4775/5453 [00:09<00:01, 462.32it/s] 88%|████████▊ | 4822/5453 [00:09<00:01, 452.69it/s] 89%|████████▉ | 4868/5453 [00:09<00:01, 447.65it/s] 90%|█████████ | 4915/5453 [00:09<00:01, 452.49it/s] 91%|█████████ | 4964/5453 [00:10<00:01, 462.24it/s] 92%|█████████▏| 5014/5453 [00:10<00:00, 472.17it/s] 93%|█████████▎| 5063/5453 [00:10<00:00, 475.64it/s] 94%|█████████▍| 5114/5453 [00:10<00:00, 485.48it/s] 95%|█████████▍| 5164/5453 [00:10<00:00, 488.75it/s] 96%|█████████▌| 5215/5453 [00:10<00:00, 493.03it/s] 97%|█████████▋| 5265/5453 [00:10<00:00, 484.24it/s] 97%|█████████▋| 5314/5453 [00:10<00:00, 476.10it/s] 98%|█████████▊| 5363/5453 [00:10<00:00, 478.27it/s] 99%|█████████▉| 5411/5453 [00:10<00:00, 476.28it/s]100%|██████████| 5453/5453 [00:11<00:00, 491.57it/s]
INFO:__main__:Dataset has 5453 samples
INFO:__main__:Dataset data example: {'pyg_data': Data(x=[22, 19], edge_index=[2, 50], edge_attr=[50, 2]), 'activity': 5.028260409112222}
INFO:__main__:Train loader has 1 data
/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([272])) that is different to the input size (torch.Size([272, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([273])) that is different to the input size (torch.Size([273, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:__main__:Epoch 0, Train Loss 56.164390563964844
INFO:__main__:Epoch 0, Valid Loss 53.22325134277344
INFO:__main__:Saving model at epoch 0
INFO:__main__:Epoch 1, Train Loss 55.79203414916992
INFO:__main__:Epoch 1, Valid Loss 52.87705612182617
INFO:__main__:Epoch 2, Train Loss 55.436893463134766
INFO:__main__:Epoch 2, Valid Loss 52.54868698120117
INFO:__main__:Epoch 3, Train Loss 55.10005187988281
INFO:__main__:Epoch 3, Valid Loss 52.247039794921875
INFO:__main__:Epoch 4, Train Loss 54.790592193603516
INFO:__main__:Epoch 4, Valid Loss 51.95801544189453
INFO:__main__:Epoch 5, Train Loss 54.49402618408203
INFO:__main__:Epoch 5, Valid Loss 51.6796760559082
INFO:__main__:Saving model at epoch 5
INFO:__main__:Epoch 6, Train Loss 54.2083854675293
INFO:__main__:Epoch 6, Valid Loss 51.41720962524414
INFO:__main__:Epoch 7, Train Loss 53.93906784057617
INFO:__main__:Epoch 7, Valid Loss 51.15970230102539
INFO:__main__:Epoch 8, Train Loss 53.67479705810547
INFO:__main__:Epoch 8, Valid Loss 50.92185592651367
INFO:__main__:Epoch 9, Train Loss 53.43064880371094
INFO:__main__:Epoch 9, Valid Loss 50.71546936035156
INFO:__main__:Epoch 10, Train Loss 53.218807220458984
INFO:__main__:Epoch 10, Valid Loss 50.530887603759766
INFO:__main__:Saving model at epoch 10
INFO:__main__:Epoch 11, Train Loss 53.02934646606445
INFO:__main__:Epoch 11, Valid Loss 50.35842514038086
INFO:__main__:Epoch 12, Train Loss 52.85232162475586
INFO:__main__:Epoch 12, Valid Loss 50.195926666259766
INFO:__main__:Epoch 13, Train Loss 52.68552017211914
INFO:__main__:Epoch 13, Valid Loss 50.03727722167969
INFO:__main__:Epoch 14, Train Loss 52.52264404296875
INFO:__main__:Epoch 14, Valid Loss 49.87532043457031
INFO:__main__:Epoch 15, Train Loss 52.35637664794922
INFO:__main__:Epoch 15, Valid Loss 49.71573257446289
INFO:__main__:Saving model at epoch 15
INFO:__main__:Epoch 16, Train Loss 52.1925163269043
INFO:__main__:Epoch 16, Valid Loss 49.56751251220703
INFO:__main__:Epoch 17, Train Loss 52.04036331176758
INFO:__main__:Epoch 17, Valid Loss 49.41763687133789
INFO:__main__:Epoch 18, Train Loss 51.886470794677734
INFO:__main__:Epoch 18, Valid Loss 49.259803771972656
INFO:__main__:Epoch 19, Train Loss 51.724403381347656
INFO:__main__:Epoch 19, Valid Loss 49.093292236328125
INFO:__main__:Epoch 20, Train Loss 51.55341720581055
INFO:__main__:Epoch 20, Valid Loss 48.91898727416992
INFO:__main__:Saving model at epoch 20
INFO:__main__:Epoch 21, Train Loss 51.374420166015625
INFO:__main__:Epoch 21, Valid Loss 48.73604202270508
INFO:__main__:Epoch 22, Train Loss 51.1865348815918
INFO:__main__:Epoch 22, Valid Loss 48.542945861816406
INFO:__main__:Epoch 23, Train Loss 50.988243103027344
INFO:__main__:Epoch 23, Valid Loss 48.339210510253906
INFO:__main__:Epoch 24, Train Loss 50.77899932861328
INFO:__main__:Epoch 24, Valid Loss 48.12185287475586
INFO:__main__:Epoch 25, Train Loss 50.55574417114258
INFO:__main__:Epoch 25, Valid Loss 47.894046783447266
INFO:__main__:Saving model at epoch 25
INFO:__main__:Epoch 26, Train Loss 50.32172775268555
INFO:__main__:Epoch 26, Valid Loss 47.66444778442383
INFO:__main__:Epoch 27, Train Loss 50.08587646484375
INFO:__main__:Epoch 27, Valid Loss 47.43199920654297
INFO:__main__:Epoch 28, Train Loss 49.84705352783203
INFO:__main__:Epoch 28, Valid Loss 47.19490051269531
INFO:__main__:Epoch 29, Train Loss 49.60344314575195
INFO:__main__:Epoch 29, Valid Loss 46.95220947265625
INFO:__main__:Epoch 30, Train Loss 49.35407257080078
INFO:__main__:Epoch 30, Valid Loss 46.69717788696289
INFO:__main__:Saving model at epoch 30
INFO:__main__:Epoch 31, Train Loss 49.092002868652344
INFO:__main__:Epoch 31, Valid Loss 46.43276596069336
INFO:__main__:Epoch 32, Train Loss 48.820281982421875
INFO:__main__:Epoch 32, Valid Loss 46.158599853515625
INFO:__main__:Epoch 33, Train Loss 48.538509368896484
INFO:__main__:Epoch 33, Valid Loss 45.86952209472656
INFO:__main__:Epoch 34, Train Loss 48.24138641357422
INFO:__main__:Epoch 34, Valid Loss 45.568546295166016
INFO:__main__:Epoch 35, Train Loss 47.93199920654297
INFO:__main__:Epoch 35, Valid Loss 45.25873947143555
INFO:__main__:Saving model at epoch 35
INFO:__main__:Epoch 36, Train Loss 47.6135139465332
INFO:__main__:Epoch 36, Valid Loss 44.94501495361328
INFO:__main__:Epoch 37, Train Loss 47.290950775146484
INFO:__main__:Epoch 37, Valid Loss 44.624916076660156
INFO:__main__:Epoch 38, Train Loss 46.9618034362793
INFO:__main__:Epoch 38, Valid Loss 44.28803253173828
INFO:__main__:Epoch 39, Train Loss 46.615379333496094
INFO:__main__:Epoch 39, Valid Loss 43.938194274902344
INFO:__main__:Epoch 40, Train Loss 46.255550384521484
INFO:__main__:Epoch 40, Valid Loss 43.579341888427734
INFO:__main__:Saving model at epoch 40
INFO:__main__:Epoch 41, Train Loss 45.88644790649414
INFO:__main__:Epoch 41, Valid Loss 43.198875427246094
INFO:__main__:Epoch 42, Train Loss 45.495079040527344
INFO:__main__:Epoch 42, Valid Loss 42.80290222167969
INFO:__main__:Epoch 43, Train Loss 45.087711334228516
INFO:__main__:Epoch 43, Valid Loss 42.39699935913086
INFO:__main__:Epoch 44, Train Loss 44.67012023925781
INFO:__main__:Epoch 44, Valid Loss 41.98225021362305
INFO:__main__:Epoch 45, Train Loss 44.24330139160156
INFO:__main__:Epoch 45, Valid Loss 41.548011779785156
INFO:__main__:Saving model at epoch 45
INFO:__main__:Epoch 46, Train Loss 43.796356201171875
INFO:__main__:Epoch 46, Valid Loss 41.093929290771484
INFO:__main__:Epoch 47, Train Loss 43.328914642333984
INFO:__main__:Epoch 47, Valid Loss 40.619022369384766
INFO:__main__:Epoch 48, Train Loss 42.83994674682617
INFO:__main__:Epoch 48, Valid Loss 40.12826156616211
INFO:__main__:Epoch 49, Train Loss 42.3345947265625
INFO:__main__:Epoch 49, Valid Loss 39.61795425415039
INFO:__main__:Epoch 50, Train Loss 41.80898666381836
INFO:__main__:Epoch 50, Valid Loss 39.08963394165039
INFO:__main__:Saving model at epoch 50
INFO:__main__:Epoch 51, Train Loss 41.26472473144531
INFO:__main__:Epoch 51, Valid Loss 38.53982925415039
INFO:__main__:Epoch 52, Train Loss 40.69820022583008
INFO:__main__:Epoch 52, Valid Loss 37.964134216308594
INFO:__main__:Epoch 53, Train Loss 40.1048698425293
INFO:__main__:Epoch 53, Valid Loss 37.36430358886719
INFO:__main__:Epoch 54, Train Loss 39.486534118652344
INFO:__main__:Epoch 54, Valid Loss 36.74411392211914
INFO:__main__:Epoch 55, Train Loss 38.8470344543457
INFO:__main__:Epoch 55, Valid Loss 36.102378845214844
INFO:__main__:Saving model at epoch 55
INFO:__main__:Epoch 56, Train Loss 38.18515396118164
INFO:__main__:Epoch 56, Valid Loss 35.441566467285156
INFO:__main__:Epoch 57, Train Loss 37.50338363647461
INFO:__main__:Epoch 57, Valid Loss 34.75459289550781
INFO:__main__:Epoch 58, Train Loss 36.794437408447266
INFO:__main__:Epoch 58, Valid Loss 34.04740524291992
INFO:__main__:Epoch 59, Train Loss 36.06439208984375
INFO:__main__:Epoch 59, Valid Loss 33.316165924072266
INFO:__main__:Epoch 60, Train Loss 35.30923843383789
INFO:__main__:Epoch 60, Valid Loss 32.563262939453125
INFO:__main__:Saving model at epoch 60
INFO:__main__:Epoch 61, Train Loss 34.53142547607422
INFO:__main__:Epoch 61, Valid Loss 31.786666870117188
INFO:__main__:Epoch 62, Train Loss 33.72882843017578
INFO:__main__:Epoch 62, Valid Loss 30.986919403076172
INFO:__main__:Epoch 63, Train Loss 32.901954650878906
INFO:__main__:Epoch 63, Valid Loss 30.165462493896484
INFO:__main__:Epoch 64, Train Loss 32.052249908447266
INFO:__main__:Epoch 64, Valid Loss 29.32206916809082
INFO:__main__:Epoch 65, Train Loss 31.179431915283203
INFO:__main__:Epoch 65, Valid Loss 28.456674575805664
INFO:__main__:Saving model at epoch 65
INFO:__main__:Epoch 66, Train Loss 30.28338050842285
INFO:__main__:Epoch 66, Valid Loss 27.57083511352539
INFO:__main__:Epoch 67, Train Loss 29.36564064025879
INFO:__main__:Epoch 67, Valid Loss 26.662952423095703
INFO:__main__:Epoch 68, Train Loss 28.424510955810547
INFO:__main__:Epoch 68, Valid Loss 25.741561889648438
INFO:__main__:Epoch 69, Train Loss 27.46877098083496
INFO:__main__:Epoch 69, Valid Loss 24.799415588378906
INFO:__main__:Epoch 70, Train Loss 26.49081039428711
INFO:__main__:Epoch 70, Valid Loss 23.838048934936523
INFO:__main__:Saving model at epoch 70
INFO:__main__:Epoch 71, Train Loss 25.49214744567871
INFO:__main__:Epoch 71, Valid Loss 22.859477996826172
INFO:__main__:Epoch 72, Train Loss 24.47479248046875
INFO:__main__:Epoch 72, Valid Loss 21.868776321411133
INFO:__main__:Epoch 73, Train Loss 23.443910598754883
INFO:__main__:Epoch 73, Valid Loss 20.86811637878418
INFO:__main__:Epoch 74, Train Loss 22.40167808532715
INFO:__main__:Epoch 74, Valid Loss 19.857524871826172
INFO:__main__:Epoch 75, Train Loss 21.348005294799805
INFO:__main__:Epoch 75, Valid Loss 18.839384078979492
INFO:__main__:Saving model at epoch 75
INFO:__main__:Epoch 76, Train Loss 20.28525733947754
INFO:__main__:Epoch 76, Valid Loss 17.816341400146484
INFO:__main__:Epoch 77, Train Loss 19.21607208251953
INFO:__main__:Epoch 77, Valid Loss 16.799283981323242
INFO:__main__:Epoch 78, Train Loss 18.151704788208008
INFO:__main__:Epoch 78, Valid Loss 15.7859525680542
INFO:__main__:Epoch 79, Train Loss 17.08965492248535
INFO:__main__:Epoch 79, Valid Loss 14.776748657226562
INFO:__main__:Epoch 80, Train Loss 16.030202865600586
INFO:__main__:Epoch 80, Valid Loss 13.775257110595703
INFO:__main__:Saving model at epoch 80
INFO:__main__:Epoch 81, Train Loss 14.976946830749512
INFO:__main__:Epoch 81, Valid Loss 12.785236358642578
INFO:__main__:Epoch 82, Train Loss 13.933664321899414
INFO:__main__:Epoch 82, Valid Loss 11.810696601867676
INFO:__main__:Epoch 83, Train Loss 12.904397964477539
INFO:__main__:Epoch 83, Valid Loss 10.851767539978027
INFO:__main__:Epoch 84, Train Loss 11.889076232910156
INFO:__main__:Epoch 84, Valid Loss 9.916489601135254
INFO:__main__:Epoch 85, Train Loss 10.89599323272705
INFO:__main__:Epoch 85, Valid Loss 9.008186340332031
INFO:__main__:Saving model at epoch 85
INFO:__main__:Epoch 86, Train Loss 9.92845630645752
INFO:__main__:Epoch 86, Valid Loss 8.13184642791748
INFO:__main__:Epoch 87, Train Loss 8.991556167602539
INFO:__main__:Epoch 87, Valid Loss 7.29263973236084
INFO:__main__:Epoch 88, Train Loss 8.090580940246582
INFO:__main__:Epoch 88, Valid Loss 6.495439052581787
INFO:__main__:Epoch 89, Train Loss 7.230536460876465
INFO:__main__:Epoch 89, Valid Loss 5.744279384613037
INFO:__main__:Epoch 90, Train Loss 6.415556907653809
INFO:__main__:Epoch 90, Valid Loss 5.044371128082275
INFO:__main__:Saving model at epoch 90
INFO:__main__:Epoch 91, Train Loss 5.651096820831299
INFO:__main__:Epoch 91, Valid Loss 4.398005962371826
INFO:__main__:Epoch 92, Train Loss 4.939471244812012
INFO:__main__:Epoch 92, Valid Loss 3.810112714767456
INFO:__main__:Epoch 93, Train Loss 4.285955429077148
INFO:__main__:Epoch 93, Valid Loss 3.2831625938415527
INFO:__main__:Epoch 94, Train Loss 3.693211317062378
INFO:__main__:Epoch 94, Valid Loss 2.8194351196289062
INFO:__main__:Epoch 95, Train Loss 3.1637911796569824
INFO:__main__:Epoch 95, Valid Loss 2.4203548431396484
INFO:__main__:Saving model at epoch 95
INFO:__main__:Epoch 96, Train Loss 2.6994245052337646
INFO:__main__:Epoch 96, Valid Loss 2.0863537788391113
INFO:__main__:Epoch 97, Train Loss 2.300877571105957
INFO:__main__:Epoch 97, Valid Loss 1.8168878555297852
INFO:__main__:Epoch 98, Train Loss 1.968011736869812
INFO:__main__:Epoch 98, Valid Loss 1.6098419427871704
INFO:__main__:Epoch 99, Train Loss 1.6990671157836914
INFO:__main__:Epoch 99, Valid Loss 1.4620100259780884
INFO:__main__:Epoch 100, Train Loss 1.4912439584732056
INFO:__main__:Epoch 100, Valid Loss 1.3689439296722412
INFO:__main__:Saving model at epoch 100
INFO:__main__:Epoch 101, Train Loss 1.3405230045318604
INFO:__main__:Epoch 101, Valid Loss 1.3249791860580444
INFO:__main__:Epoch 102, Train Loss 1.2416759729385376
INFO:__main__:Epoch 102, Valid Loss 1.3233697414398193
INFO:__main__:Epoch 103, Train Loss 1.1883896589279175
INFO:__main__:Epoch 103, Valid Loss 1.3565021753311157
INFO:__main__:Epoch 104, Train Loss 1.1734665632247925
INFO:__main__:Epoch 104, Valid Loss 1.4161876440048218
INFO:__main__:Epoch 105, Train Loss 1.1891025304794312
INFO:__main__:Epoch 105, Valid Loss 1.4940105676651
INFO:__main__:Epoch 106, Train Loss 1.2272225618362427
INFO:__main__:Epoch 106, Valid Loss 1.5816906690597534
INFO:__main__:Epoch 107, Train Loss 1.2798360586166382
INFO:__main__:Epoch 107, Valid Loss 1.6714805364608765
INFO:__main__:Epoch 108, Train Loss 1.3394145965576172
INFO:__main__:Epoch 108, Valid Loss 1.7566020488739014
INFO:__main__:Epoch 109, Train Loss 1.3992981910705566
INFO:__main__:Epoch 109, Valid Loss 1.831508755683899
INFO:__main__:Epoch 110, Train Loss 1.4539674520492554
INFO:__main__:Epoch 110, Valid Loss 1.892068862915039
INFO:__main__:Epoch 111, Train Loss 1.4992278814315796
INFO:__main__:Epoch 111, Valid Loss 1.9356236457824707
INFO:__main__:Epoch 112, Train Loss 1.5322747230529785
INFO:__main__:Epoch 112, Valid Loss 1.9609436988830566
INFO:__main__:Epoch 113, Train Loss 1.5516576766967773
INFO:__main__:Epoch 113, Valid Loss 1.9680557250976562
INFO:__main__:Epoch 114, Train Loss 1.557124376296997
INFO:__main__:Epoch 114, Valid Loss 1.9579931497573853
INFO:__main__:Epoch 115, Train Loss 1.5493971109390259
INFO:__main__:Epoch 115, Valid Loss 1.932522177696228
INFO:__main__:Epoch 116, Train Loss 1.5299180746078491
INFO:__main__:Epoch 116, Valid Loss 1.8938835859298706
INFO:__main__:Epoch 117, Train Loss 1.5006103515625
INFO:__main__:Epoch 117, Valid Loss 1.844474196434021
INFO:__main__:Epoch 118, Train Loss 1.4636021852493286
INFO:__main__:Epoch 118, Valid Loss 1.7866631746292114
INFO:__main__:Epoch 119, Train Loss 1.4210667610168457
INFO:__main__:Epoch 119, Valid Loss 1.7227087020874023
INFO:__main__:Epoch 120, Train Loss 1.3751581907272339
INFO:__main__:Epoch 120, Valid Loss 1.6545087099075317
INFO:__main__:Epoch 121, Train Loss 1.3278599977493286
INFO:__main__:Epoch 121, Valid Loss 1.5841690301895142
INFO:__main__:Epoch 122, Train Loss 1.2814511060714722
INFO:__main__:Epoch 122, Valid Loss 1.514284372329712
INFO:__main__:Epoch 123, Train Loss 1.2387558221817017
INFO:__main__:Epoch 123, Valid Loss 1.448005199432373
INFO:__main__:Epoch 124, Train Loss 1.2032840251922607
INFO:__main__:Epoch 124, Valid Loss 1.38987135887146
INFO:__main__:Epoch 125, Train Loss 1.1797763109207153
INFO:__main__:Epoch 125, Valid Loss 1.3469035625457764
INFO:__main__:Epoch 126, Train Loss 1.1738134622573853
INFO:__main__:Epoch 126, Valid Loss 1.3245409727096558
INFO:__main__:Epoch 127, Train Loss 1.1863263845443726
INFO:__main__:Epoch 127, Valid Loss 1.319319725036621
INFO:__main__:Epoch 128, Train Loss 1.2076377868652344
INFO:__main__:Epoch 128, Valid Loss 1.3205838203430176
INFO:__main__:Epoch 129, Train Loss 1.2224481105804443
INFO:__main__:Epoch 129, Valid Loss 1.3207645416259766
INFO:__main__:Epoch 130, Train Loss 1.2235476970672607
INFO:__main__:Epoch 130, Valid Loss 1.319578766822815
INFO:__main__:Epoch 131, Train Loss 1.2142314910888672
INFO:__main__:Epoch 131, Valid Loss 1.3196860551834106
INFO:__main__:Epoch 132, Train Loss 1.2012134790420532
INFO:__main__:Epoch 132, Valid Loss 1.3228092193603516
INFO:__main__:Epoch 133, Train Loss 1.1895055770874023
INFO:__main__:Epoch 133, Valid Loss 1.3287534713745117
INFO:__main__:Epoch 134, Train Loss 1.181304693222046
INFO:__main__:Epoch 134, Valid Loss 1.3362666368484497
INFO:__main__:Epoch 135, Train Loss 1.1765292882919312
INFO:__main__:Epoch 135, Valid Loss 1.3443129062652588
INFO:__main__:Epoch 136, Train Loss 1.1741961240768433
INFO:__main__:Epoch 136, Valid Loss 1.3520729541778564
INFO:__main__:Epoch 137, Train Loss 1.1734451055526733
INFO:__main__:Epoch 137, Valid Loss 1.3590041399002075
INFO:__main__:Epoch 138, Train Loss 1.1735955476760864
INFO:__main__:Epoch 138, Valid Loss 1.364797830581665
INFO:__main__:Epoch 139, Train Loss 1.1741645336151123
INFO:__main__:Epoch 139, Valid Loss 1.3693469762802124
INFO:__main__:Epoch 140, Train Loss 1.1748405694961548
INFO:__main__:Epoch 140, Valid Loss 1.3726654052734375
INFO:__main__:Epoch 141, Train Loss 1.1754436492919922
INFO:__main__:Epoch 141, Valid Loss 1.3748189210891724
INFO:__main__:Epoch 142, Train Loss 1.175879955291748
INFO:__main__:Epoch 142, Valid Loss 1.3759191036224365
INFO:__main__:Epoch 143, Train Loss 1.1761155128479004
INFO:__main__:Epoch 143, Valid Loss 1.3761028051376343
INFO:__main__:Epoch 144, Train Loss 1.1761554479599
INFO:__main__:Epoch 144, Valid Loss 1.3755172491073608
INFO:__main__:Epoch 145, Train Loss 1.1760280132293701
INFO:__main__:Epoch 145, Valid Loss 1.3743118047714233
INFO:__main__:Epoch 146, Train Loss 1.1757732629776
INFO:__main__:Epoch 146, Valid Loss 1.3726295232772827
INFO:__main__:Epoch 147, Train Loss 1.1754353046417236
INFO:__main__:Epoch 147, Valid Loss 1.370603084564209
INFO:__main__:Epoch 148, Train Loss 1.175057291984558
INFO:__main__:Epoch 148, Valid Loss 1.3683524131774902
INFO:__main__:Epoch 149, Train Loss 1.174675703048706
INFO:__main__:Epoch 149, Valid Loss 1.3659824132919312
INFO:__main__:Epoch 150, Train Loss 1.1743210554122925
INFO:__main__:Epoch 150, Valid Loss 1.3635828495025635
INFO:__main__:Epoch 151, Train Loss 1.1740148067474365
INFO:__main__:Epoch 151, Valid Loss 1.36122727394104
INFO:__main__:Epoch 152, Train Loss 1.1737703084945679
INFO:__main__:Epoch 152, Valid Loss 1.358974814414978
INFO:__main__:Epoch 153, Train Loss 1.1735926866531372
INFO:__main__:Epoch 153, Valid Loss 1.3568707704544067
INFO:__main__:Epoch 154, Train Loss 1.17348051071167
INFO:__main__:Epoch 154, Valid Loss 1.3549480438232422
INFO:__main__:Epoch 155, Train Loss 1.1734275817871094
INFO:__main__:Epoch 155, Valid Loss 1.3532289266586304
INFO:__main__:Epoch 156, Train Loss 1.1734235286712646
INFO:__main__:Epoch 156, Valid Loss 1.35172700881958
INFO:__main__:Epoch 157, Train Loss 1.173455834388733
INFO:__main__:Epoch 157, Valid Loss 1.3504478931427002
INFO:__main__:Epoch 158, Train Loss 1.1735117435455322
INFO:__main__:Epoch 158, Valid Loss 1.3493916988372803
INFO:__main__:Epoch 159, Train Loss 1.1735785007476807
INFO:__main__:Epoch 159, Valid Loss 1.348554015159607
INFO:__main__:Epoch 160, Train Loss 1.1736457347869873
INFO:__main__:Epoch 160, Valid Loss 1.3479264974594116
INFO:__main__:Epoch 161, Train Loss 1.1737045049667358
INFO:__main__:Epoch 161, Valid Loss 1.3474985361099243
INFO:__main__:Epoch 162, Train Loss 1.1737489700317383
INFO:__main__:Epoch 162, Valid Loss 1.347257375717163
INFO:__main__:Epoch 163, Train Loss 1.173775553703308
INFO:__main__:Epoch 163, Valid Loss 1.3471888303756714
INFO:__main__:Epoch 164, Train Loss 1.173783302307129
INFO:__main__:Epoch 164, Valid Loss 1.347277283668518
INFO:__main__:Epoch 165, Train Loss 1.1737734079360962
INFO:__main__:Epoch 165, Valid Loss 1.3475062847137451
INFO:__main__:Epoch 166, Train Loss 1.1737481355667114
INFO:__main__:Epoch 166, Valid Loss 1.3478585481643677
INFO:__main__:Epoch 167, Train Loss 1.1737112998962402
INFO:__main__:Epoch 167, Valid Loss 1.348315954208374
INFO:__main__:Epoch 168, Train Loss 1.173667073249817
INFO:__main__:Epoch 168, Valid Loss 1.348860263824463
INFO:__main__:Epoch 169, Train Loss 1.1736197471618652
INFO:__main__:Epoch 169, Valid Loss 1.349472165107727
INFO:__main__:Epoch 170, Train Loss 1.1735727787017822
INFO:__main__:Epoch 170, Valid Loss 1.3501332998275757
INFO:__main__:Epoch 171, Train Loss 1.1735295057296753
INFO:__main__:Epoch 171, Valid Loss 1.3508244752883911
INFO:__main__:Epoch 172, Train Loss 1.1734923124313354
INFO:__main__:Epoch 172, Valid Loss 1.3515275716781616
INFO:__main__:Epoch 173, Train Loss 1.1734627485275269
INFO:__main__:Epoch 173, Valid Loss 1.352225422859192
INFO:__main__:Epoch 174, Train Loss 1.1734412908554077
INFO:__main__:Epoch 174, Valid Loss 1.3529020547866821
INFO:__main__:Epoch 175, Train Loss 1.1734275817871094
INFO:__main__:Epoch 175, Valid Loss 1.3535430431365967
INFO:__main__:Epoch 176, Train Loss 1.173421025276184
INFO:__main__:Epoch 176, Valid Loss 1.3541353940963745
INFO:__main__:Epoch 177, Train Loss 1.1734203100204468
INFO:__main__:Epoch 177, Valid Loss 1.3546688556671143
INFO:__main__:Epoch 178, Train Loss 1.1734240055084229
INFO:__main__:Epoch 178, Valid Loss 1.3551350831985474
INFO:__main__:Epoch 179, Train Loss 1.1734306812286377
INFO:__main__:Epoch 179, Valid Loss 1.3555277585983276
INFO:__main__:Epoch 180, Train Loss 1.1734384298324585
INFO:__main__:Epoch 180, Valid Loss 1.3558441400527954
INFO:__main__:Epoch 181, Train Loss 1.1734460592269897
INFO:__main__:Epoch 181, Valid Loss 1.3560823202133179
INFO:__main__:Epoch 182, Train Loss 1.1734529733657837
INFO:__main__:Epoch 182, Valid Loss 1.3562434911727905
INFO:__main__:Epoch 183, Train Loss 1.1734579801559448
INFO:__main__:Epoch 183, Valid Loss 1.3563309907913208
INFO:__main__:Epoch 184, Train Loss 1.173460841178894
INFO:__main__:Epoch 184, Valid Loss 1.356348991394043
INFO:__main__:Epoch 185, Train Loss 1.1734613180160522
INFO:__main__:Epoch 185, Valid Loss 1.3563035726547241
INFO:__main__:Epoch 186, Train Loss 1.1734598875045776
INFO:__main__:Epoch 186, Valid Loss 1.3562023639678955
INFO:__main__:Epoch 187, Train Loss 1.1734566688537598
INFO:__main__:Epoch 187, Valid Loss 1.356053352355957
INFO:__main__:Epoch 188, Train Loss 1.1734520196914673
INFO:__main__:Epoch 188, Valid Loss 1.3558647632598877
INFO:__main__:Epoch 189, Train Loss 1.173446774482727
INFO:__main__:Epoch 189, Valid Loss 1.3556454181671143
INFO:__main__:Epoch 190, Train Loss 1.1734411716461182
INFO:__main__:Epoch 190, Valid Loss 1.355404257774353
INFO:__main__:Epoch 191, Train Loss 1.1734358072280884
INFO:__main__:Epoch 191, Valid Loss 1.355149507522583
INFO:__main__:Epoch 192, Train Loss 1.1734309196472168
INFO:__main__:Epoch 192, Valid Loss 1.354888677597046
INFO:__main__:Epoch 193, Train Loss 1.1734267473220825
INFO:__main__:Epoch 193, Valid Loss 1.3546298742294312
INFO:__main__:Epoch 194, Train Loss 1.1734237670898438
INFO:__main__:Epoch 194, Valid Loss 1.3543784618377686
INFO:__main__:Epoch 195, Train Loss 1.1734216213226318
INFO:__main__:Epoch 195, Valid Loss 1.3541407585144043
INFO:__main__:Epoch 196, Train Loss 1.1734204292297363
INFO:__main__:Epoch 196, Valid Loss 1.3539211750030518
INFO:__main__:Epoch 197, Train Loss 1.1734200716018677
INFO:__main__:Epoch 197, Valid Loss 1.353723406791687
INFO:__main__:Epoch 198, Train Loss 1.1734203100204468
INFO:__main__:Epoch 198, Valid Loss 1.3535505533218384
INFO:__main__:Epoch 199, Train Loss 1.173421025276184
INFO:__main__:Epoch 199, Valid Loss 1.35340416431427
INFO:__main__:Epoch 200, Train Loss 1.1734219789505005
INFO:__main__:Epoch 200, Valid Loss 1.353285789489746
INFO:__main__:Epoch 201, Train Loss 1.173422932624817
INFO:__main__:Epoch 201, Valid Loss 1.3531951904296875
INFO:__main__:Epoch 202, Train Loss 1.1734238862991333
INFO:__main__:Epoch 202, Valid Loss 1.3531321287155151
INFO:__main__:Epoch 203, Train Loss 1.1734246015548706
INFO:__main__:Epoch 203, Valid Loss 1.3530958890914917
INFO:__main__:Epoch 204, Train Loss 1.1734249591827393
INFO:__main__:Epoch 204, Valid Loss 1.3530844449996948
INFO:__main__:Epoch 205, Train Loss 1.1734250783920288
INFO:__main__:Epoch 205, Valid Loss 1.3530957698822021
INFO:__main__:Epoch 206, Train Loss 1.1734249591827393
INFO:__main__:Epoch 206, Valid Loss 1.3531274795532227
INFO:__main__:Epoch 207, Train Loss 1.1734246015548706
INFO:__main__:Epoch 207, Valid Loss 1.3531769514083862
INFO:__main__:Epoch 208, Train Loss 1.1734241247177124
INFO:__main__:Epoch 208, Valid Loss 1.3532410860061646
INFO:__main__:Epoch 209, Train Loss 1.1734232902526855
INFO:__main__:Epoch 209, Valid Loss 1.3533167839050293
INFO:__main__:Epoch 210, Train Loss 1.1734226942062378
INFO:__main__:Epoch 210, Valid Loss 1.3534010648727417
INFO:__main__:Epoch 211, Train Loss 1.1734219789505005
INFO:__main__:Epoch 211, Valid Loss 1.353490948677063
INFO:__main__:Epoch 212, Train Loss 1.1734213829040527
INFO:__main__:Epoch 212, Valid Loss 1.3535834550857544
INFO:__main__:Epoch 213, Train Loss 1.173421025276184
INFO:__main__:Epoch 213, Valid Loss 1.3536760807037354
INFO:__main__:Epoch 214, Train Loss 1.1734204292297363
INFO:__main__:Epoch 214, Valid Loss 1.3537660837173462
INFO:__main__:Epoch 215, Train Loss 1.1734201908111572
INFO:__main__:Epoch 215, Valid Loss 1.3538516759872437
INFO:__main__:Epoch 216, Train Loss 1.1734201908111572
INFO:__main__:Epoch 216, Valid Loss 1.3539307117462158
INFO:__main__:Epoch 217, Train Loss 1.1734201908111572
INFO:__main__:Epoch 217, Valid Loss 1.3540016412734985
INFO:__main__:Epoch 218, Train Loss 1.1734201908111572
INFO:__main__:Epoch 218, Valid Loss 1.3540632724761963
INFO:__main__:Epoch 219, Train Loss 1.1734203100204468
INFO:__main__:Epoch 219, Valid Loss 1.3541151285171509
INFO:__main__:Epoch 220, Train Loss 1.1734203100204468
INFO:__main__:Epoch 220, Valid Loss 1.3541563749313354
INFO:__main__:Epoch 221, Train Loss 1.1734204292297363
INFO:__main__:Epoch 221, Valid Loss 1.354187250137329
INFO:__main__:Epoch 222, Train Loss 1.1734205484390259
INFO:__main__:Epoch 222, Valid Loss 1.3542076349258423
INFO:__main__:Epoch 223, Train Loss 1.1734206676483154
INFO:__main__:Epoch 223, Valid Loss 1.3542180061340332
INFO:__main__:Epoch 224, Train Loss 1.173420786857605
INFO:__main__:Epoch 224, Valid Loss 1.35421884059906
INFO:__main__:Epoch 225, Train Loss 1.173420786857605
INFO:__main__:Epoch 225, Valid Loss 1.3542113304138184
INFO:__main__:Epoch 226, Train Loss 1.1734206676483154
INFO:__main__:Epoch 226, Valid Loss 1.3541966676712036
INFO:__main__:Epoch 227, Train Loss 1.1734205484390259
INFO:__main__:Epoch 227, Valid Loss 1.3541754484176636
INFO:__main__:Epoch 228, Train Loss 1.1734205484390259
INFO:__main__:Epoch 228, Valid Loss 1.3541489839553833
INFO:__main__:Epoch 229, Train Loss 1.1734204292297363
INFO:__main__:Epoch 229, Valid Loss 1.3541189432144165
INFO:__main__:Epoch 230, Train Loss 1.1734203100204468
INFO:__main__:Epoch 230, Valid Loss 1.3540860414505005
INFO:__main__:Epoch 231, Train Loss 1.1734201908111572
INFO:__main__:Epoch 231, Valid Loss 1.3540515899658203
INFO:__main__:Epoch 232, Train Loss 1.1734201908111572
INFO:__main__:Epoch 232, Valid Loss 1.354016900062561
INFO:__main__:Epoch 233, Train Loss 1.1734201908111572
INFO:__main__:Epoch 233, Valid Loss 1.3539828062057495
INFO:__main__:Epoch 234, Train Loss 1.1734201908111572
INFO:__main__:Epoch 234, Valid Loss 1.353950023651123
INFO:__main__:Epoch 235, Train Loss 1.1734200716018677
INFO:__main__:Epoch 235, Valid Loss 1.3539196252822876
INFO:__main__:Epoch 236, Train Loss 1.1734200716018677
INFO:__main__:Epoch 236, Valid Loss 1.3538923263549805
INFO:__main__:Epoch 237, Train Loss 1.1734201908111572
INFO:__main__:Epoch 237, Valid Loss 1.353868007659912
INFO:__main__:Epoch 238, Train Loss 1.1734201908111572
INFO:__main__:Epoch 238, Valid Loss 1.3538477420806885
INFO:__main__:Epoch 239, Train Loss 1.1734201908111572
INFO:__main__:Epoch 239, Valid Loss 1.35383141040802
INFO:__main__:Epoch 240, Train Loss 1.1734201908111572
INFO:__main__:Epoch 240, Valid Loss 1.3538191318511963
INFO:__main__:Epoch 241, Train Loss 1.1734201908111572
INFO:__main__:Epoch 241, Valid Loss 1.3538107872009277
INFO:__main__:Epoch 242, Train Loss 1.1734201908111572
INFO:__main__:Epoch 242, Valid Loss 1.353806495666504
INFO:__main__:Epoch 243, Train Loss 1.1734201908111572
INFO:__main__:Epoch 243, Valid Loss 1.3538058996200562
INFO:__main__:Epoch 244, Train Loss 1.1734201908111572
INFO:__main__:Epoch 244, Valid Loss 1.3538082838058472
INFO:__main__:Epoch 245, Train Loss 1.1734201908111572
INFO:__main__:Epoch 245, Valid Loss 1.3538134098052979
INFO:__main__:Epoch 246, Train Loss 1.1734201908111572
INFO:__main__:Epoch 246, Valid Loss 1.3538212776184082
INFO:__main__:Epoch 247, Train Loss 1.1734201908111572
INFO:__main__:Epoch 247, Valid Loss 1.3538310527801514
INFO:__main__:Epoch 248, Train Loss 1.1734201908111572
INFO:__main__:Epoch 248, Valid Loss 1.3538426160812378
INFO:__main__:Epoch 249, Train Loss 1.1734201908111572
INFO:__main__:Epoch 249, Valid Loss 1.3538548946380615
INFO:__main__:Epoch 250, Train Loss 1.1734201908111572
INFO:__main__:Epoch 250, Valid Loss 1.353868007659912
INFO:__main__:Epoch 251, Train Loss 1.1734201908111572
INFO:__main__:Epoch 251, Valid Loss 1.3538810014724731
INFO:__main__:Epoch 252, Train Loss 1.1734201908111572
INFO:__main__:Epoch 252, Valid Loss 1.3538939952850342
INFO:__main__:Epoch 253, Train Loss 1.1734201908111572
INFO:__main__:Epoch 253, Valid Loss 1.353906273841858
INFO:__main__:Epoch 254, Train Loss 1.1734200716018677
INFO:__main__:Epoch 254, Valid Loss 1.3539175987243652
INFO:__main__:Epoch 255, Train Loss 1.1734200716018677
INFO:__main__:Epoch 255, Valid Loss 1.3539279699325562
INFO:__main__:Epoch 256, Train Loss 1.1734200716018677
INFO:__main__:Epoch 256, Valid Loss 1.353937029838562
INFO:__main__:Epoch 257, Train Loss 1.1734201908111572
INFO:__main__:Epoch 257, Valid Loss 1.3539444208145142
INFO:__main__:Epoch 258, Train Loss 1.1734201908111572
INFO:__main__:Epoch 258, Valid Loss 1.3539503812789917
INFO:__main__:Epoch 259, Train Loss 1.1734201908111572
INFO:__main__:Epoch 259, Valid Loss 1.353954792022705
INFO:__main__:Epoch 260, Train Loss 1.1734200716018677
INFO:__main__:Epoch 260, Valid Loss 1.3539574146270752
INFO:__main__:Epoch 261, Train Loss 1.1734201908111572
INFO:__main__:Epoch 261, Valid Loss 1.3539589643478394
INFO:__main__:Epoch 262, Train Loss 1.1734201908111572
INFO:__main__:Epoch 262, Valid Loss 1.3539589643478394
INFO:__main__:Epoch 263, Train Loss 1.1734200716018677
INFO:__main__:Epoch 263, Valid Loss 1.3539574146270752
INFO:__main__:Epoch 264, Train Loss 1.1734200716018677
INFO:__main__:Epoch 264, Valid Loss 1.3539552688598633
INFO:__main__:Epoch 265, Train Loss 1.1734200716018677
INFO:__main__:Epoch 265, Valid Loss 1.3539519309997559
INFO:__main__:Epoch 266, Train Loss 1.1734200716018677
INFO:__main__:Epoch 266, Valid Loss 1.3539478778839111
INFO:__main__:Epoch 267, Train Loss 1.1734200716018677
INFO:__main__:Epoch 267, Valid Loss 1.3539432287216187
INFO:__main__:Epoch 268, Train Loss 1.1734201908111572
INFO:__main__:Epoch 268, Valid Loss 1.353938341140747
INFO:__main__:Epoch 269, Train Loss 1.1734200716018677
INFO:__main__:Epoch 269, Valid Loss 1.3539334535598755
INFO:__main__:Epoch 270, Train Loss 1.1734201908111572
INFO:__main__:Epoch 270, Valid Loss 1.3539284467697144
INFO:__main__:Epoch 271, Train Loss 1.1734201908111572
INFO:__main__:Epoch 271, Valid Loss 1.3539235591888428
INFO:__main__:Epoch 272, Train Loss 1.1734201908111572
INFO:__main__:Epoch 272, Valid Loss 1.3539189100265503
INFO:__main__:Epoch 273, Train Loss 1.1734201908111572
INFO:__main__:Epoch 273, Valid Loss 1.353914737701416
INFO:__main__:Epoch 274, Train Loss 1.1734201908111572
INFO:__main__:Epoch 274, Valid Loss 1.3539106845855713
INFO:__main__:Epoch 275, Train Loss 1.1734199523925781
INFO:__main__:Epoch 275, Valid Loss 1.353907585144043
INFO:__main__:Epoch 276, Train Loss 1.1734201908111572
INFO:__main__:Epoch 276, Valid Loss 1.3539049625396729
INFO:__main__:Epoch 277, Train Loss 1.1734201908111572
INFO:__main__:Epoch 277, Valid Loss 1.3539031744003296
INFO:__main__:Epoch 278, Train Loss 1.1734201908111572
INFO:__main__:Epoch 278, Valid Loss 1.353901743888855
INFO:__main__:Epoch 279, Train Loss 1.1734201908111572
INFO:__main__:Epoch 279, Valid Loss 1.3539012670516968
INFO:__main__:Epoch 280, Train Loss 1.1734200716018677
INFO:__main__:Epoch 280, Valid Loss 1.3539010286331177
INFO:__main__:Epoch 281, Train Loss 1.1734201908111572
INFO:__main__:Epoch 281, Valid Loss 1.3539012670516968
INFO:__main__:Epoch 282, Train Loss 1.1734200716018677
INFO:__main__:Epoch 282, Valid Loss 1.3539018630981445
INFO:__main__:Epoch 283, Train Loss 1.1734201908111572
INFO:__main__:Epoch 283, Valid Loss 1.3539029359817505
INFO:__main__:Epoch 284, Train Loss 1.1734201908111572
INFO:__main__:Epoch 284, Valid Loss 1.3539046049118042
INFO:__main__:Epoch 285, Train Loss 1.1734201908111572
INFO:__main__:Epoch 285, Valid Loss 1.353906273841858
INFO:__main__:Epoch 286, Train Loss 1.1734200716018677
INFO:__main__:Epoch 286, Valid Loss 1.3539081811904907
INFO:__main__:Epoch 287, Train Loss 1.1734201908111572
INFO:__main__:Epoch 287, Valid Loss 1.3539100885391235
INFO:__main__:Epoch 288, Train Loss 1.1734201908111572
INFO:__main__:Epoch 288, Valid Loss 1.3539122343063354
INFO:__main__:Epoch 289, Train Loss 1.1734201908111572
INFO:__main__:Epoch 289, Valid Loss 1.3539140224456787
INFO:__main__:Epoch 290, Train Loss 1.1734201908111572
INFO:__main__:Epoch 290, Valid Loss 1.3539159297943115
INFO:__main__:Epoch 291, Train Loss 1.1734201908111572
INFO:__main__:Epoch 291, Valid Loss 1.3539175987243652
INFO:__main__:Epoch 292, Train Loss 1.1734200716018677
INFO:__main__:Epoch 292, Valid Loss 1.3539191484451294
INFO:__main__:Epoch 293, Train Loss 1.1734200716018677
INFO:__main__:Epoch 293, Valid Loss 1.353920340538025
INFO:__main__:Epoch 294, Train Loss 1.1734201908111572
INFO:__main__:Epoch 294, Valid Loss 1.3539214134216309
INFO:__main__:Epoch 295, Train Loss 1.1734200716018677
INFO:__main__:Epoch 295, Valid Loss 1.3539220094680786
INFO:__main__:Epoch 296, Train Loss 1.1734200716018677
INFO:__main__:Epoch 296, Valid Loss 1.3539223670959473
INFO:__main__:Epoch 297, Train Loss 1.1734200716018677
INFO:__main__:Epoch 297, Valid Loss 1.3539226055145264
INFO:__main__:Epoch 298, Train Loss 1.1734201908111572
INFO:__main__:Epoch 298, Valid Loss 1.3539226055145264
INFO:__main__:Epoch 299, Train Loss 1.1734201908111572
INFO:__main__:Epoch 299, Valid Loss 1.353922724723816
INFO:__main__:Epoch 300, Train Loss 1.1734201908111572
INFO:__main__:Epoch 300, Valid Loss 1.3539226055145264
INFO:__main__:Epoch 301, Train Loss 1.1734200716018677
INFO:__main__:Epoch 301, Valid Loss 1.3539223670959473
INFO:__main__:Epoch 302, Train Loss 1.1734200716018677
INFO:__main__:Epoch 302, Valid Loss 1.3539220094680786
INFO:__main__:Epoch 303, Train Loss 1.1734200716018677
INFO:__main__:Epoch 303, Valid Loss 1.35392165184021
INFO:__main__:Epoch 304, Train Loss 1.1734199523925781
INFO:__main__:Epoch 304, Valid Loss 1.3539210557937622
INFO:__main__:Epoch 305, Train Loss 1.1734201908111572
INFO:__main__:Epoch 305, Valid Loss 1.353920578956604
INFO:__main__:Epoch 306, Train Loss 1.1734200716018677
INFO:__main__:Epoch 306, Valid Loss 1.3539199829101562
INFO:__main__:Epoch 307, Train Loss 1.1734200716018677
INFO:__main__:Epoch 307, Valid Loss 1.353919267654419
INFO:__main__:Epoch 308, Train Loss 1.1734201908111572
INFO:__main__:Epoch 308, Valid Loss 1.3539187908172607
INFO:__main__:Epoch 309, Train Loss 1.1734200716018677
INFO:__main__:Epoch 309, Valid Loss 1.3539183139801025
INFO:__main__:Epoch 310, Train Loss 1.1734200716018677
INFO:__main__:Epoch 310, Valid Loss 1.3539175987243652
INFO:__main__:Epoch 311, Train Loss 1.1734201908111572
INFO:__main__:Epoch 311, Valid Loss 1.3539172410964966
INFO:__main__:Epoch 312, Train Loss 1.1734201908111572
INFO:__main__:Epoch 312, Valid Loss 1.3539167642593384
INFO:__main__:Epoch 313, Train Loss 1.1734201908111572
INFO:__main__:Epoch 313, Valid Loss 1.3539164066314697
INFO:__main__:Epoch 314, Train Loss 1.1734201908111572
INFO:__main__:Epoch 314, Valid Loss 1.3539161682128906
INFO:__main__:Epoch 315, Train Loss 1.1734200716018677
INFO:__main__:Epoch 315, Valid Loss 1.353915810585022
INFO:__main__:Epoch 316, Train Loss 1.1734201908111572
INFO:__main__:Epoch 316, Valid Loss 1.3539156913757324
INFO:__main__:Epoch 317, Train Loss 1.1734201908111572
INFO:__main__:Epoch 317, Valid Loss 1.3539155721664429
INFO:__main__:Epoch 318, Train Loss 1.1734201908111572
INFO:__main__:Epoch 318, Valid Loss 1.3539155721664429
INFO:__main__:Epoch 319, Train Loss 1.1734201908111572
INFO:__main__:Epoch 319, Valid Loss 1.3539155721664429
INFO:__main__:Epoch 320, Train Loss 1.1734200716018677
INFO:__main__:Epoch 320, Valid Loss 1.3539155721664429
INFO:__main__:Epoch 321, Train Loss 1.1734201908111572
INFO:__main__:Epoch 321, Valid Loss 1.3539155721664429
INFO:__main__:Epoch 322, Train Loss 1.1734200716018677
INFO:__main__:Epoch 322, Valid Loss 1.3539155721664429
INFO:__main__:Epoch 323, Train Loss 1.1734201908111572
INFO:__main__:Epoch 323, Valid Loss 1.3539155721664429
INFO:__main__:Epoch 324, Train Loss 1.1734201908111572
INFO:__main__:Epoch 324, Valid Loss 1.3539155721664429
INFO:__main__:Epoch 325, Train Loss 1.1734201908111572
INFO:__main__:Epoch 325, Valid Loss 1.3539156913757324
INFO:__main__:Epoch 326, Train Loss 1.1734200716018677
INFO:__main__:Epoch 326, Valid Loss 1.3539156913757324
INFO:__main__:Epoch 327, Train Loss 1.1734200716018677
INFO:__main__:Epoch 327, Valid Loss 1.3539156913757324
INFO:__main__:Epoch 328, Train Loss 1.1734201908111572
INFO:__main__:Epoch 328, Valid Loss 1.3539159297943115
INFO:__main__:Epoch 329, Train Loss 1.1734200716018677
INFO:__main__:Epoch 329, Valid Loss 1.3539159297943115
INFO:__main__:Epoch 330, Train Loss 1.1734201908111572
INFO:__main__:Epoch 330, Valid Loss 1.3539161682128906
INFO:__main__:Epoch 331, Train Loss 1.1734201908111572
INFO:__main__:Epoch 331, Valid Loss 1.3539161682128906
INFO:__main__:Epoch 332, Train Loss 1.1734201908111572
INFO:__main__:Epoch 332, Valid Loss 1.3539164066314697
INFO:__main__:Epoch 333, Train Loss 1.1734201908111572
INFO:__main__:Epoch 333, Valid Loss 1.3539165258407593
INFO:__main__:Epoch 334, Train Loss 1.1734200716018677
INFO:__main__:Epoch 334, Valid Loss 1.3539166450500488
INFO:__main__:Epoch 335, Train Loss 1.1734201908111572
INFO:__main__:Epoch 335, Valid Loss 1.3539167642593384
INFO:__main__:Epoch 336, Train Loss 1.1734201908111572
INFO:__main__:Epoch 336, Valid Loss 1.3539167642593384
INFO:__main__:Epoch 337, Train Loss 1.1734200716018677
INFO:__main__:Epoch 337, Valid Loss 1.3539167642593384
INFO:__main__:Epoch 338, Train Loss 1.1734201908111572
INFO:__main__:Epoch 338, Valid Loss 1.3539170026779175
INFO:__main__:Epoch 339, Train Loss 1.1734201908111572
INFO:__main__:Epoch 339, Valid Loss 1.3539170026779175
INFO:__main__:Epoch 340, Train Loss 1.1734201908111572
INFO:__main__:Epoch 340, Valid Loss 1.3539170026779175
INFO:__main__:Epoch 341, Train Loss 1.1734200716018677
INFO:__main__:Epoch 341, Valid Loss 1.3539172410964966
INFO:__main__:Epoch 342, Train Loss 1.1734201908111572
INFO:__main__:Epoch 342, Valid Loss 1.3539172410964966
INFO:__main__:Epoch 343, Train Loss 1.1734201908111572
INFO:__main__:Epoch 343, Valid Loss 1.3539172410964966
INFO:__main__:Epoch 344, Train Loss 1.1734201908111572
INFO:__main__:Epoch 344, Valid Loss 1.3539172410964966
INFO:__main__:Epoch 345, Train Loss 1.1734201908111572
INFO:__main__:Epoch 345, Valid Loss 1.3539172410964966
INFO:__main__:Epoch 346, Train Loss 1.1734201908111572
INFO:__main__:Epoch 346, Valid Loss 1.3539172410964966
INFO:__main__:Epoch 347, Train Loss 1.1734200716018677
INFO:__main__:Epoch 347, Valid Loss 1.3539172410964966
INFO:__main__:Epoch 348, Train Loss 1.1734200716018677
INFO:__main__:Epoch 348, Valid Loss 1.3539172410964966
Killed
nohup: ignoring input
INFO:__main__:name: ca9_25
INFO:__main__:base_fname: ca9_large
INFO:__main__:seed: 4
INFO:__main__:device: cuda:2
INFO:__main__:batch_size: 2048
INFO:__main__:epochs: 200
INFO:__main__:lr: 0.0003
INFO:__main__:log_interval: 1
INFO:__main__:save_interval: 5
INFO:__main__:save_path: /data02/gtguo/DEL/data/weights/refnet_ca9_transfer/
INFO:__main__:load_path: /data02/gtguo/DEL/data/weights/refnet/
INFO:__main__:target_name: ca9
INFO:__main__:fp_size: 2048
INFO:__main__:transfer_learning: True
INFO:__main__:transfer_learning_ratio: 0.5
INFO:__main__:num_workers: 8
INFO:__main__:enc_node_feat_dim: 19
INFO:__main__:enc_edge_feat_dim: 2
INFO:__main__:enc_node_embedding_size: 64
INFO:__main__:enc_edge_embedding_size: 64
INFO:__main__:enc_n_layers: 5
INFO:__main__:enc_gat_n_heads: 4
INFO:__main__:enc_gat_ffn_ratio: 4
INFO:__main__:enc_fp_embedding_size: 32
INFO:__main__:enc_fp_ffn_size: 128
INFO:__main__:enc_fp_gated: False
INFO:__main__:enc_fp_n_heads: 4
INFO:__main__:enc_fp_size: 256
INFO:__main__:enc_fp_to_gat_feedback: add
INFO:__main__:enc_gat_to_fp_pooling: mean
INFO:__main__:dec_node_input_size: 64
INFO:__main__:dec_node_emb_size: 64
INFO:__main__:dec_fp_input_size: 32
INFO:__main__:dec_fp_emb_size: 64
INFO:__main__:dec_output_size: 2
INFO:__main__:reg_input_size: 64
INFO:__main__:reg_hidden_size: 64
INFO:__main__:reg_output_size: 1
INFO:__main__:record_path: /data02/gtguo/DEL/data/records/refnet_ca9_transfer/
INFO:__main__:Model has 408772 parameters
INFO:__main__:Head has 4225 parameters
Splitting dataset:   0%|          | 0/2372674 [00:00<?, ?it/s]Splitting dataset:   1%|          | 17136/2372674 [00:00<00:13, 171348.32it/s]Splitting dataset:   1%|▏         | 34711/2372674 [00:00<00:13, 173897.81it/s]Splitting dataset:   2%|▏         | 52707/2372674 [00:00<00:13, 176663.13it/s]Splitting dataset:   3%|▎         | 71193/2372674 [00:00<00:12, 179894.02it/s]Splitting dataset:   4%|▍         | 90207/2372674 [00:00<00:12, 183586.37it/s]Splitting dataset:   5%|▍         | 109367/2372674 [00:00<00:12, 186307.59it/s]Splitting dataset:   5%|▌         | 127998/2372674 [00:00<00:12, 186090.44it/s]Splitting dataset:   6%|▌         | 147234/2372674 [00:00<00:11, 188081.34it/s]Splitting dataset:   7%|▋         | 166311/2372674 [00:00<00:11, 188916.64it/s]Splitting dataset:   8%|▊         | 185203/2372674 [00:01<00:11, 188587.78it/s]Splitting dataset:   9%|▊         | 204063/2372674 [00:01<00:11, 186277.56it/s]Splitting dataset:   9%|▉         | 223032/2372674 [00:01<00:11, 187301.03it/s]Splitting dataset:  10%|█         | 242400/2372674 [00:01<00:11, 189216.54it/s]Splitting dataset:  11%|█         | 262073/2372674 [00:01<00:11, 191470.50it/s]Splitting dataset:  12%|█▏        | 281279/2372674 [00:01<00:10, 191640.11it/s]Splitting dataset:  13%|█▎        | 301745/2372674 [00:01<00:10, 195548.25it/s]Splitting dataset:  14%|█▎        | 321810/2372674 [00:01<00:10, 197075.50it/s]Splitting dataset:  14%|█▍        | 341830/2372674 [00:01<00:10, 198011.77it/s]Splitting dataset:  15%|█▌        | 361633/2372674 [00:01<00:10, 197550.58it/s]Splitting dataset:  16%|█▌        | 381390/2372674 [00:02<00:10, 196660.45it/s]Splitting dataset:  17%|█▋        | 401058/2372674 [00:02<00:10, 195329.48it/s]Splitting dataset:  18%|█▊        | 420594/2372674 [00:02<00:10, 194304.72it/s]Splitting dataset:  19%|█▊        | 440027/2372674 [00:02<00:09, 193742.02it/s]Splitting dataset:  19%|█▉        | 459534/2372674 [00:02<00:09, 194133.67it/s]Splitting dataset:  20%|██        | 479908/2372674 [00:02<00:09, 196996.29it/s]Splitting dataset:  21%|██        | 500419/2372674 [00:02<00:09, 199416.77it/s]Splitting dataset:  22%|██▏       | 520364/2372674 [00:02<00:09, 193163.01it/s]Splitting dataset:  23%|██▎       | 539724/2372674 [00:02<00:09, 189787.20it/s]Splitting dataset:  24%|██▎       | 558775/2372674 [00:02<00:09, 189993.69it/s]Splitting dataset:  24%|██▍       | 577801/2372674 [00:03<00:09, 187974.49it/s]Splitting dataset:  25%|██▌       | 597929/2372674 [00:03<00:09, 191874.47it/s]Splitting dataset:  26%|██▌       | 617851/2372674 [00:03<00:09, 194039.85it/s]Splitting dataset:  27%|██▋       | 637275/2372674 [00:03<00:08, 193873.85it/s]Splitting dataset:  28%|██▊       | 657455/2372674 [00:03<00:08, 196229.10it/s]Splitting dataset:  29%|██▊       | 677538/2372674 [00:03<00:08, 197597.25it/s]Splitting dataset:  29%|██▉       | 698092/2372674 [00:03<00:08, 199964.96it/s]Splitting dataset:  30%|███       | 718096/2372674 [00:03<00:08, 199922.17it/s]Splitting dataset:  31%|███       | 738620/2372674 [00:03<00:08, 201506.37it/s]Splitting dataset:  32%|███▏      | 758775/2372674 [00:03<00:08, 201148.63it/s]Splitting dataset:  33%|███▎      | 778893/2372674 [00:04<00:07, 199640.48it/s]Splitting dataset:  34%|███▎      | 798862/2372674 [00:04<00:07, 198977.93it/s]Splitting dataset:  35%|███▍      | 818945/2372674 [00:04<00:07, 199525.59it/s]Splitting dataset:  35%|███▌      | 838915/2372674 [00:04<00:07, 199576.14it/s]Splitting dataset:  36%|███▌      | 859104/2372674 [00:04<00:07, 200265.72it/s]Splitting dataset:  37%|███▋      | 879480/2372674 [00:04<00:07, 201308.82it/s]Splitting dataset:  38%|███▊      | 899916/2372674 [00:04<00:07, 202220.01it/s]Splitting dataset:  39%|███▉      | 920140/2372674 [00:04<00:07, 200832.99it/s]Splitting dataset:  40%|███▉      | 940227/2372674 [00:04<00:07, 199948.81it/s]Splitting dataset:  40%|████      | 960242/2372674 [00:04<00:07, 200006.24it/s]Splitting dataset:  41%|████▏     | 980652/2372674 [00:05<00:06, 201216.11it/s]Splitting dataset:  42%|████▏     | 1001154/2372674 [00:05<00:06, 202349.96it/s]Splitting dataset:  43%|████▎     | 1021391/2372674 [00:05<00:06, 201611.20it/s]Splitting dataset:  44%|████▍     | 1041554/2372674 [00:05<00:06, 200114.92it/s]Splitting dataset:  45%|████▍     | 1062861/2372674 [00:05<00:06, 203967.12it/s]Splitting dataset:  46%|████▌     | 1083618/2372674 [00:05<00:06, 205036.53it/s]Splitting dataset:  47%|████▋     | 1104482/2372674 [00:05<00:06, 206110.57it/s]Splitting dataset:  47%|████▋     | 1125097/2372674 [00:05<00:06, 204532.55it/s]Splitting dataset:  48%|████▊     | 1146167/2372674 [00:05<00:05, 206366.19it/s]Splitting dataset:  49%|████▉     | 1166869/2372674 [00:05<00:05, 206557.26it/s]Splitting dataset:  50%|█████     | 1187529/2372674 [00:06<00:05, 204252.82it/s]Splitting dataset:  51%|█████     | 1207962/2372674 [00:06<00:05, 202859.07it/s]Splitting dataset:  52%|█████▏    | 1228841/2372674 [00:06<00:05, 204614.58it/s]Splitting dataset:  53%|█████▎    | 1249309/2372674 [00:06<00:05, 200469.81it/s]Splitting dataset:  54%|█████▎    | 1269786/2372674 [00:06<00:05, 201729.79it/s]Splitting dataset:  54%|█████▍    | 1289976/2372674 [00:06<00:05, 201200.43it/s]Splitting dataset:  55%|█████▌    | 1310108/2372674 [00:06<00:05, 200550.59it/s]Splitting dataset:  56%|█████▌    | 1330171/2372674 [00:06<00:05, 196115.97it/s]Splitting dataset:  57%|█████▋    | 1349900/2372674 [00:06<00:05, 196457.45it/s]Splitting dataset:  58%|█████▊    | 1369570/2372674 [00:06<00:05, 196523.41it/s]Splitting dataset:  59%|█████▊    | 1389235/2372674 [00:07<00:05, 195451.52it/s]Splitting dataset:  59%|█████▉    | 1408789/2372674 [00:07<00:04, 194500.45it/s]Splitting dataset:  60%|██████    | 1428245/2372674 [00:07<00:04, 194163.04it/s]Splitting dataset:  61%|██████    | 1448330/2372674 [00:07<00:04, 196148.12it/s]Splitting dataset:  62%|██████▏   | 1468438/2372674 [00:07<00:04, 197615.59it/s]Splitting dataset:  63%|██████▎   | 1488263/2372674 [00:07<00:04, 197802.10it/s]Splitting dataset:  64%|██████▎   | 1508830/2372674 [00:07<00:04, 200150.33it/s]Splitting dataset:  64%|██████▍   | 1530291/2372674 [00:07<00:04, 204474.53it/s]Splitting dataset:  65%|██████▌   | 1551793/2372674 [00:07<00:03, 207631.02it/s]Splitting dataset:  66%|██████▋   | 1573479/2372674 [00:07<00:03, 210394.12it/s]Splitting dataset:  67%|██████▋   | 1594521/2372674 [00:08<00:03, 208653.84it/s]Splitting dataset:  68%|██████▊   | 1615392/2372674 [00:08<00:03, 206371.70it/s]Splitting dataset:  69%|██████▉   | 1636462/2372674 [00:08<00:03, 207645.15it/s]Splitting dataset:  70%|██████▉   | 1657873/2372674 [00:08<00:03, 209565.44it/s]Splitting dataset:  71%|███████   | 1679059/2372674 [00:08<00:03, 210237.89it/s]Splitting dataset:  72%|███████▏  | 1700088/2372674 [00:08<00:03, 209813.90it/s]Splitting dataset:  73%|███████▎  | 1721073/2372674 [00:08<00:03, 205243.06it/s]Splitting dataset:  73%|███████▎  | 1741621/2372674 [00:08<00:03, 204800.90it/s]Splitting dataset:  74%|███████▍  | 1762117/2372674 [00:08<00:02, 204127.29it/s]Splitting dataset:  75%|███████▌  | 1782745/2372674 [00:08<00:02, 204760.55it/s]Splitting dataset:  76%|███████▌  | 1803230/2372674 [00:09<00:02, 203134.71it/s]Splitting dataset:  77%|███████▋  | 1823551/2372674 [00:09<00:02, 200702.35it/s]Splitting dataset:  78%|███████▊  | 1843631/2372674 [00:09<00:02, 192395.73it/s]Splitting dataset:  79%|███████▊  | 1862939/2372674 [00:09<00:02, 185109.83it/s]Splitting dataset:  79%|███████▉  | 1882891/2372674 [00:09<00:02, 189187.84it/s]Splitting dataset:  80%|████████  | 1902479/2372674 [00:09<00:02, 191111.83it/s]Splitting dataset:  81%|████████  | 1921880/2372674 [00:09<00:02, 191955.09it/s]Splitting dataset:  82%|████████▏ | 1941565/2372674 [00:09<00:02, 193390.96it/s]Splitting dataset:  83%|████████▎ | 1961010/2372674 [00:09<00:02, 193702.61it/s]Splitting dataset:  83%|████████▎ | 1980589/2372674 [00:10<00:02, 194319.22it/s]Splitting dataset:  84%|████████▍ | 2000040/2372674 [00:10<00:01, 193241.04it/s]Splitting dataset:  85%|████████▌ | 2019734/2372674 [00:10<00:01, 194336.82it/s]Splitting dataset:  86%|████████▌ | 2039537/2372674 [00:10<00:01, 195436.03it/s]Splitting dataset:  87%|████████▋ | 2059687/2372674 [00:10<00:01, 197243.55it/s]Splitting dataset:  88%|████████▊ | 2079513/2372674 [00:10<00:01, 197544.35it/s]Splitting dataset:  88%|████████▊ | 2099273/2372674 [00:10<00:01, 196378.74it/s]Splitting dataset:  89%|████████▉ | 2118916/2372674 [00:10<00:01, 192559.37it/s]Splitting dataset:  90%|█████████ | 2138190/2372674 [00:10<00:01, 191298.48it/s]Splitting dataset:  91%|█████████ | 2157333/2372674 [00:10<00:01, 190499.85it/s]Splitting dataset:  92%|█████████▏| 2177968/2372674 [00:11<00:00, 195179.09it/s]Splitting dataset:  93%|█████████▎| 2198588/2372674 [00:11<00:00, 198446.31it/s]Splitting dataset:  94%|█████████▎| 2220445/2372674 [00:11<00:00, 204434.92it/s]Splitting dataset:  94%|█████████▍| 2241987/2372674 [00:11<00:00, 207709.55it/s]Splitting dataset:  95%|█████████▌| 2262920/2372674 [00:11<00:00, 208191.12it/s]Splitting dataset:  96%|█████████▋| 2283747/2372674 [00:11<00:00, 207871.04it/s]Splitting dataset:  97%|█████████▋| 2304568/2372674 [00:11<00:00, 207967.88it/s]Splitting dataset:  98%|█████████▊| 2325369/2372674 [00:11<00:00, 196890.72it/s]Splitting dataset:  99%|█████████▉| 2345182/2372674 [00:11<00:00, 196041.29it/s]Splitting dataset: 100%|█████████▉| 2364871/2372674 [00:11<00:00, 195485.80it/s]Splitting dataset: 100%|██████████| 2372674/2372674 [00:12<00:00, 197648.87it/s]
INFO:root:processing dataset...
  0%|          | 0/5453 [00:00<?, ?it/s]  1%|          | 54/5453 [00:00<00:10, 535.44it/s]  2%|▏         | 109/5453 [00:00<00:09, 538.64it/s]  3%|▎         | 173/5453 [00:00<00:09, 581.99it/s]  4%|▍         | 232/5453 [00:00<00:09, 563.49it/s]/data02/gtguo/DEL/pkg/utils/mol_feat.py:71: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matricesor `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484808560/work/aten/src/ATen/native/TensorShape.cpp:2981.)
  return torch.tensor(coo, dtype=torch.long).T
  6%|▌         | 301/5453 [00:00<00:08, 606.08it/s]  7%|▋         | 362/5453 [00:00<00:08, 571.86it/s]  8%|▊         | 426/5453 [00:00<00:08, 591.11it/s]  9%|▉         | 486/5453 [00:00<00:08, 553.37it/s] 10%|▉         | 542/5453 [00:00<00:09, 539.00it/s] 11%|█         | 603/5453 [00:01<00:08, 558.35it/s] 12%|█▏        | 660/5453 [00:01<00:08, 539.58it/s] 13%|█▎        | 715/5453 [00:01<00:08, 536.38it/s] 14%|█▍        | 775/5453 [00:01<00:08, 554.04it/s] 15%|█▌        | 831/5453 [00:01<00:08, 551.36it/s] 16%|█▋        | 887/5453 [00:01<00:08, 545.63it/s] 17%|█▋        | 942/5453 [00:01<00:08, 531.10it/s] 19%|█▊        | 1011/5453 [00:01<00:07, 574.45it/s] 20%|█▉        | 1069/5453 [00:01<00:07, 556.84it/s] 21%|██        | 1128/5453 [00:02<00:07, 565.50it/s] 22%|██▏       | 1187/5453 [00:02<00:07, 569.80it/s] 23%|██▎       | 1245/5453 [00:02<00:07, 563.04it/s] 24%|██▍       | 1302/5453 [00:02<00:07, 532.37it/s] 25%|██▌       | 1366/5453 [00:02<00:07, 561.42it/s] 26%|██▌       | 1423/5453 [00:02<00:07, 559.99it/s] 27%|██▋       | 1480/5453 [00:02<00:07, 556.63it/s] 28%|██▊       | 1538/5453 [00:02<00:06, 561.74it/s] 29%|██▉       | 1598/5453 [00:02<00:06, 571.26it/s] 30%|███       | 1656/5453 [00:02<00:06, 549.23it/s] 31%|███▏      | 1714/5453 [00:03<00:06, 557.67it/s] 32%|███▏      | 1770/5453 [00:03<00:06, 549.93it/s] 33%|███▎      | 1826/5453 [00:03<00:06, 546.81it/s] 34%|███▍      | 1881/5453 [00:03<00:07, 508.45it/s] 35%|███▌      | 1933/5453 [00:03<00:07, 489.28it/s] 36%|███▋      | 1990/5453 [00:03<00:06, 510.50it/s] 37%|███▋      | 2042/5453 [00:03<00:06, 511.99it/s] 38%|███▊      | 2097/5453 [00:03<00:06, 522.31it/s] 39%|███▉      | 2150/5453 [00:03<00:06, 522.16it/s] 41%|████      | 2209/5453 [00:04<00:05, 541.09it/s] 42%|████▏     | 2264/5453 [00:04<00:06, 520.74it/s] 43%|████▎     | 2320/5453 [00:04<00:05, 530.56it/s] 44%|████▎     | 2374/5453 [00:04<00:05, 532.29it/s] 45%|████▍     | 2431/5453 [00:04<00:05, 542.76it/s] 46%|████▌     | 2486/5453 [00:04<00:05, 538.41it/s] 47%|████▋     | 2540/5453 [00:04<00:05, 517.62it/s] 48%|████▊     | 2593/5453 [00:04<00:05, 518.85it/s] 49%|████▊     | 2649/5453 [00:04<00:05, 528.49it/s] 50%|████▉     | 2702/5453 [00:04<00:05, 498.64it/s] 50%|█████     | 2753/5453 [00:05<00:05, 484.16it/s] 52%|█████▏    | 2812/5453 [00:05<00:05, 511.31it/s] 53%|█████▎    | 2866/5453 [00:05<00:04, 518.52it/s] 54%|█████▎    | 2919/5453 [00:05<00:05, 500.34it/s] 54%|█████▍    | 2970/5453 [00:05<00:05, 466.66it/s] 55%|█████▌    | 3023/5453 [00:05<00:05, 482.94it/s] 56%|█████▋    | 3073/5453 [00:05<00:04, 486.00it/s] 57%|█████▋    | 3124/5453 [00:05<00:04, 491.81it/s] 58%|█████▊    | 3174/5453 [00:05<00:04, 490.49it/s] 59%|█████▉    | 3224/5453 [00:06<00:04, 491.72it/s] 60%|██████    | 3276/5453 [00:06<00:04, 497.36it/s] 61%|██████    | 3327/5453 [00:06<00:04, 500.84it/s] 62%|██████▏   | 3380/5453 [00:06<00:04, 507.27it/s] 63%|██████▎   | 3432/5453 [00:06<00:03, 509.59it/s] 64%|██████▍   | 3485/5453 [00:06<00:03, 512.50it/s] 65%|██████▍   | 3538/5453 [00:06<00:03, 515.81it/s] 66%|██████▌   | 3591/5453 [00:06<00:03, 518.57it/s] 67%|██████▋   | 3643/5453 [00:06<00:03, 513.57it/s] 68%|██████▊   | 3695/5453 [00:06<00:03, 511.93it/s] 69%|██████▊   | 3747/5453 [00:07<00:05, 308.66it/s] 70%|██████▉   | 3799/5453 [00:07<00:04, 350.34it/s] 71%|███████   | 3849/5453 [00:07<00:04, 382.59it/s] 72%|███████▏  | 3901/5453 [00:07<00:03, 414.21it/s] 72%|███████▏  | 3952/5453 [00:07<00:03, 437.57it/s] 73%|███████▎  | 4003/5453 [00:07<00:03, 455.81it/s] 74%|███████▍  | 4053/5453 [00:07<00:02, 467.86it/s] 75%|███████▌  | 4105/5453 [00:07<00:02, 481.92it/s] 76%|███████▌  | 4156/5453 [00:08<00:02, 489.84it/s] 77%|███████▋  | 4208/5453 [00:08<00:02, 494.71it/s] 78%|███████▊  | 4259/5453 [00:08<00:02, 494.32it/s] 79%|███████▉  | 4310/5453 [00:08<00:02, 498.20it/s] 80%|███████▉  | 4361/5453 [00:08<00:02, 497.21it/s] 81%|████████  | 4412/5453 [00:08<00:02, 493.62it/s] 82%|████████▏ | 4462/5453 [00:08<00:02, 477.58it/s] 83%|████████▎ | 4511/5453 [00:08<00:01, 477.07it/s] 84%|████████▎ | 4559/5453 [00:08<00:01, 468.59it/s] 84%|████████▍ | 4606/5453 [00:09<00:01, 459.28it/s] 85%|████████▌ | 4654/5453 [00:09<00:01, 463.84it/s] 86%|████████▌ | 4701/5453 [00:09<00:01, 458.73it/s] 87%|████████▋ | 4748/5453 [00:09<00:01, 461.13it/s] 88%|████████▊ | 4797/5453 [00:09<00:01, 467.83it/s] 89%|████████▉ | 4847/5453 [00:09<00:01, 476.31it/s] 90%|████████▉ | 4896/5453 [00:09<00:01, 480.35it/s] 91%|█████████ | 4945/5453 [00:09<00:01, 482.22it/s] 92%|█████████▏| 4995/5453 [00:09<00:00, 486.52it/s] 92%|█████████▏| 5044/5453 [00:09<00:00, 486.06it/s] 93%|█████████▎| 5094/5453 [00:10<00:00, 488.97it/s] 94%|█████████▍| 5145/5453 [00:10<00:00, 493.73it/s] 95%|█████████▌| 5197/5453 [00:10<00:00, 498.79it/s] 96%|█████████▌| 5247/5453 [00:10<00:00, 490.60it/s] 97%|█████████▋| 5297/5453 [00:10<00:00, 484.78it/s] 98%|█████████▊| 5347/5453 [00:10<00:00, 488.79it/s] 99%|█████████▉| 5396/5453 [00:10<00:00, 486.26it/s]100%|█████████▉| 5445/5453 [00:10<00:00, 449.93it/s]100%|██████████| 5453/5453 [00:10<00:00, 503.09it/s]
INFO:__main__:Dataset has 5453 samples
INFO:__main__:Dataset data example: {'pyg_data': Data(x=[22, 19], edge_index=[2, 50], edge_attr=[50, 2]), 'activity': 5.028260409112222}
INFO:__main__:Train loader has 1 data
2726 2727
1363 1363
/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([1363])) that is different to the input size (torch.Size([1363, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:__main__:Epoch 0, Train Loss 53.972293853759766
INFO:__main__:Epoch 0, Valid Loss 52.37192153930664
INFO:__main__:Saving model at epoch 0
INFO:__main__:Epoch 1, Train Loss 53.60905075073242
INFO:__main__:Epoch 1, Valid Loss 52.028987884521484
INFO:__main__:Epoch 2, Train Loss 53.262664794921875
INFO:__main__:Epoch 2, Valid Loss 51.70376205444336
INFO:__main__:Epoch 3, Train Loss 52.93415451049805
INFO:__main__:Epoch 3, Valid Loss 51.40496826171875
INFO:__main__:Epoch 4, Train Loss 52.632328033447266
INFO:__main__:Epoch 4, Valid Loss 51.118690490722656
INFO:__main__:Epoch 5, Train Loss 52.3431396484375
INFO:__main__:Epoch 5, Valid Loss 50.84297180175781
INFO:__main__:Saving model at epoch 5
INFO:__main__:Epoch 6, Train Loss 52.06460189819336
INFO:__main__:Epoch 6, Valid Loss 50.578956604003906
INFO:__main__:Epoch 7, Train Loss 51.797882080078125
INFO:__main__:Epoch 7, Valid Loss 50.325077056884766
INFO:__main__:Epoch 8, Train Loss 51.5413932800293
INFO:__main__:Epoch 8, Valid Loss 50.089630126953125
INFO:__main__:Epoch 9, Train Loss 51.30351638793945
INFO:__main__:Epoch 9, Valid Loss 49.88685607910156
INFO:__main__:Epoch 10, Train Loss 51.09865188598633
INFO:__main__:Epoch 10, Valid Loss 49.70117950439453
INFO:__main__:Saving model at epoch 10
INFO:__main__:Epoch 11, Train Loss 50.91104507446289
INFO:__main__:Epoch 11, Valid Loss 49.53247833251953
INFO:__main__:Epoch 12, Train Loss 50.74058151245117
INFO:__main__:Epoch 12, Valid Loss 49.369686126708984
INFO:__main__:Epoch 13, Train Loss 50.576087951660156
INFO:__main__:Epoch 13, Valid Loss 49.21232604980469
INFO:__main__:Epoch 14, Train Loss 50.417076110839844
INFO:__main__:Epoch 14, Valid Loss 49.051368713378906
INFO:__main__:Epoch 15, Train Loss 50.25442123413086
INFO:__main__:Epoch 15, Valid Loss 48.88780212402344
INFO:__main__:Saving model at epoch 15
INFO:__main__:Epoch 16, Train Loss 50.089134216308594
INFO:__main__:Epoch 16, Valid Loss 48.737030029296875
INFO:__main__:Epoch 17, Train Loss 49.936767578125
INFO:__main__:Epoch 17, Valid Loss 48.58928680419922
INFO:__main__:Epoch 18, Train Loss 49.78746032714844
INFO:__main__:Epoch 18, Valid Loss 48.43374252319336
INFO:__main__:Epoch 19, Train Loss 49.63026428222656
INFO:__main__:Epoch 19, Valid Loss 48.27076721191406
INFO:__main__:Epoch 20, Train Loss 49.4655647277832
INFO:__main__:Epoch 20, Valid Loss 48.09915542602539
INFO:__main__:Saving model at epoch 20
INFO:__main__:Epoch 21, Train Loss 49.29212951660156
INFO:__main__:Epoch 21, Valid Loss 47.91727828979492
INFO:__main__:Epoch 22, Train Loss 49.10832214355469
INFO:__main__:Epoch 22, Valid Loss 47.72562026977539
INFO:__main__:Epoch 23, Train Loss 48.91461944580078
INFO:__main__:Epoch 23, Valid Loss 47.52154541015625
INFO:__main__:Epoch 24, Train Loss 48.708370208740234
INFO:__main__:Epoch 24, Valid Loss 47.30775451660156
INFO:__main__:Epoch 25, Train Loss 48.4922981262207
INFO:__main__:Epoch 25, Valid Loss 47.088687896728516
INFO:__main__:Saving model at epoch 25
INFO:__main__:Epoch 26, Train Loss 48.27088165283203
INFO:__main__:Epoch 26, Valid Loss 46.861061096191406
INFO:__main__:Epoch 27, Train Loss 48.04080581665039
INFO:__main__:Epoch 27, Valid Loss 46.63058853149414
INFO:__main__:Epoch 28, Train Loss 47.807857513427734
INFO:__main__:Epoch 28, Valid Loss 46.39118957519531
INFO:__main__:Epoch 29, Train Loss 47.56587219238281
INFO:__main__:Epoch 29, Valid Loss 46.146888732910156
INFO:__main__:Epoch 30, Train Loss 47.31892013549805
INFO:__main__:Epoch 30, Valid Loss 45.89561080932617
INFO:__main__:Saving model at epoch 30
INFO:__main__:Epoch 31, Train Loss 47.064918518066406
INFO:__main__:Epoch 31, Valid Loss 45.630958557128906
INFO:__main__:Epoch 32, Train Loss 46.79738998413086
INFO:__main__:Epoch 32, Valid Loss 45.357337951660156
INFO:__main__:Epoch 33, Train Loss 46.520782470703125
INFO:__main__:Epoch 33, Valid Loss 45.0714111328125
INFO:__main__:Epoch 34, Train Loss 46.231719970703125
INFO:__main__:Epoch 34, Valid Loss 44.77342224121094
INFO:__main__:Epoch 35, Train Loss 45.93045425415039
INFO:__main__:Epoch 35, Valid Loss 44.4637336730957
INFO:__main__:Saving model at epoch 35
INFO:__main__:Epoch 36, Train Loss 45.61735534667969
INFO:__main__:Epoch 36, Valid Loss 44.15177917480469
INFO:__main__:Epoch 37, Train Loss 45.30195236206055
INFO:__main__:Epoch 37, Valid Loss 43.831825256347656
INFO:__main__:Epoch 38, Train Loss 44.978431701660156
INFO:__main__:Epoch 38, Valid Loss 43.498687744140625
INFO:__main__:Epoch 39, Train Loss 44.64157485961914
INFO:__main__:Epoch 39, Valid Loss 43.15834426879883
INFO:__main__:Epoch 40, Train Loss 44.29741668701172
INFO:__main__:Epoch 40, Valid Loss 42.79899978637695
INFO:__main__:Saving model at epoch 40
INFO:__main__:Epoch 41, Train Loss 43.93402862548828
INFO:__main__:Epoch 41, Valid Loss 42.42298889160156
INFO:__main__:Epoch 42, Train Loss 43.55376434326172
INFO:__main__:Epoch 42, Valid Loss 42.028690338134766
INFO:__main__:Epoch 43, Train Loss 43.15498733520508
INFO:__main__:Epoch 43, Valid Loss 41.624114990234375
INFO:__main__:Epoch 44, Train Loss 42.74578857421875
INFO:__main__:Epoch 44, Valid Loss 41.20991134643555
INFO:__main__:Epoch 45, Train Loss 42.326820373535156
INFO:__main__:Epoch 45, Valid Loss 40.78126525878906
INFO:__main__:Saving model at epoch 45
INFO:__main__:Epoch 46, Train Loss 41.893218994140625
INFO:__main__:Epoch 46, Valid Loss 40.330467224121094
INFO:__main__:Epoch 47, Train Loss 41.43717956542969
INFO:__main__:Epoch 47, Valid Loss 39.85548400878906
INFO:__main__:Epoch 48, Train Loss 40.956634521484375
INFO:__main__:Epoch 48, Valid Loss 39.354339599609375
INFO:__main__:Epoch 49, Train Loss 40.4495849609375
INFO:__main__:Epoch 49, Valid Loss 38.831031799316406
INFO:__main__:Epoch 50, Train Loss 39.92007064819336
INFO:__main__:Epoch 50, Valid Loss 38.28560256958008
INFO:__main__:Saving model at epoch 50
INFO:__main__:Epoch 51, Train Loss 39.368106842041016
INFO:__main__:Epoch 51, Valid Loss 37.72068405151367
INFO:__main__:Epoch 52, Train Loss 38.79637145996094
INFO:__main__:Epoch 52, Valid Loss 37.13662338256836
INFO:__main__:Epoch 53, Train Loss 38.205196380615234
INFO:__main__:Epoch 53, Valid Loss 36.53287124633789
INFO:__main__:Epoch 54, Train Loss 37.594032287597656
INFO:__main__:Epoch 54, Valid Loss 35.9090576171875
INFO:__main__:Epoch 55, Train Loss 36.96247863769531
INFO:__main__:Epoch 55, Valid Loss 35.265384674072266
INFO:__main__:Saving model at epoch 55
INFO:__main__:Epoch 56, Train Loss 36.31073760986328
INFO:__main__:Epoch 56, Valid Loss 34.603668212890625
INFO:__main__:Epoch 57, Train Loss 35.640647888183594
INFO:__main__:Epoch 57, Valid Loss 33.919952392578125
INFO:__main__:Epoch 58, Train Loss 34.94816970825195
INFO:__main__:Epoch 58, Valid Loss 33.21396255493164
INFO:__main__:Epoch 59, Train Loss 34.2330322265625
INFO:__main__:Epoch 59, Valid Loss 32.48662185668945
INFO:__main__:Epoch 60, Train Loss 33.49614715576172
INFO:__main__:Epoch 60, Valid Loss 31.735450744628906
INFO:__main__:Saving model at epoch 60
INFO:__main__:Epoch 61, Train Loss 32.7349967956543
INFO:__main__:Epoch 61, Valid Loss 30.96209144592285
INFO:__main__:Epoch 62, Train Loss 31.951215744018555
INFO:__main__:Epoch 62, Valid Loss 30.167186737060547
INFO:__main__:Epoch 63, Train Loss 31.145448684692383
INFO:__main__:Epoch 63, Valid Loss 29.349794387817383
INFO:__main__:Epoch 64, Train Loss 30.31670379638672
INFO:__main__:Epoch 64, Valid Loss 28.512182235717773
INFO:__main__:Epoch 65, Train Loss 29.46727180480957
INFO:__main__:Epoch 65, Valid Loss 27.6533203125
INFO:__main__:Saving model at epoch 65
INFO:__main__:Epoch 66, Train Loss 28.59607696533203
INFO:__main__:Epoch 66, Valid Loss 26.775789260864258
INFO:__main__:Epoch 67, Train Loss 27.705717086791992
INFO:__main__:Epoch 67, Valid Loss 25.879127502441406
INFO:__main__:Epoch 68, Train Loss 26.795690536499023
INFO:__main__:Epoch 68, Valid Loss 24.962055206298828
INFO:__main__:Epoch 69, Train Loss 25.864667892456055
INFO:__main__:Epoch 69, Valid Loss 24.026315689086914
INFO:__main__:Epoch 70, Train Loss 24.91438865661621
INFO:__main__:Epoch 70, Valid Loss 23.07793426513672
INFO:__main__:Saving model at epoch 70
INFO:__main__:Epoch 71, Train Loss 23.950931549072266
INFO:__main__:Epoch 71, Valid Loss 22.115068435668945
INFO:__main__:Epoch 72, Train Loss 22.972389221191406
INFO:__main__:Epoch 72, Valid Loss 21.139497756958008
INFO:__main__:Epoch 73, Train Loss 21.98052406311035
INFO:__main__:Epoch 73, Valid Loss 20.15319061279297
INFO:__main__:Epoch 74, Train Loss 20.977294921875
INFO:__main__:Epoch 74, Valid Loss 19.158388137817383
INFO:__main__:Epoch 75, Train Loss 19.964929580688477
INFO:__main__:Epoch 75, Valid Loss 18.158645629882812
INFO:__main__:Saving model at epoch 75
INFO:__main__:Epoch 76, Train Loss 18.946992874145508
INFO:__main__:Epoch 76, Valid Loss 17.153810501098633
INFO:__main__:Epoch 77, Train Loss 17.92327117919922
INFO:__main__:Epoch 77, Valid Loss 16.156679153442383
INFO:__main__:Epoch 78, Train Loss 16.906747817993164
INFO:__main__:Epoch 78, Valid Loss 15.161025047302246
INFO:__main__:Epoch 79, Train Loss 15.891016960144043
INFO:__main__:Epoch 79, Valid Loss 14.171534538269043
INFO:__main__:Epoch 80, Train Loss 14.880786895751953
INFO:__main__:Epoch 80, Valid Loss 13.188843727111816
INFO:__main__:Saving model at epoch 80
INFO:__main__:Epoch 81, Train Loss 13.876632690429688
INFO:__main__:Epoch 81, Valid Loss 12.219144821166992
INFO:__main__:Epoch 82, Train Loss 12.884797096252441
INFO:__main__:Epoch 82, Valid Loss 11.266959190368652
INFO:__main__:Epoch 83, Train Loss 11.909826278686523
INFO:__main__:Epoch 83, Valid Loss 10.3357515335083
INFO:__main__:Epoch 84, Train Loss 10.955177307128906
INFO:__main__:Epoch 84, Valid Loss 9.429826736450195
INFO:__main__:Epoch 85, Train Loss 10.025177955627441
INFO:__main__:Epoch 85, Valid Loss 8.553828239440918
INFO:__main__:Saving model at epoch 85
INFO:__main__:Epoch 86, Train Loss 9.124496459960938
INFO:__main__:Epoch 86, Valid Loss 7.711706638336182
INFO:__main__:Epoch 87, Train Loss 8.257102012634277
INFO:__main__:Epoch 87, Valid Loss 6.908540725708008
INFO:__main__:Epoch 88, Train Loss 7.42812967300415
INFO:__main__:Epoch 88, Valid Loss 6.148592948913574
INFO:__main__:Epoch 89, Train Loss 6.6418867111206055
INFO:__main__:Epoch 89, Valid Loss 5.436295032501221
INFO:__main__:Epoch 90, Train Loss 5.9028730392456055
INFO:__main__:Epoch 90, Valid Loss 4.775506496429443
INFO:__main__:Saving model at epoch 90
INFO:__main__:Epoch 91, Train Loss 5.215014457702637
INFO:__main__:Epoch 91, Valid Loss 4.169830799102783
INFO:__main__:Epoch 92, Train Loss 4.581999778747559
INFO:__main__:Epoch 92, Valid Loss 3.6223278045654297
INFO:__main__:Epoch 93, Train Loss 4.006987571716309
INFO:__main__:Epoch 93, Valid Loss 3.1355466842651367
INFO:__main__:Epoch 94, Train Loss 3.4926462173461914
INFO:__main__:Epoch 94, Valid Loss 2.711270809173584
INFO:__main__:Epoch 95, Train Loss 3.040900230407715
INFO:__main__:Epoch 95, Valid Loss 2.3499019145965576
INFO:__main__:Saving model at epoch 95
INFO:__main__:Epoch 96, Train Loss 2.652280330657959
INFO:__main__:Epoch 96, Valid Loss 2.050736665725708
INFO:__main__:Epoch 97, Train Loss 2.326209783554077
INFO:__main__:Epoch 97, Valid Loss 1.8126543760299683
INFO:__main__:Epoch 98, Train Loss 2.061767816543579
INFO:__main__:Epoch 98, Valid Loss 1.632901668548584
INFO:__main__:Epoch 99, Train Loss 1.8563830852508545
INFO:__main__:Epoch 99, Valid Loss 1.5074824094772339
INFO:__main__:Epoch 100, Train Loss 1.7062464952468872
INFO:__main__:Epoch 100, Valid Loss 1.4312349557876587
INFO:__main__:Saving model at epoch 100
INFO:__main__:Epoch 101, Train Loss 1.606379747390747
INFO:__main__:Epoch 101, Valid Loss 1.398026704788208
INFO:__main__:Epoch 102, Train Loss 1.550845980644226
INFO:__main__:Epoch 102, Valid Loss 1.4008617401123047
INFO:__main__:Epoch 103, Train Loss 1.5328384637832642
INFO:__main__:Epoch 103, Valid Loss 1.432141661643982
INFO:__main__:Epoch 104, Train Loss 1.5449376106262207
INFO:__main__:Epoch 104, Valid Loss 1.483993411064148
INFO:__main__:Epoch 105, Train Loss 1.5794291496276855
INFO:__main__:Epoch 105, Valid Loss 1.5486382246017456
INFO:__main__:Epoch 106, Train Loss 1.6286662817001343
INFO:__main__:Epoch 106, Valid Loss 1.6187623739242554
INFO:__main__:Epoch 107, Train Loss 1.6854361295700073
INFO:__main__:Epoch 107, Valid Loss 1.6878639459609985
INFO:__main__:Epoch 108, Train Loss 1.7432994842529297
INFO:__main__:Epoch 108, Valid Loss 1.7505333423614502
INFO:__main__:Epoch 109, Train Loss 1.7968703508377075
INFO:__main__:Epoch 109, Valid Loss 1.802641749382019
INFO:__main__:Epoch 110, Train Loss 1.8420016765594482
INFO:__main__:Epoch 110, Valid Loss 1.8414212465286255
INFO:__main__:Epoch 111, Train Loss 1.875868558883667
INFO:__main__:Epoch 111, Valid Loss 1.8654378652572632
INFO:__main__:Epoch 112, Train Loss 1.8969444036483765
INFO:__main__:Epoch 112, Valid Loss 1.8744689226150513
INFO:__main__:Epoch 113, Train Loss 1.904882550239563
INFO:__main__:Epoch 113, Valid Loss 1.8693088293075562
INFO:__main__:Epoch 114, Train Loss 1.9003304243087769
INFO:__main__:Epoch 114, Valid Loss 1.8515275716781616
INFO:__main__:Epoch 115, Train Loss 1.8846931457519531
INFO:__main__:Epoch 115, Valid Loss 1.8232189416885376
INFO:__main__:Epoch 116, Train Loss 1.8598912954330444
INFO:__main__:Epoch 116, Valid Loss 1.7867605686187744
INFO:__main__:Epoch 117, Train Loss 1.8281259536743164
INFO:__main__:Epoch 117, Valid Loss 1.7445875406265259
INFO:__main__:Epoch 118, Train Loss 1.7916620969772339
INFO:__main__:Epoch 118, Valid Loss 1.6990411281585693
INFO:__main__:Epoch 119, Train Loss 1.752681016921997
INFO:__main__:Epoch 119, Valid Loss 1.6522332429885864
INFO:__main__:Epoch 120, Train Loss 1.7131519317626953
INFO:__main__:Epoch 120, Valid Loss 1.6060307025909424
INFO:__main__:Epoch 121, Train Loss 1.6748126745224
INFO:__main__:Epoch 121, Valid Loss 1.5619767904281616
INFO:__main__:Epoch 122, Train Loss 1.6391010284423828
INFO:__main__:Epoch 122, Valid Loss 1.5213189125061035
INFO:__main__:Epoch 123, Train Loss 1.607183575630188
INFO:__main__:Epoch 123, Valid Loss 1.4850926399230957
INFO:__main__:Epoch 124, Train Loss 1.580026388168335
INFO:__main__:Epoch 124, Valid Loss 1.4541434049606323
INFO:__main__:Epoch 125, Train Loss 1.55841863155365
INFO:__main__:Epoch 125, Valid Loss 1.4292558431625366
INFO:__main__:Epoch 126, Train Loss 1.543056607246399
INFO:__main__:Epoch 126, Valid Loss 1.4110721349716187
INFO:__main__:Epoch 127, Train Loss 1.5344330072402954
INFO:__main__:Epoch 127, Valid Loss 1.3998957872390747
INFO:__main__:Epoch 128, Train Loss 1.5326021909713745
INFO:__main__:Epoch 128, Valid Loss 1.3953704833984375
INFO:__main__:Epoch 129, Train Loss 1.5367944240570068
INFO:__main__:Epoch 129, Valid Loss 1.3960013389587402
INFO:__main__:Epoch 130, Train Loss 1.5447970628738403
INFO:__main__:Epoch 130, Valid Loss 1.3990693092346191
INFO:__main__:Epoch 131, Train Loss 1.55327308177948
INFO:__main__:Epoch 131, Valid Loss 1.40198814868927
INFO:__main__:Epoch 132, Train Loss 1.559451937675476
INFO:__main__:Epoch 132, Valid Loss 1.4033544063568115
INFO:__main__:Epoch 133, Train Loss 1.5620957612991333
INFO:__main__:Epoch 133, Valid Loss 1.402982473373413
INFO:__main__:Epoch 134, Train Loss 1.5613819360733032
INFO:__main__:Epoch 134, Valid Loss 1.4014536142349243
INFO:__main__:Epoch 135, Train Loss 1.5583704710006714
INFO:__main__:Epoch 135, Valid Loss 1.3994842767715454
INFO:__main__:Epoch 136, Train Loss 1.5541977882385254
INFO:__main__:Epoch 136, Valid Loss 1.3976449966430664
INFO:__main__:Epoch 137, Train Loss 1.5497876405715942
INFO:__main__:Epoch 137, Valid Loss 1.3962533473968506
INFO:__main__:Epoch 138, Train Loss 1.5457135438919067
INFO:__main__:Epoch 138, Valid Loss 1.3954081535339355
INFO:__main__:Epoch 139, Train Loss 1.5422254800796509
INFO:__main__:Epoch 139, Valid Loss 1.3950884342193604
INFO:__main__:Epoch 140, Train Loss 1.5394009351730347
INFO:__main__:Epoch 140, Valid Loss 1.3952075242996216
INFO:__main__:Epoch 141, Train Loss 1.5372108221054077
INFO:__main__:Epoch 141, Valid Loss 1.395658016204834
INFO:__main__:Epoch 142, Train Loss 1.5355676412582397
INFO:__main__:Epoch 142, Valid Loss 1.3963383436203003
INFO:__main__:Epoch 143, Train Loss 1.5343705415725708
INFO:__main__:Epoch 143, Valid Loss 1.3971627950668335
INFO:__main__:Epoch 144, Train Loss 1.5335237979888916
INFO:__main__:Epoch 144, Valid Loss 1.3980640172958374
INFO:__main__:Epoch 145, Train Loss 1.5329458713531494
INFO:__main__:Epoch 145, Valid Loss 1.398990511894226
INFO:__main__:Epoch 146, Train Loss 1.532569169998169
INFO:__main__:Epoch 146, Valid Loss 1.3999041318893433
INFO:__main__:Epoch 147, Train Loss 1.5323406457901
INFO:__main__:Epoch 147, Valid Loss 1.4007757902145386
INFO:__main__:Epoch 148, Train Loss 1.5322182178497314
INFO:__main__:Epoch 148, Valid Loss 1.4015846252441406
INFO:__main__:Epoch 149, Train Loss 1.5321688652038574
INFO:__main__:Epoch 149, Valid Loss 1.402315616607666
INFO:__main__:Epoch 150, Train Loss 1.5321671962738037
INFO:__main__:Epoch 150, Valid Loss 1.4029580354690552
INFO:__main__:Epoch 151, Train Loss 1.5321937799453735
INFO:__main__:Epoch 151, Valid Loss 1.403504490852356
INFO:__main__:Epoch 152, Train Loss 1.5322335958480835
INFO:__main__:Epoch 152, Valid Loss 1.4039515256881714
INFO:__main__:Epoch 153, Train Loss 1.5322755575180054
INFO:__main__:Epoch 153, Valid Loss 1.4042978286743164
INFO:__main__:Epoch 154, Train Loss 1.532312035560608
INFO:__main__:Epoch 154, Valid Loss 1.4045449495315552
INFO:__main__:Epoch 155, Train Loss 1.5323377847671509
INFO:__main__:Epoch 155, Valid Loss 1.4046963453292847
INFO:__main__:Epoch 156, Train Loss 1.5323498249053955
INFO:__main__:Epoch 156, Valid Loss 1.404758095741272
INFO:__main__:Epoch 157, Train Loss 1.532347321510315
INFO:__main__:Epoch 157, Valid Loss 1.4047383069992065
INFO:__main__:Epoch 158, Train Loss 1.5323307514190674
INFO:__main__:Epoch 158, Valid Loss 1.4046458005905151
INFO:__main__:Epoch 159, Train Loss 1.5323017835617065
INFO:__main__:Epoch 159, Valid Loss 1.40449059009552
INFO:__main__:Epoch 160, Train Loss 1.532263159751892
INFO:__main__:Epoch 160, Valid Loss 1.4042788743972778
INFO:__main__:Epoch 161, Train Loss 1.5322169065475464
INFO:__main__:Epoch 161, Valid Loss 1.4040225744247437
INFO:__main__:Epoch 162, Train Loss 1.5321660041809082
INFO:__main__:Epoch 162, Valid Loss 1.4037339687347412
INFO:__main__:Epoch 163, Train Loss 1.532114028930664
INFO:__main__:Epoch 163, Valid Loss 1.4034242630004883
INFO:__main__:Epoch 164, Train Loss 1.5320637226104736
INFO:__main__:Epoch 164, Valid Loss 1.4031028747558594
INFO:__main__:Epoch 165, Train Loss 1.5320173501968384
INFO:__main__:Epoch 165, Valid Loss 1.4027793407440186
INFO:__main__:Epoch 166, Train Loss 1.5319762229919434
INFO:__main__:Epoch 166, Valid Loss 1.4024615287780762
INFO:__main__:Epoch 167, Train Loss 1.5319414138793945
INFO:__main__:Epoch 167, Valid Loss 1.4021565914154053
INFO:__main__:Epoch 168, Train Loss 1.5319132804870605
INFO:__main__:Epoch 168, Valid Loss 1.4018703699111938
INFO:__main__:Epoch 169, Train Loss 1.5318913459777832
INFO:__main__:Epoch 169, Valid Loss 1.4016077518463135
INFO:__main__:Epoch 170, Train Loss 1.5318753719329834
INFO:__main__:Epoch 170, Valid Loss 1.4013727903366089
INFO:__main__:Epoch 171, Train Loss 1.531863808631897
INFO:__main__:Epoch 171, Valid Loss 1.4011679887771606
INFO:__main__:Epoch 172, Train Loss 1.5318551063537598
INFO:__main__:Epoch 172, Valid Loss 1.4009950160980225
INFO:__main__:Epoch 173, Train Loss 1.531848430633545
INFO:__main__:Epoch 173, Valid Loss 1.4008547067642212
INFO:__main__:Epoch 174, Train Loss 1.5318418741226196
INFO:__main__:Epoch 174, Valid Loss 1.4007470607757568
INFO:__main__:Epoch 175, Train Loss 1.531834602355957
INFO:__main__:Epoch 175, Valid Loss 1.4006714820861816
INFO:__main__:Epoch 176, Train Loss 1.5318257808685303
INFO:__main__:Epoch 176, Valid Loss 1.4006260633468628
INFO:__main__:Epoch 177, Train Loss 1.5318150520324707
INFO:__main__:Epoch 177, Valid Loss 1.4006093740463257
INFO:__main__:Epoch 178, Train Loss 1.5318018198013306
INFO:__main__:Epoch 178, Valid Loss 1.4006186723709106
INFO:__main__:Epoch 179, Train Loss 1.531786561012268
INFO:__main__:Epoch 179, Valid Loss 1.4006515741348267
INFO:__main__:Epoch 180, Train Loss 1.5317693948745728
INFO:__main__:Epoch 180, Valid Loss 1.4007046222686768
INFO:__main__:Epoch 181, Train Loss 1.5317509174346924
INFO:__main__:Epoch 181, Valid Loss 1.4007747173309326
INFO:__main__:Epoch 182, Train Loss 1.5317314863204956
INFO:__main__:Epoch 182, Valid Loss 1.4008585214614868
INFO:__main__:Epoch 183, Train Loss 1.5317116975784302
INFO:__main__:Epoch 183, Valid Loss 1.4009523391723633
INFO:__main__:Epoch 184, Train Loss 1.5316921472549438
INFO:__main__:Epoch 184, Valid Loss 1.4010528326034546
INFO:__main__:Epoch 185, Train Loss 1.5316731929779053
INFO:__main__:Epoch 185, Valid Loss 1.4011569023132324
INFO:__main__:Epoch 186, Train Loss 1.531654953956604
INFO:__main__:Epoch 186, Valid Loss 1.4012614488601685
INFO:__main__:Epoch 187, Train Loss 1.5316376686096191
INFO:__main__:Epoch 187, Valid Loss 1.4013631343841553
INFO:__main__:Epoch 188, Train Loss 1.5316213369369507
INFO:__main__:Epoch 188, Valid Loss 1.40146005153656
INFO:__main__:Epoch 189, Train Loss 1.5316061973571777
INFO:__main__:Epoch 189, Valid Loss 1.401549220085144
INFO:__main__:Epoch 190, Train Loss 1.5315916538238525
INFO:__main__:Epoch 190, Valid Loss 1.4016293287277222
INFO:__main__:Epoch 191, Train Loss 1.531577706336975
INFO:__main__:Epoch 191, Valid Loss 1.4016988277435303
INFO:__main__:Epoch 192, Train Loss 1.5315641164779663
INFO:__main__:Epoch 192, Valid Loss 1.4017566442489624
INFO:__main__:Epoch 193, Train Loss 1.5315507650375366
INFO:__main__:Epoch 193, Valid Loss 1.4018018245697021
INFO:__main__:Epoch 194, Train Loss 1.5315370559692383
INFO:__main__:Epoch 194, Valid Loss 1.4018347263336182
INFO:__main__:Epoch 195, Train Loss 1.531523585319519
INFO:__main__:Epoch 195, Valid Loss 1.4018551111221313
INFO:__main__:Epoch 196, Train Loss 1.5315096378326416
INFO:__main__:Epoch 196, Valid Loss 1.4018638134002686
INFO:__main__:Epoch 197, Train Loss 1.531495451927185
INFO:__main__:Epoch 197, Valid Loss 1.4018610715866089
INFO:__main__:Epoch 198, Train Loss 1.5314809083938599
INFO:__main__:Epoch 198, Valid Loss 1.4018481969833374
INFO:__main__:Epoch 199, Train Loss 1.531466007232666
INFO:__main__:Epoch 199, Valid Loss 1.4018263816833496
INFO:__main__:R2: 0.19862496815171338
INFO:__main__:Loss: 1.133091688156128
INFO:__main__:R2: 0.07220127298573364
INFO:__main__:Loss: 1.3118464946746826
nohup: ignoring input
INFO:__main__:name: ca9_25_init
INFO:__main__:base_fname: ca9_large
INFO:__main__:seed: 4
INFO:__main__:device: cuda:2
INFO:__main__:batch_size: 2048
INFO:__main__:epochs: 200
INFO:__main__:lr: 0.0003
INFO:__main__:log_interval: 1
INFO:__main__:save_interval: 5
INFO:__main__:save_path: /data02/gtguo/DEL/data/weights/refnet_ca9_transfer/
INFO:__main__:load_path: /data02/gtguo/DEL/data/weights/refnet/
INFO:__main__:target_name: ca9
INFO:__main__:fp_size: 2048
INFO:__main__:transfer_learning: False
INFO:__main__:transfer_learning_ratio: 0.5
INFO:__main__:num_workers: 8
INFO:__main__:enc_node_feat_dim: 19
INFO:__main__:enc_edge_feat_dim: 2
INFO:__main__:enc_node_embedding_size: 64
INFO:__main__:enc_edge_embedding_size: 64
INFO:__main__:enc_n_layers: 5
INFO:__main__:enc_gat_n_heads: 4
INFO:__main__:enc_gat_ffn_ratio: 4
INFO:__main__:enc_fp_embedding_size: 32
INFO:__main__:enc_fp_ffn_size: 128
INFO:__main__:enc_fp_gated: False
INFO:__main__:enc_fp_n_heads: 4
INFO:__main__:enc_fp_size: 256
INFO:__main__:enc_fp_to_gat_feedback: add
INFO:__main__:enc_gat_to_fp_pooling: mean
INFO:__main__:dec_node_input_size: 64
INFO:__main__:dec_node_emb_size: 64
INFO:__main__:dec_fp_input_size: 32
INFO:__main__:dec_fp_emb_size: 64
INFO:__main__:dec_output_size: 2
INFO:__main__:reg_input_size: 64
INFO:__main__:reg_hidden_size: 64
INFO:__main__:reg_output_size: 1
INFO:__main__:record_path: /data02/gtguo/DEL/data/records/refnet_ca9_transfer/
INFO:__main__:Model has 408772 parameters
INFO:__main__:Head has 4225 parameters
Splitting dataset:   0%|          | 0/2372674 [00:00<?, ?it/s]Splitting dataset:   1%|          | 16727/2372674 [00:00<00:14, 167261.79it/s]Splitting dataset:   1%|▏         | 34451/2372674 [00:00<00:13, 173126.69it/s]Splitting dataset:   2%|▏         | 51964/2372674 [00:00<00:13, 174035.43it/s]Splitting dataset:   3%|▎         | 69638/2372674 [00:00<00:13, 175097.75it/s]Splitting dataset:   4%|▎         | 87747/2372674 [00:00<00:12, 177253.39it/s]Splitting dataset:   4%|▍         | 105508/2372674 [00:00<00:12, 177360.31it/s]Splitting dataset:   5%|▌         | 123245/2372674 [00:00<00:12, 177008.30it/s]Splitting dataset:   6%|▌         | 140946/2372674 [00:00<00:12, 175039.83it/s]Splitting dataset:   7%|▋         | 158455/2372674 [00:00<00:12, 174154.67it/s]Splitting dataset:   7%|▋         | 175874/2372674 [00:01<00:12, 169140.51it/s]Splitting dataset:   8%|▊         | 192817/2372674 [00:01<00:13, 166376.12it/s]Splitting dataset:   9%|▉         | 210607/2372674 [00:01<00:12, 169773.15it/s]Splitting dataset:  10%|▉         | 229221/2372674 [00:01<00:12, 174621.77it/s]Splitting dataset:  10%|█         | 247740/2372674 [00:01<00:11, 177762.59it/s]Splitting dataset:  11%|█         | 265862/2372674 [00:01<00:11, 178792.13it/s]Splitting dataset:  12%|█▏        | 283760/2372674 [00:01<00:11, 177916.33it/s]Splitting dataset:  13%|█▎        | 303637/2372674 [00:01<00:11, 184127.82it/s]Splitting dataset:  14%|█▎        | 322250/2372674 [00:01<00:11, 184721.30it/s]Splitting dataset:  14%|█▍        | 341662/2372674 [00:01<00:10, 187527.82it/s]Splitting dataset:  15%|█▌        | 360424/2372674 [00:02<00:10, 186624.54it/s]Splitting dataset:  16%|█▌        | 379330/2372674 [00:02<00:10, 187349.39it/s]Splitting dataset:  17%|█▋        | 398299/2372674 [00:02<00:10, 188046.98it/s]Splitting dataset:  18%|█▊        | 417583/2372674 [00:02<00:10, 189479.17it/s]Splitting dataset:  18%|█▊        | 436925/2372674 [00:02<00:10, 190657.35it/s]Splitting dataset:  19%|█▉        | 456062/2372674 [00:02<00:10, 190868.89it/s]Splitting dataset:  20%|██        | 476113/2372674 [00:02<00:09, 193755.69it/s]Splitting dataset:  21%|██        | 496630/2372674 [00:02<00:09, 197165.07it/s]Splitting dataset:  22%|██▏       | 516348/2372674 [00:02<00:09, 194307.32it/s]Splitting dataset:  23%|██▎       | 535789/2372674 [00:02<00:09, 193160.16it/s]Splitting dataset:  23%|██▎       | 555350/2372674 [00:03<00:09, 193881.55it/s]Splitting dataset:  24%|██▍       | 574745/2372674 [00:03<00:09, 191159.43it/s]Splitting dataset:  25%|██▌       | 594603/2372674 [00:03<00:09, 193347.26it/s]Splitting dataset:  26%|██▌       | 613949/2372674 [00:03<00:09, 193056.97it/s]Splitting dataset:  27%|██▋       | 633263/2372674 [00:03<00:09, 188585.49it/s]Splitting dataset:  27%|██▋       | 652359/2372674 [00:03<00:09, 189275.70it/s]Splitting dataset:  28%|██▊       | 671306/2372674 [00:03<00:09, 188291.05it/s]Splitting dataset:  29%|██▉       | 691209/2372674 [00:03<00:08, 191460.10it/s]Splitting dataset:  30%|██▉       | 711362/2372674 [00:03<00:08, 194446.62it/s]Splitting dataset:  31%|███       | 731355/2372674 [00:03<00:08, 196075.39it/s]Splitting dataset:  32%|███▏      | 751027/2372674 [00:04<00:08, 196264.88it/s]Splitting dataset:  32%|███▏      | 770813/2372674 [00:04<00:08, 196738.90it/s]Splitting dataset:  33%|███▎      | 790564/2372674 [00:04<00:08, 196966.74it/s]Splitting dataset:  34%|███▍      | 810437/2372674 [00:04<00:07, 197491.70it/s]Splitting dataset:  35%|███▍      | 830189/2372674 [00:04<00:07, 196640.72it/s]Splitting dataset:  36%|███▌      | 849856/2372674 [00:04<00:07, 196386.91it/s]Splitting dataset:  37%|███▋      | 869497/2372674 [00:04<00:07, 195395.52it/s]Splitting dataset:  37%|███▋      | 889039/2372674 [00:04<00:07, 193112.58it/s]Splitting dataset:  38%|███▊      | 908357/2372674 [00:04<00:07, 185161.14it/s]Splitting dataset:  39%|███▉      | 926940/2372674 [00:04<00:07, 181402.84it/s]Splitting dataset:  40%|███▉      | 945132/2372674 [00:05<00:07, 179383.70it/s]Splitting dataset:  41%|████      | 963104/2372674 [00:05<00:08, 161403.89it/s]Splitting dataset:  41%|████▏     | 981240/2372674 [00:05<00:08, 166799.55it/s]Splitting dataset:  42%|████▏     | 1000520/2372674 [00:05<00:07, 174044.57it/s]Splitting dataset:  43%|████▎     | 1018785/2372674 [00:05<00:07, 176492.74it/s]Splitting dataset:  44%|████▎     | 1037317/2372674 [00:05<00:07, 179042.07it/s]Splitting dataset:  44%|████▍     | 1055365/2372674 [00:05<00:07, 178510.04it/s]Splitting dataset:  45%|████▌     | 1074246/2372674 [00:05<00:07, 181531.43it/s]Splitting dataset:  46%|████▌     | 1092770/2372674 [00:05<00:07, 182623.83it/s]Splitting dataset:  47%|████▋     | 1111088/2372674 [00:06<00:07, 180086.82it/s]Splitting dataset:  48%|████▊     | 1129397/2372674 [00:06<00:06, 180967.25it/s]Splitting dataset:  48%|████▊     | 1147726/2372674 [00:06<00:06, 181651.61it/s]Splitting dataset:  49%|████▉     | 1165916/2372674 [00:06<00:06, 180587.55it/s]Splitting dataset:  50%|████▉     | 1183993/2372674 [00:06<00:06, 178959.30it/s]Splitting dataset:  51%|█████     | 1201903/2372674 [00:06<00:06, 178831.33it/s]Splitting dataset:  51%|█████▏    | 1220645/2372674 [00:06<00:06, 181368.16it/s]Splitting dataset:  52%|█████▏    | 1238792/2372674 [00:06<00:06, 178596.71it/s]Splitting dataset:  53%|█████▎    | 1256666/2372674 [00:06<00:06, 176886.59it/s]Splitting dataset:  54%|█████▎    | 1274793/2372674 [00:06<00:06, 177284.88it/s]Splitting dataset:  54%|█████▍    | 1292530/2372674 [00:07<00:06, 171426.77it/s]Splitting dataset:  55%|█████▌    | 1309713/2372674 [00:07<00:06, 169858.48it/s]Splitting dataset:  56%|█████▌    | 1326725/2372674 [00:07<00:06, 167301.83it/s]Splitting dataset:  57%|█████▋    | 1343475/2372674 [00:07<00:06, 165767.50it/s]Splitting dataset:  57%|█████▋    | 1360064/2372674 [00:07<00:06, 165482.59it/s]Splitting dataset:  58%|█████▊    | 1376620/2372674 [00:07<00:06, 165263.95it/s]Splitting dataset:  59%|█████▊    | 1393206/2372674 [00:07<00:05, 165437.48it/s]Splitting dataset:  59%|█████▉    | 1410318/2372674 [00:07<00:05, 167120.09it/s]Splitting dataset:  60%|██████    | 1427734/2372674 [00:07<00:05, 169212.12it/s]Splitting dataset:  61%|██████    | 1445485/2372674 [00:07<00:05, 171683.67it/s]Splitting dataset:  62%|██████▏   | 1462659/2372674 [00:08<00:05, 171237.05it/s]Splitting dataset:  62%|██████▏   | 1479878/2372674 [00:08<00:05, 171515.39it/s]Splitting dataset:  63%|██████▎   | 1497033/2372674 [00:08<00:05, 170797.59it/s]Splitting dataset:  64%|██████▍   | 1514875/2372674 [00:08<00:04, 173069.09it/s]Splitting dataset:  65%|██████▍   | 1533286/2372674 [00:08<00:04, 176365.97it/s]Splitting dataset:  65%|██████▌   | 1551130/2372674 [00:08<00:04, 176982.54it/s]Splitting dataset:  66%|██████▌   | 1569476/2372674 [00:08<00:04, 178918.63it/s]Splitting dataset:  67%|██████▋   | 1587370/2372674 [00:08<00:04, 177664.26it/s]Splitting dataset:  68%|██████▊   | 1605140/2372674 [00:08<00:04, 175922.19it/s]Splitting dataset:  68%|██████▊   | 1622835/2372674 [00:08<00:04, 176224.31it/s]Splitting dataset:  69%|██████▉   | 1641045/2372674 [00:09<00:04, 177971.76it/s]Splitting dataset:  70%|██████▉   | 1658972/2372674 [00:09<00:04, 178355.74it/s]Splitting dataset:  71%|███████   | 1676971/2372674 [00:09<00:03, 178841.26it/s]Splitting dataset:  71%|███████▏  | 1694894/2372674 [00:09<00:03, 178954.68it/s]Splitting dataset:  72%|███████▏  | 1712792/2372674 [00:09<00:03, 177698.55it/s]Splitting dataset:  73%|███████▎  | 1730565/2372674 [00:09<00:03, 175586.00it/s]Splitting dataset:  74%|███████▎  | 1748131/2372674 [00:09<00:03, 171624.05it/s]Splitting dataset:  74%|███████▍  | 1765314/2372674 [00:09<00:03, 169676.78it/s]Splitting dataset:  75%|███████▌  | 1783084/2372674 [00:09<00:03, 172018.27it/s]Splitting dataset:  76%|███████▌  | 1801076/2372674 [00:09<00:03, 174341.70it/s]Splitting dataset:  77%|███████▋  | 1818526/2372674 [00:10<00:03, 174340.62it/s]Splitting dataset:  77%|███████▋  | 1835972/2372674 [00:10<00:03, 172345.08it/s]Splitting dataset:  78%|███████▊  | 1853218/2372674 [00:10<00:03, 170555.97it/s]Splitting dataset:  79%|███████▉  | 1871229/2372674 [00:10<00:02, 173369.85it/s]Splitting dataset:  80%|███████▉  | 1888821/2372674 [00:10<00:02, 174122.86it/s]Splitting dataset:  80%|████████  | 1906243/2372674 [00:10<00:02, 173725.62it/s]Splitting dataset:  81%|████████  | 1923915/2372674 [00:10<00:02, 174614.96it/s]Splitting dataset:  82%|████████▏ | 1941502/2372674 [00:10<00:02, 174986.96it/s]Splitting dataset:  83%|████████▎ | 1959005/2372674 [00:10<00:02, 173151.32it/s]Splitting dataset:  83%|████████▎ | 1976327/2372674 [00:11<00:02, 173060.02it/s]Splitting dataset:  84%|████████▍ | 1993638/2372674 [00:11<00:02, 171680.54it/s]Splitting dataset:  85%|████████▍ | 2010811/2372674 [00:11<00:02, 169385.20it/s]Splitting dataset:  85%|████████▌ | 2027758/2372674 [00:11<00:02, 169388.34it/s]Splitting dataset:  86%|████████▌ | 2045236/2372674 [00:11<00:01, 170983.99it/s]Splitting dataset:  87%|████████▋ | 2062341/2372674 [00:11<00:01, 170991.31it/s]Splitting dataset:  88%|████████▊ | 2079953/2372674 [00:11<00:01, 172517.74it/s]Splitting dataset:  88%|████████▊ | 2097209/2372674 [00:11<00:01, 170859.13it/s]Splitting dataset:  89%|████████▉ | 2114301/2372674 [00:11<00:01, 166799.43it/s]Splitting dataset:  90%|████████▉ | 2131003/2372674 [00:11<00:01, 165937.01it/s]Splitting dataset:  91%|█████████ | 2148164/2372674 [00:12<00:01, 167600.89it/s]Splitting dataset:  91%|█████████ | 2165033/2372674 [00:12<00:01, 167920.34it/s]Splitting dataset:  92%|█████████▏| 2183555/2372674 [00:12<00:01, 173055.29it/s]Splitting dataset:  93%|█████████▎| 2202129/2372674 [00:12<00:00, 176831.15it/s]Splitting dataset:  94%|█████████▎| 2220845/2372674 [00:12<00:00, 179911.98it/s]Splitting dataset:  94%|█████████▍| 2239496/2372674 [00:12<00:00, 181879.39it/s]Splitting dataset:  95%|█████████▌| 2258953/2372674 [00:12<00:00, 185675.14it/s]Splitting dataset:  96%|█████████▌| 2277966/2372674 [00:12<00:00, 187007.17it/s]Splitting dataset:  97%|█████████▋| 2296672/2372674 [00:12<00:00, 185515.69it/s]Splitting dataset:  98%|█████████▊| 2315230/2372674 [00:12<00:00, 181831.16it/s]Splitting dataset:  98%|█████████▊| 2333432/2372674 [00:13<00:00, 176469.43it/s]Splitting dataset:  99%|█████████▉| 2351118/2372674 [00:13<00:00, 172246.90it/s]Splitting dataset: 100%|█████████▉| 2368380/2372674 [00:13<00:00, 160764.74it/s]Splitting dataset: 100%|██████████| 2372674/2372674 [00:13<00:00, 178315.53it/s]
INFO:root:processing dataset...
  0%|          | 0/5453 [00:00<?, ?it/s]  1%|          | 44/5453 [00:00<00:12, 435.80it/s]  2%|▏         | 93/5453 [00:00<00:11, 466.39it/s]  3%|▎         | 145/5453 [00:00<00:10, 486.45it/s]  4%|▎         | 195/5453 [00:00<00:10, 487.22it/s]/data02/gtguo/DEL/pkg/utils/mol_feat.py:71: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matricesor `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484808560/work/aten/src/ATen/native/TensorShape.cpp:2981.)
  return torch.tensor(coo, dtype=torch.long).T
  4%|▍         | 244/5453 [00:00<00:10, 484.24it/s]  6%|▌         | 302/5453 [00:00<00:09, 515.92it/s]  6%|▋         | 354/5453 [00:00<00:10, 480.42it/s]  7%|▋         | 408/5453 [00:00<00:10, 498.15it/s]  8%|▊         | 459/5453 [00:00<00:10, 480.74it/s]  9%|▉         | 508/5453 [00:01<00:10, 466.29it/s] 10%|█         | 555/5453 [00:01<00:10, 467.12it/s] 11%|█         | 608/5453 [00:01<00:10, 483.28it/s] 12%|█▏        | 657/5453 [00:01<00:10, 439.54it/s] 13%|█▎        | 702/5453 [00:01<00:10, 441.68it/s] 14%|█▎        | 747/5453 [00:01<00:10, 438.68it/s] 15%|█▍        | 792/5453 [00:01<00:10, 429.13it/s] 15%|█▌        | 841/5453 [00:01<00:10, 445.08it/s] 16%|█▌        | 886/5453 [00:01<00:10, 443.30it/s] 17%|█▋        | 931/5453 [00:02<00:10, 444.13it/s] 18%|█▊        | 978/5453 [00:02<00:09, 450.11it/s] 19%|█▉        | 1029/5453 [00:02<00:09, 467.22it/s] 20%|█▉        | 1076/5453 [00:02<00:09, 460.14it/s] 21%|██        | 1125/5453 [00:02<00:09, 467.66it/s] 22%|██▏       | 1174/5453 [00:02<00:09, 473.55it/s] 22%|██▏       | 1226/5453 [00:02<00:08, 483.49it/s] 23%|██▎       | 1275/5453 [00:02<00:08, 479.97it/s] 24%|██▍       | 1324/5453 [00:02<00:08, 465.46it/s] 25%|██▌       | 1388/5453 [00:02<00:07, 514.91it/s] 26%|██▋       | 1441/5453 [00:03<00:07, 517.99it/s] 27%|██▋       | 1494/5453 [00:03<00:07, 521.21it/s] 28%|██▊       | 1547/5453 [00:03<00:07, 523.08it/s] 29%|██▉       | 1605/5453 [00:03<00:07, 538.33it/s] 30%|███       | 1659/5453 [00:03<00:07, 534.58it/s] 32%|███▏      | 1719/5453 [00:03<00:06, 550.90it/s] 33%|███▎      | 1775/5453 [00:03<00:06, 537.18it/s] 34%|███▎      | 1830/5453 [00:03<00:06, 540.58it/s] 35%|███▍      | 1885/5453 [00:03<00:07, 498.55it/s] 36%|███▌      | 1936/5453 [00:04<00:07, 485.94it/s] 37%|███▋      | 1991/5453 [00:04<00:06, 503.50it/s] 37%|███▋      | 2042/5453 [00:04<00:06, 503.28it/s] 38%|███▊      | 2095/5453 [00:04<00:06, 508.85it/s] 39%|███▉      | 2147/5453 [00:04<00:06, 509.86it/s] 40%|████      | 2205/5453 [00:04<00:06, 528.69it/s] 41%|████▏     | 2259/5453 [00:04<00:06, 504.86it/s] 42%|████▏     | 2311/5453 [00:04<00:06, 506.86it/s] 43%|████▎     | 2362/5453 [00:04<00:06, 495.30it/s] 44%|████▍     | 2422/5453 [00:04<00:05, 522.74it/s] 45%|████▌     | 2475/5453 [00:05<00:05, 509.58it/s] 46%|████▋     | 2527/5453 [00:05<00:06, 485.06it/s] 47%|████▋     | 2576/5453 [00:05<00:05, 486.42it/s] 48%|████▊     | 2634/5453 [00:05<00:05, 510.71it/s] 49%|████▉     | 2686/5453 [00:05<00:05, 486.15it/s] 50%|█████     | 2737/5453 [00:05<00:05, 491.76it/s] 51%|█████     | 2792/5453 [00:05<00:05, 506.77it/s] 52%|█████▏    | 2846/5453 [00:05<00:05, 515.68it/s] 53%|█████▎    | 2898/5453 [00:05<00:05, 501.25it/s] 54%|█████▍    | 2949/5453 [00:06<00:05, 464.73it/s] 55%|█████▌    | 3001/5453 [00:06<00:05, 477.28it/s] 56%|█████▌    | 3051/5453 [00:06<00:04, 481.23it/s] 57%|█████▋    | 3101/5453 [00:06<00:04, 484.36it/s] 58%|█████▊    | 3150/5453 [00:06<00:04, 484.31it/s] 59%|█████▊    | 3199/5453 [00:06<00:04, 485.57it/s] 60%|█████▉    | 3249/5453 [00:06<00:04, 488.87it/s] 61%|██████    | 3300/5453 [00:06<00:04, 494.16it/s] 61%|██████▏   | 3351/5453 [00:06<00:04, 498.59it/s] 62%|██████▏   | 3401/5453 [00:06<00:04, 498.67it/s] 63%|██████▎   | 3451/5453 [00:07<00:04, 498.37it/s] 64%|██████▍   | 3502/5453 [00:07<00:03, 500.46it/s] 65%|██████▌   | 3554/5453 [00:07<00:03, 505.61it/s] 66%|██████▌   | 3605/5453 [00:07<00:03, 498.68it/s] 67%|██████▋   | 3656/5453 [00:07<00:03, 501.29it/s] 68%|██████▊   | 3707/5453 [00:07<00:03, 501.65it/s] 69%|██████▉   | 3758/5453 [00:07<00:04, 340.27it/s] 70%|██████▉   | 3810/5453 [00:07<00:04, 379.24it/s] 71%|███████   | 3860/5453 [00:08<00:03, 407.40it/s] 72%|███████▏  | 3910/5453 [00:08<00:03, 430.64it/s] 73%|███████▎  | 3957/5453 [00:08<00:03, 438.08it/s] 73%|███████▎  | 4006/5453 [00:08<00:03, 449.84it/s] 74%|███████▍  | 4054/5453 [00:08<00:03, 447.56it/s] 75%|███████▌  | 4101/5453 [00:08<00:03, 450.42it/s] 76%|███████▌  | 4148/5453 [00:08<00:02, 454.95it/s] 77%|███████▋  | 4195/5453 [00:08<00:02, 451.97it/s] 78%|███████▊  | 4241/5453 [00:08<00:02, 448.07it/s] 79%|███████▊  | 4287/5453 [00:08<00:02, 450.11it/s] 79%|███████▉  | 4333/5453 [00:09<00:02, 439.56it/s] 80%|████████  | 4378/5453 [00:09<00:02, 440.74it/s] 81%|████████  | 4423/5453 [00:09<00:02, 434.75it/s] 82%|████████▏ | 4467/5453 [00:09<00:02, 433.85it/s] 83%|████████▎ | 4511/5453 [00:09<00:02, 433.36it/s] 84%|████████▎ | 4555/5453 [00:09<00:02, 426.55it/s] 84%|████████▍ | 4598/5453 [00:09<00:02, 424.28it/s] 85%|████████▌ | 4641/5453 [00:09<00:01, 421.53it/s] 86%|████████▌ | 4684/5453 [00:09<00:01, 417.84it/s] 87%|████████▋ | 4726/5453 [00:09<00:01, 415.87it/s] 87%|████████▋ | 4770/5453 [00:10<00:01, 421.37it/s] 88%|████████▊ | 4813/5453 [00:10<00:01, 415.45it/s] 89%|████████▉ | 4855/5453 [00:10<00:01, 414.38it/s] 90%|████████▉ | 4899/5453 [00:10<00:01, 419.91it/s] 91%|█████████ | 4942/5453 [00:10<00:01, 396.65it/s] 91%|█████████▏| 4982/5453 [00:10<00:01, 385.44it/s] 92%|█████████▏| 5021/5453 [00:10<00:01, 385.65it/s] 93%|█████████▎| 5063/5453 [00:10<00:00, 395.02it/s] 94%|█████████▎| 5106/5453 [00:10<00:00, 404.51it/s] 94%|█████████▍| 5150/5453 [00:11<00:00, 414.26it/s] 95%|█████████▌| 5194/5453 [00:11<00:00, 420.13it/s] 96%|█████████▌| 5237/5453 [00:11<00:00, 417.17it/s] 97%|█████████▋| 5279/5453 [00:11<00:00, 409.09it/s] 98%|█████████▊| 5321/5453 [00:11<00:00, 411.60it/s] 98%|█████████▊| 5364/5453 [00:11<00:00, 415.31it/s] 99%|█████████▉| 5406/5453 [00:11<00:00, 415.09it/s]100%|█████████▉| 5448/5453 [00:11<00:00, 373.57it/s]100%|██████████| 5453/5453 [00:11<00:00, 461.67it/s]
INFO:__main__:Dataset has 5453 samples
INFO:__main__:Dataset data example: {'pyg_data': Data(x=[22, 19], edge_index=[2, 50], edge_attr=[50, 2]), 'activity': 5.028260409112222}
INFO:__main__:Train loader has 1 data
2726 2727
1363 1363
/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([1363])) that is different to the input size (torch.Size([1363, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:__main__:Epoch 0, Train Loss 52.34691619873047
INFO:__main__:Epoch 0, Valid Loss 50.382808685302734
INFO:__main__:Saving model at epoch 0
INFO:__main__:Epoch 1, Train Loss 51.6003532409668
INFO:__main__:Epoch 1, Valid Loss 49.95374298095703
INFO:__main__:Epoch 2, Train Loss 51.1662483215332
INFO:__main__:Epoch 2, Valid Loss 49.70476531982422
INFO:__main__:Epoch 3, Train Loss 50.91509246826172
INFO:__main__:Epoch 3, Valid Loss 49.54143524169922
INFO:__main__:Epoch 4, Train Loss 50.74996566772461
INFO:__main__:Epoch 4, Valid Loss 49.38898468017578
INFO:__main__:Epoch 5, Train Loss 50.595829010009766
INFO:__main__:Epoch 5, Valid Loss 49.196327209472656
INFO:__main__:Saving model at epoch 5
INFO:__main__:Epoch 6, Train Loss 50.401023864746094
INFO:__main__:Epoch 6, Valid Loss 48.988189697265625
INFO:__main__:Epoch 7, Train Loss 50.19049835205078
INFO:__main__:Epoch 7, Valid Loss 48.79451370239258
INFO:__main__:Epoch 8, Train Loss 49.99496078491211
INFO:__main__:Epoch 8, Valid Loss 48.605987548828125
INFO:__main__:Epoch 9, Train Loss 49.80434799194336
INFO:__main__:Epoch 9, Valid Loss 48.41007614135742
INFO:__main__:Epoch 10, Train Loss 49.606388092041016
INFO:__main__:Epoch 10, Valid Loss 48.19405746459961
INFO:__main__:Saving model at epoch 10
INFO:__main__:Epoch 11, Train Loss 49.38821029663086
INFO:__main__:Epoch 11, Valid Loss 47.965843200683594
INFO:__main__:Epoch 12, Train Loss 49.157833099365234
INFO:__main__:Epoch 12, Valid Loss 47.709442138671875
INFO:__main__:Epoch 13, Train Loss 48.898704528808594
INFO:__main__:Epoch 13, Valid Loss 47.455116271972656
INFO:__main__:Epoch 14, Train Loss 48.6413688659668
INFO:__main__:Epoch 14, Valid Loss 47.20808792114258
INFO:__main__:Epoch 15, Train Loss 48.39175796508789
INFO:__main__:Epoch 15, Valid Loss 46.944007873535156
INFO:__main__:Saving model at epoch 15
INFO:__main__:Epoch 16, Train Loss 48.124813079833984
INFO:__main__:Epoch 16, Valid Loss 46.668434143066406
INFO:__main__:Epoch 17, Train Loss 47.84626007080078
INFO:__main__:Epoch 17, Valid Loss 46.3816032409668
INFO:__main__:Epoch 18, Train Loss 47.55630874633789
INFO:__main__:Epoch 18, Valid Loss 46.088382720947266
INFO:__main__:Epoch 19, Train Loss 47.26003646850586
INFO:__main__:Epoch 19, Valid Loss 45.78458786010742
INFO:__main__:Epoch 20, Train Loss 46.95301818847656
INFO:__main__:Epoch 20, Valid Loss 45.46320724487305
INFO:__main__:Saving model at epoch 20
INFO:__main__:Epoch 21, Train Loss 46.628273010253906
INFO:__main__:Epoch 21, Valid Loss 45.1203498840332
INFO:__main__:Epoch 22, Train Loss 46.28165817260742
INFO:__main__:Epoch 22, Valid Loss 44.76649475097656
INFO:__main__:Epoch 23, Train Loss 45.92390060424805
INFO:__main__:Epoch 23, Valid Loss 44.40166473388672
INFO:__main__:Epoch 24, Train Loss 45.55500411987305
INFO:__main__:Epoch 24, Valid Loss 44.0338134765625
INFO:__main__:Epoch 25, Train Loss 45.1829719543457
INFO:__main__:Epoch 25, Valid Loss 43.649749755859375
INFO:__main__:Saving model at epoch 25
INFO:__main__:Epoch 26, Train Loss 44.794620513916016
INFO:__main__:Epoch 26, Valid Loss 43.24478530883789
INFO:__main__:Epoch 27, Train Loss 44.38508224487305
INFO:__main__:Epoch 27, Valid Loss 42.8210334777832
INFO:__main__:Epoch 28, Train Loss 43.95656967163086
INFO:__main__:Epoch 28, Valid Loss 42.38827896118164
INFO:__main__:Epoch 29, Train Loss 43.51899719238281
INFO:__main__:Epoch 29, Valid Loss 41.9427490234375
INFO:__main__:Epoch 30, Train Loss 43.068382263183594
INFO:__main__:Epoch 30, Valid Loss 41.47903060913086
INFO:__main__:Saving model at epoch 30
INFO:__main__:Epoch 31, Train Loss 42.599342346191406
INFO:__main__:Epoch 31, Valid Loss 40.9969596862793
INFO:__main__:Epoch 32, Train Loss 42.1117057800293
INFO:__main__:Epoch 32, Valid Loss 40.496726989746094
INFO:__main__:Epoch 33, Train Loss 41.605628967285156
INFO:__main__:Epoch 33, Valid Loss 39.98505401611328
INFO:__main__:Epoch 34, Train Loss 41.08792495727539
INFO:__main__:Epoch 34, Valid Loss 39.461666107177734
INFO:__main__:Epoch 35, Train Loss 40.55827331542969
INFO:__main__:Epoch 35, Valid Loss 38.920345306396484
INFO:__main__:Saving model at epoch 35
INFO:__main__:Epoch 36, Train Loss 40.010555267333984
INFO:__main__:Epoch 36, Valid Loss 38.361576080322266
INFO:__main__:Epoch 37, Train Loss 39.445194244384766
INFO:__main__:Epoch 37, Valid Loss 37.78546905517578
INFO:__main__:Epoch 38, Train Loss 38.86221694946289
INFO:__main__:Epoch 38, Valid Loss 37.190765380859375
INFO:__main__:Epoch 39, Train Loss 38.2603874206543
INFO:__main__:Epoch 39, Valid Loss 36.57526397705078
INFO:__main__:Epoch 40, Train Loss 37.637367248535156
INFO:__main__:Epoch 40, Valid Loss 35.9366340637207
INFO:__main__:Saving model at epoch 40
INFO:__main__:Epoch 41, Train Loss 36.99083709716797
INFO:__main__:Epoch 41, Valid Loss 35.27397918701172
INFO:__main__:Epoch 42, Train Loss 36.31989288330078
INFO:__main__:Epoch 42, Valid Loss 34.58726501464844
INFO:__main__:Epoch 43, Train Loss 35.62449645996094
INFO:__main__:Epoch 43, Valid Loss 33.882328033447266
INFO:__main__:Epoch 44, Train Loss 34.910552978515625
INFO:__main__:Epoch 44, Valid Loss 33.160186767578125
INFO:__main__:Epoch 45, Train Loss 34.17902755737305
INFO:__main__:Epoch 45, Valid Loss 32.41116714477539
INFO:__main__:Saving model at epoch 45
INFO:__main__:Epoch 46, Train Loss 33.4201774597168
INFO:__main__:Epoch 46, Valid Loss 31.635875701904297
INFO:__main__:Epoch 47, Train Loss 32.63456726074219
INFO:__main__:Epoch 47, Valid Loss 30.835813522338867
INFO:__main__:Epoch 48, Train Loss 31.823640823364258
INFO:__main__:Epoch 48, Valid Loss 30.019147872924805
INFO:__main__:Epoch 49, Train Loss 30.99583625793457
INFO:__main__:Epoch 49, Valid Loss 29.177837371826172
INFO:__main__:Epoch 50, Train Loss 30.1428279876709
INFO:__main__:Epoch 50, Valid Loss 28.312326431274414
INFO:__main__:Saving model at epoch 50
INFO:__main__:Epoch 51, Train Loss 29.2650203704834
INFO:__main__:Epoch 51, Valid Loss 27.42481803894043
INFO:__main__:Epoch 52, Train Loss 28.364686965942383
INFO:__main__:Epoch 52, Valid Loss 26.513639450073242
INFO:__main__:Epoch 53, Train Loss 27.440092086791992
INFO:__main__:Epoch 53, Valid Loss 25.58194923400879
INFO:__main__:Epoch 54, Train Loss 26.49441146850586
INFO:__main__:Epoch 54, Valid Loss 24.62855339050293
INFO:__main__:Epoch 55, Train Loss 25.526397705078125
INFO:__main__:Epoch 55, Valid Loss 23.654308319091797
INFO:__main__:Saving model at epoch 55
INFO:__main__:Epoch 56, Train Loss 24.536890029907227
INFO:__main__:Epoch 56, Valid Loss 22.660354614257812
INFO:__main__:Epoch 57, Train Loss 23.526994705200195
INFO:__main__:Epoch 57, Valid Loss 21.648855209350586
INFO:__main__:Epoch 58, Train Loss 22.498865127563477
INFO:__main__:Epoch 58, Valid Loss 20.630125045776367
INFO:__main__:Epoch 59, Train Loss 21.462923049926758
INFO:__main__:Epoch 59, Valid Loss 19.599153518676758
INFO:__main__:Epoch 60, Train Loss 20.414016723632812
INFO:__main__:Epoch 60, Valid Loss 18.55744171142578
INFO:__main__:Saving model at epoch 60
INFO:__main__:Epoch 61, Train Loss 19.35361671447754
INFO:__main__:Epoch 61, Valid Loss 17.508127212524414
INFO:__main__:Epoch 62, Train Loss 18.28484535217285
INFO:__main__:Epoch 62, Valid Loss 16.45439338684082
INFO:__main__:Epoch 63, Train Loss 17.21087074279785
INFO:__main__:Epoch 63, Valid Loss 15.399477005004883
INFO:__main__:Epoch 64, Train Loss 16.134902954101562
INFO:__main__:Epoch 64, Valid Loss 14.345998764038086
INFO:__main__:Epoch 65, Train Loss 15.059518814086914
INFO:__main__:Epoch 65, Valid Loss 13.296980857849121
INFO:__main__:Saving model at epoch 65
INFO:__main__:Epoch 66, Train Loss 13.987716674804688
INFO:__main__:Epoch 66, Valid Loss 12.257837295532227
INFO:__main__:Epoch 67, Train Loss 12.924932479858398
INFO:__main__:Epoch 67, Valid Loss 11.2344388961792
INFO:__main__:Epoch 68, Train Loss 11.877056121826172
INFO:__main__:Epoch 68, Valid Loss 10.233641624450684
INFO:__main__:Epoch 69, Train Loss 10.8509521484375
INFO:__main__:Epoch 69, Valid Loss 9.25937557220459
INFO:__main__:Epoch 70, Train Loss 9.850622177124023
INFO:__main__:Epoch 70, Valid Loss 8.31865406036377
INFO:__main__:Saving model at epoch 70
INFO:__main__:Epoch 71, Train Loss 8.883028984069824
INFO:__main__:Epoch 71, Valid Loss 7.416322231292725
INFO:__main__:Epoch 72, Train Loss 7.9530534744262695
INFO:__main__:Epoch 72, Valid Loss 6.558618545532227
INFO:__main__:Epoch 73, Train Loss 7.0669755935668945
INFO:__main__:Epoch 73, Valid Loss 5.751183986663818
INFO:__main__:Epoch 74, Train Loss 6.23049259185791
INFO:__main__:Epoch 74, Valid Loss 5.000847816467285
INFO:__main__:Epoch 75, Train Loss 5.450523853302002
INFO:__main__:Epoch 75, Valid Loss 4.3140974044799805
INFO:__main__:Saving model at epoch 75
INFO:__main__:Epoch 76, Train Loss 4.7337141036987305
INFO:__main__:Epoch 76, Valid Loss 3.693047285079956
INFO:__main__:Epoch 77, Train Loss 4.082113742828369
INFO:__main__:Epoch 77, Valid Loss 3.144763946533203
INFO:__main__:Epoch 78, Train Loss 3.5031001567840576
INFO:__main__:Epoch 78, Valid Loss 2.6714115142822266
INFO:__main__:Epoch 79, Train Loss 2.9989280700683594
INFO:__main__:Epoch 79, Valid Loss 2.2752370834350586
INFO:__main__:Epoch 80, Train Loss 2.5720536708831787
INFO:__main__:Epoch 80, Valid Loss 1.9561429023742676
INFO:__main__:Saving model at epoch 80
INFO:__main__:Epoch 81, Train Loss 2.222547769546509
INFO:__main__:Epoch 81, Valid Loss 1.7130008935928345
INFO:__main__:Epoch 82, Train Loss 1.9495171308517456
INFO:__main__:Epoch 82, Valid Loss 1.542751669883728
INFO:__main__:Epoch 83, Train Loss 1.7501742839813232
INFO:__main__:Epoch 83, Valid Loss 1.4402148723602295
INFO:__main__:Epoch 84, Train Loss 1.619645118713379
INFO:__main__:Epoch 84, Valid Loss 1.3979265689849854
INFO:__main__:Epoch 85, Train Loss 1.5507409572601318
INFO:__main__:Epoch 85, Valid Loss 1.4067133665084839
INFO:__main__:Epoch 86, Train Loss 1.5345860719680786
INFO:__main__:Epoch 86, Valid Loss 1.4559178352355957
INFO:__main__:Epoch 87, Train Loss 1.5608147382736206
INFO:__main__:Epoch 87, Valid Loss 1.5339162349700928
INFO:__main__:Epoch 88, Train Loss 1.6180776357650757
INFO:__main__:Epoch 88, Valid Loss 1.6287988424301147
INFO:__main__:Epoch 89, Train Loss 1.694702386856079
INFO:__main__:Epoch 89, Valid Loss 1.7291127443313599
INFO:__main__:Epoch 90, Train Loss 1.7794277667999268
INFO:__main__:Epoch 90, Valid Loss 1.8245787620544434
INFO:__main__:Epoch 91, Train Loss 1.862109661102295
INFO:__main__:Epoch 91, Valid Loss 1.9066643714904785
INFO:__main__:Epoch 92, Train Loss 1.9342923164367676
INFO:__main__:Epoch 92, Valid Loss 1.9689269065856934
INFO:__main__:Epoch 93, Train Loss 1.9895570278167725
INFO:__main__:Epoch 93, Valid Loss 2.007021188735962
INFO:__main__:Epoch 94, Train Loss 2.0235512256622314
INFO:__main__:Epoch 94, Valid Loss 2.0183281898498535
INFO:__main__:Epoch 95, Train Loss 2.033661365509033
INFO:__main__:Epoch 95, Valid Loss 2.001251697540283
INFO:__main__:Epoch 96, Train Loss 2.018380641937256
INFO:__main__:Epoch 96, Valid Loss 1.954436182975769
INFO:__main__:Epoch 97, Train Loss 1.9766334295272827
INFO:__main__:Epoch 97, Valid Loss 1.8756808042526245
INFO:__main__:Epoch 98, Train Loss 1.9069061279296875
INFO:__main__:Epoch 98, Valid Loss 1.7639061212539673
INFO:__main__:Epoch 99, Train Loss 1.8093161582946777
INFO:__main__:Epoch 99, Valid Loss 1.6269609928131104
INFO:__main__:Epoch 100, Train Loss 1.6931025981903076
INFO:__main__:Epoch 100, Valid Loss 1.489519476890564
INFO:__main__:Epoch 101, Train Loss 1.5843448638916016
INFO:__main__:Epoch 101, Valid Loss 1.4028090238571167
INFO:__main__:Epoch 102, Train Loss 1.5339436531066895
INFO:__main__:Epoch 102, Valid Loss 1.4166630506515503
INFO:__main__:Epoch 103, Train Loss 1.5852683782577515
INFO:__main__:Epoch 103, Valid Loss 1.4774165153503418
INFO:__main__:Epoch 104, Train Loss 1.6689352989196777
INFO:__main__:Epoch 104, Valid Loss 1.4854991436004639
INFO:__main__:Epoch 105, Train Loss 1.6792553663253784
INFO:__main__:Epoch 105, Valid Loss 1.4461841583251953
INFO:__main__:Epoch 106, Train Loss 1.6277687549591064
INFO:__main__:Epoch 106, Valid Loss 1.4085808992385864
INFO:__main__:Epoch 107, Train Loss 1.5721020698547363
INFO:__main__:Epoch 107, Valid Loss 1.3956266641616821
INFO:__main__:Epoch 108, Train Loss 1.5410085916519165
INFO:__main__:Epoch 108, Valid Loss 1.4037548303604126
INFO:__main__:Epoch 109, Train Loss 1.533982276916504
INFO:__main__:Epoch 109, Valid Loss 1.420986294746399
INFO:__main__:Epoch 110, Train Loss 1.5399152040481567
INFO:__main__:Epoch 110, Valid Loss 1.43807852268219
INFO:__main__:Epoch 111, Train Loss 1.549347162246704
INFO:__main__:Epoch 111, Valid Loss 1.4503545761108398
INFO:__main__:Epoch 112, Train Loss 1.5570520162582397
INFO:__main__:Epoch 112, Valid Loss 1.4564017057418823
INFO:__main__:Epoch 113, Train Loss 1.5610352754592896
INFO:__main__:Epoch 113, Valid Loss 1.4565962553024292
INFO:__main__:Epoch 114, Train Loss 1.5611636638641357
INFO:__main__:Epoch 114, Valid Loss 1.452122449874878
INFO:__main__:Epoch 115, Train Loss 1.558200716972351
INFO:__main__:Epoch 115, Valid Loss 1.444423794746399
INFO:__main__:Epoch 116, Train Loss 1.5532503128051758
INFO:__main__:Epoch 116, Valid Loss 1.4349677562713623
INFO:__main__:Epoch 117, Train Loss 1.547485589981079
INFO:__main__:Epoch 117, Valid Loss 1.4250998497009277
INFO:__main__:Epoch 118, Train Loss 1.5419803857803345
INFO:__main__:Epoch 118, Valid Loss 1.4159361124038696
INFO:__main__:Epoch 119, Train Loss 1.537584900856018
INFO:__main__:Epoch 119, Valid Loss 1.408282995223999
INFO:__main__:Epoch 120, Train Loss 1.5348340272903442
INFO:__main__:Epoch 120, Valid Loss 1.4025638103485107
INFO:__main__:Epoch 121, Train Loss 1.5338813066482544
INFO:__main__:Epoch 121, Valid Loss 1.3988149166107178
INFO:__main__:Epoch 122, Train Loss 1.534501075744629
INFO:__main__:Epoch 122, Valid Loss 1.3967444896697998
INFO:__main__:Epoch 123, Train Loss 1.53616464138031
INFO:__main__:Epoch 123, Valid Loss 1.3958561420440674
INFO:__main__:Epoch 124, Train Loss 1.538185954093933
INFO:__main__:Epoch 124, Valid Loss 1.3956161737442017
INFO:__main__:Epoch 125, Train Loss 1.5399084091186523
INFO:__main__:Epoch 125, Valid Loss 1.395604133605957
INFO:__main__:Saving model at epoch 125
INFO:__main__:Epoch 126, Train Loss 1.5408666133880615
INFO:__main__:Epoch 126, Valid Loss 1.3956035375595093
INFO:__main__:Epoch 127, Train Loss 1.540879726409912
INFO:__main__:Epoch 127, Valid Loss 1.3956071138381958
INFO:__main__:Epoch 128, Train Loss 1.5400453805923462
INFO:__main__:Epoch 128, Valid Loss 1.3957496881484985
INFO:__main__:Epoch 129, Train Loss 1.5386581420898438
INFO:__main__:Epoch 129, Valid Loss 1.3962112665176392
INFO:__main__:Epoch 130, Train Loss 1.5370856523513794
INFO:__main__:Epoch 130, Valid Loss 1.3971271514892578
INFO:__main__:Epoch 131, Train Loss 1.535658597946167
INFO:__main__:Epoch 131, Valid Loss 1.3985358476638794
INFO:__main__:Epoch 132, Train Loss 1.5346002578735352
INFO:__main__:Epoch 132, Valid Loss 1.4003686904907227
INFO:__main__:Epoch 133, Train Loss 1.534002661705017
INFO:__main__:Epoch 133, Valid Loss 1.4024710655212402
INFO:__main__:Epoch 134, Train Loss 1.533842921257019
INFO:__main__:Epoch 134, Valid Loss 1.4046401977539062
INFO:__main__:Epoch 135, Train Loss 1.5340185165405273
INFO:__main__:Epoch 135, Valid Loss 1.4066673517227173
INFO:__main__:Epoch 136, Train Loss 1.5343893766403198
INFO:__main__:Epoch 136, Valid Loss 1.4083713293075562
INFO:__main__:Epoch 137, Train Loss 1.534814715385437
INFO:__main__:Epoch 137, Valid Loss 1.4096204042434692
INFO:__main__:Epoch 138, Train Loss 1.5351790189743042
INFO:__main__:Epoch 138, Valid Loss 1.4103413820266724
INFO:__main__:Epoch 139, Train Loss 1.535406231880188
INFO:__main__:Epoch 139, Valid Loss 1.4105209112167358
INFO:__main__:Epoch 140, Train Loss 1.5354626178741455
INFO:__main__:Epoch 140, Valid Loss 1.410199522972107
INFO:__main__:Epoch 141, Train Loss 1.5353546142578125
INFO:__main__:Epoch 141, Valid Loss 1.4094562530517578
INFO:__main__:Epoch 142, Train Loss 1.5351179838180542
INFO:__main__:Epoch 142, Valid Loss 1.4083960056304932
INFO:__main__:Epoch 143, Train Loss 1.534805178642273
INFO:__main__:Epoch 143, Valid Loss 1.4071344137191772
INFO:__main__:Epoch 144, Train Loss 1.534475564956665
INFO:__main__:Epoch 144, Valid Loss 1.4057857990264893
INFO:__main__:Epoch 145, Train Loss 1.5341813564300537
INFO:__main__:Epoch 145, Valid Loss 1.4044504165649414
INFO:__main__:Epoch 146, Train Loss 1.5339611768722534
INFO:__main__:Epoch 146, Valid Loss 1.4032087326049805
INFO:__main__:Epoch 147, Train Loss 1.533834457397461
INFO:__main__:Epoch 147, Valid Loss 1.4021167755126953
INFO:__main__:Epoch 148, Train Loss 1.5338001251220703
INFO:__main__:Epoch 148, Valid Loss 1.4012064933776855
INFO:__main__:Epoch 149, Train Loss 1.5338406562805176
INFO:__main__:Epoch 149, Valid Loss 1.4004902839660645
INFO:__main__:Epoch 150, Train Loss 1.533927321434021
INFO:__main__:Epoch 150, Valid Loss 1.3999660015106201
INFO:__main__:Epoch 151, Train Loss 1.5340276956558228
INFO:__main__:Epoch 151, Valid Loss 1.3996224403381348
INFO:__main__:Epoch 152, Train Loss 1.5341126918792725
INFO:__main__:Epoch 152, Valid Loss 1.3994450569152832
INFO:__main__:Epoch 153, Train Loss 1.5341622829437256
INFO:__main__:Epoch 153, Valid Loss 1.3994189500808716
INFO:__main__:Epoch 154, Train Loss 1.5341668128967285
INFO:__main__:Epoch 154, Valid Loss 1.3995290994644165
INFO:__main__:Epoch 155, Train Loss 1.5341293811798096
INFO:__main__:Epoch 155, Valid Loss 1.3997617959976196
INFO:__main__:Epoch 156, Train Loss 1.5340609550476074
INFO:__main__:Epoch 156, Valid Loss 1.400100588798523
INFO:__main__:Epoch 157, Train Loss 1.5339770317077637
INFO:__main__:Epoch 157, Valid Loss 1.4005266427993774
INFO:__main__:Epoch 158, Train Loss 1.533894419670105
INFO:__main__:Epoch 158, Valid Loss 1.4010164737701416
INFO:__main__:Epoch 159, Train Loss 1.5338261127471924
INFO:__main__:Epoch 159, Valid Loss 1.4015429019927979
INFO:__main__:Epoch 160, Train Loss 1.5337800979614258
INFO:__main__:Epoch 160, Valid Loss 1.4020754098892212
INFO:__main__:Epoch 161, Train Loss 1.533758521080017
INFO:__main__:Epoch 161, Valid Loss 1.402583122253418
INFO:__main__:Epoch 162, Train Loss 1.533758521080017
INFO:__main__:Epoch 162, Valid Loss 1.4030370712280273
INFO:__main__:Epoch 163, Train Loss 1.5337735414505005
INFO:__main__:Epoch 163, Valid Loss 1.4034126996994019
INFO:__main__:Epoch 164, Train Loss 1.5337952375411987
INFO:__main__:Epoch 164, Valid Loss 1.4036920070648193
INFO:__main__:Epoch 165, Train Loss 1.5338159799575806
INFO:__main__:Epoch 165, Valid Loss 1.4038643836975098
INFO:__main__:Epoch 166, Train Loss 1.533829689025879
INFO:__main__:Epoch 166, Valid Loss 1.403928279876709
INFO:__main__:Epoch 167, Train Loss 1.5338330268859863
INFO:__main__:Epoch 167, Valid Loss 1.4038892984390259
INFO:__main__:Epoch 168, Train Loss 1.5338257551193237
INFO:__main__:Epoch 168, Valid Loss 1.4037597179412842
INFO:__main__:Epoch 169, Train Loss 1.5338094234466553
INFO:__main__:Epoch 169, Valid Loss 1.4035569429397583
INFO:__main__:Epoch 170, Train Loss 1.5337873697280884
INFO:__main__:Epoch 170, Valid Loss 1.4033008813858032
INFO:__main__:Epoch 171, Train Loss 1.5337636470794678
INFO:__main__:Epoch 171, Valid Loss 1.4030126333236694
INFO:__main__:Epoch 172, Train Loss 1.533742070198059
INFO:__main__:Epoch 172, Valid Loss 1.4027129411697388
INFO:__main__:Epoch 173, Train Loss 1.5337252616882324
INFO:__main__:Epoch 173, Valid Loss 1.402420163154602
INFO:__main__:Epoch 174, Train Loss 1.5337146520614624
INFO:__main__:Epoch 174, Valid Loss 1.4021497964859009
INFO:__main__:Epoch 175, Train Loss 1.5337101221084595
INFO:__main__:Epoch 175, Valid Loss 1.401914119720459
INFO:__main__:Epoch 176, Train Loss 1.5337098836898804
INFO:__main__:Epoch 176, Valid Loss 1.401721477508545
INFO:__main__:Epoch 177, Train Loss 1.5337125062942505
INFO:__main__:Epoch 177, Valid Loss 1.401577353477478
INFO:__main__:Epoch 178, Train Loss 1.5337154865264893
INFO:__main__:Epoch 178, Valid Loss 1.4014837741851807
INFO:__main__:Epoch 179, Train Loss 1.5337170362472534
INFO:__main__:Epoch 179, Valid Loss 1.4014402627944946
INFO:__main__:Epoch 180, Train Loss 1.5337159633636475
INFO:__main__:Epoch 180, Valid Loss 1.4014443159103394
INFO:__main__:Epoch 181, Train Loss 1.5337121486663818
INFO:__main__:Epoch 181, Valid Loss 1.4014910459518433
INFO:__main__:Epoch 182, Train Loss 1.5337055921554565
INFO:__main__:Epoch 182, Valid Loss 1.401573657989502
INFO:__main__:Epoch 183, Train Loss 1.533697247505188
INFO:__main__:Epoch 183, Valid Loss 1.4016849994659424
INFO:__main__:Epoch 184, Train Loss 1.5336884260177612
INFO:__main__:Epoch 184, Valid Loss 1.4018166065216064
INFO:__main__:Epoch 185, Train Loss 1.5336799621582031
INFO:__main__:Epoch 185, Valid Loss 1.4019590616226196
INFO:__main__:Epoch 186, Train Loss 1.533672571182251
INFO:__main__:Epoch 186, Valid Loss 1.402103304862976
INFO:__main__:Epoch 187, Train Loss 1.5336668491363525
INFO:__main__:Epoch 187, Valid Loss 1.4022411108016968
INFO:__main__:Epoch 188, Train Loss 1.5336627960205078
INFO:__main__:Epoch 188, Valid Loss 1.4023643732070923
INFO:__main__:Epoch 189, Train Loss 1.5336599349975586
INFO:__main__:Epoch 189, Valid Loss 1.402467131614685
INFO:__main__:Epoch 190, Train Loss 1.5336579084396362
INFO:__main__:Epoch 190, Valid Loss 1.4025448560714722
INFO:__main__:Epoch 191, Train Loss 1.5336560010910034
INFO:__main__:Epoch 191, Valid Loss 1.402595043182373
INFO:__main__:Epoch 192, Train Loss 1.5336538553237915
INFO:__main__:Epoch 192, Valid Loss 1.4026168584823608
INFO:__main__:Epoch 193, Train Loss 1.5336508750915527
INFO:__main__:Epoch 193, Valid Loss 1.4026117324829102
INFO:__main__:Epoch 194, Train Loss 1.533647060394287
INFO:__main__:Epoch 194, Valid Loss 1.4025822877883911
INFO:__main__:Epoch 195, Train Loss 1.5336427688598633
INFO:__main__:Epoch 195, Valid Loss 1.402532696723938
INFO:__main__:Epoch 196, Train Loss 1.5336380004882812
INFO:__main__:Epoch 196, Valid Loss 1.402467966079712
INFO:__main__:Epoch 197, Train Loss 1.533632755279541
INFO:__main__:Epoch 197, Valid Loss 1.4023934602737427
INFO:__main__:Epoch 198, Train Loss 1.5336278676986694
INFO:__main__:Epoch 198, Valid Loss 1.4023144245147705
INFO:__main__:Epoch 199, Train Loss 1.5336233377456665
INFO:__main__:Epoch 199, Valid Loss 1.402235984802246
INFO:__main__:R2: 0.26075148207494836
INFO:__main__:Loss: 1.0452488660812378
INFO:__main__:R2: 0.07232695095135044
INFO:__main__:Loss: 1.311668872833252
nohup: ignoring input
INFO:__main__:name: ca9_50
INFO:__main__:base_fname: ca9_large
INFO:__main__:seed: 4
INFO:__main__:device: cuda:2
INFO:__main__:batch_size: 2048
INFO:__main__:epochs: 200
INFO:__main__:lr: 0.0003
INFO:__main__:log_interval: 1
INFO:__main__:save_interval: 5
INFO:__main__:save_path: /data02/gtguo/DEL/data/weights/refnet_ca9_transfer/
INFO:__main__:load_path: /data02/gtguo/DEL/data/weights/refnet/
INFO:__main__:target_name: ca9
INFO:__main__:fp_size: 2048
INFO:__main__:transfer_learning: True
INFO:__main__:transfer_learning_ratio: 0.625
INFO:__main__:num_workers: 8
INFO:__main__:enc_node_feat_dim: 19
INFO:__main__:enc_edge_feat_dim: 2
INFO:__main__:enc_node_embedding_size: 64
INFO:__main__:enc_edge_embedding_size: 64
INFO:__main__:enc_n_layers: 5
INFO:__main__:enc_gat_n_heads: 4
INFO:__main__:enc_gat_ffn_ratio: 4
INFO:__main__:enc_fp_embedding_size: 32
INFO:__main__:enc_fp_ffn_size: 128
INFO:__main__:enc_fp_gated: False
INFO:__main__:enc_fp_n_heads: 4
INFO:__main__:enc_fp_size: 256
INFO:__main__:enc_fp_to_gat_feedback: add
INFO:__main__:enc_gat_to_fp_pooling: mean
INFO:__main__:dec_node_input_size: 64
INFO:__main__:dec_node_emb_size: 64
INFO:__main__:dec_fp_input_size: 32
INFO:__main__:dec_fp_emb_size: 64
INFO:__main__:dec_output_size: 2
INFO:__main__:reg_input_size: 64
INFO:__main__:reg_hidden_size: 64
INFO:__main__:reg_output_size: 1
INFO:__main__:record_path: /data02/gtguo/DEL/data/records/refnet_ca9_transfer/
INFO:__main__:Model has 408772 parameters
INFO:__main__:Head has 4225 parameters
Splitting dataset:   0%|          | 0/2372674 [00:00<?, ?it/s]Splitting dataset:   1%|          | 18013/2372674 [00:00<00:13, 180100.54it/s]Splitting dataset:   2%|▏         | 36714/2372674 [00:00<00:12, 184158.30it/s]Splitting dataset:   2%|▏         | 55354/2372674 [00:00<00:12, 185175.48it/s]Splitting dataset:   3%|▎         | 74085/2372674 [00:00<00:12, 186012.41it/s]Splitting dataset:   4%|▍         | 93305/2372674 [00:00<00:12, 188239.76it/s]Splitting dataset:   5%|▍         | 112277/2372674 [00:00<00:11, 188736.04it/s]Splitting dataset:   6%|▌         | 131151/2372674 [00:00<00:11, 186863.42it/s]Splitting dataset:   6%|▋         | 150893/2372674 [00:00<00:11, 190196.37it/s]Splitting dataset:   7%|▋         | 170389/2372674 [00:00<00:11, 191675.88it/s]Splitting dataset:   8%|▊         | 190398/2372674 [00:01<00:11, 194260.90it/s]Splitting dataset:   9%|▉         | 210694/2372674 [00:01<00:10, 196914.36it/s]Splitting dataset:  10%|▉         | 230582/2372674 [00:01<00:10, 197507.71it/s]Splitting dataset:  11%|█         | 251228/2372674 [00:01<00:10, 200213.62it/s]Splitting dataset:  11%|█▏        | 271252/2372674 [00:01<00:10, 199864.41it/s]Splitting dataset:  12%|█▏        | 291724/2372674 [00:01<00:10, 201321.40it/s]Splitting dataset:  13%|█▎        | 311891/2372674 [00:01<00:10, 201421.92it/s]Splitting dataset:  14%|█▍        | 332035/2372674 [00:01<00:10, 195996.41it/s]Splitting dataset:  15%|█▍        | 352206/2372674 [00:01<00:10, 197677.25it/s]Splitting dataset:  16%|█▌        | 372000/2372674 [00:01<00:10, 197284.38it/s]Splitting dataset:  17%|█▋        | 391747/2372674 [00:02<00:10, 195447.94it/s]Splitting dataset:  17%|█▋        | 411307/2372674 [00:02<00:10, 189482.79it/s]Splitting dataset:  18%|█▊        | 430298/2372674 [00:02<00:10, 184660.50it/s]Splitting dataset:  19%|█▉        | 449186/2372674 [00:02<00:10, 185870.99it/s]Splitting dataset:  20%|█▉        | 467809/2372674 [00:02<00:10, 184095.15it/s]Splitting dataset:  21%|██        | 487022/2372674 [00:02<00:10, 186432.94it/s]Splitting dataset:  21%|██▏       | 507033/2372674 [00:02<00:09, 190454.52it/s]Splitting dataset:  22%|██▏       | 526291/2372674 [00:02<00:09, 191079.51it/s]Splitting dataset:  23%|██▎       | 546009/2372674 [00:02<00:09, 192890.04it/s]Splitting dataset:  24%|██▍       | 566617/2372674 [00:02<00:09, 196816.32it/s]Splitting dataset:  25%|██▍       | 586445/2372674 [00:03<00:09, 197250.45it/s]Splitting dataset:  26%|██▌       | 606180/2372674 [00:03<00:08, 197273.24it/s]Splitting dataset:  26%|██▋       | 625914/2372674 [00:03<00:08, 194702.16it/s]Splitting dataset:  27%|██▋       | 646120/2372674 [00:03<00:08, 196883.62it/s]Splitting dataset:  28%|██▊       | 665819/2372674 [00:03<00:08, 195209.83it/s]Splitting dataset:  29%|██▉       | 685350/2372674 [00:03<00:08, 188042.59it/s]Splitting dataset:  30%|██▉       | 704211/2372674 [00:03<00:08, 187638.19it/s]Splitting dataset:  31%|███       | 724296/2372674 [00:03<00:08, 191493.96it/s]Splitting dataset:  31%|███▏      | 745214/2372674 [00:03<00:08, 196700.01it/s]Splitting dataset:  32%|███▏      | 765457/2372674 [00:03<00:08, 198394.77it/s]Splitting dataset:  33%|███▎      | 786856/2372674 [00:04<00:07, 203027.77it/s]Splitting dataset:  34%|███▍      | 807813/2372674 [00:04<00:07, 204976.44it/s]Splitting dataset:  35%|███▍      | 828329/2372674 [00:04<00:07, 204979.50it/s]Splitting dataset:  36%|███▌      | 849607/2372674 [00:04<00:07, 207309.39it/s]Splitting dataset:  37%|███▋      | 870418/2372674 [00:04<00:07, 207544.62it/s]Splitting dataset:  38%|███▊      | 891180/2372674 [00:04<00:07, 207155.59it/s]Splitting dataset:  38%|███▊      | 912110/2372674 [00:04<00:07, 207795.85it/s]Splitting dataset:  39%|███▉      | 933059/2372674 [00:04<00:06, 208297.59it/s]Splitting dataset:  40%|████      | 954537/2372674 [00:04<00:06, 210237.80it/s]Splitting dataset:  41%|████      | 975709/2372674 [00:04<00:06, 210679.11it/s]Splitting dataset:  42%|████▏     | 998194/2372674 [00:05<00:06, 214924.34it/s]Splitting dataset:  43%|████▎     | 1019688/2372674 [00:05<00:06, 213686.31it/s]Splitting dataset:  44%|████▍     | 1041060/2372674 [00:05<00:06, 206421.63it/s]Splitting dataset:  45%|████▍     | 1062742/2372674 [00:05<00:06, 209453.46it/s]Splitting dataset:  46%|████▌     | 1084237/2372674 [00:05<00:06, 211065.45it/s]Splitting dataset:  47%|████▋     | 1105379/2372674 [00:05<00:06, 210919.37it/s]Splitting dataset:  47%|████▋     | 1126496/2372674 [00:05<00:05, 209493.97it/s]Splitting dataset:  48%|████▊     | 1147844/2372674 [00:05<00:05, 210671.67it/s]Splitting dataset:  49%|████▉     | 1169424/2372674 [00:05<00:05, 212195.06it/s]Splitting dataset:  50%|█████     | 1190655/2372674 [00:05<00:05, 211373.58it/s]Splitting dataset:  51%|█████     | 1212381/2372674 [00:06<00:05, 213123.34it/s]Splitting dataset:  52%|█████▏    | 1233701/2372674 [00:06<00:05, 210790.05it/s]Splitting dataset:  53%|█████▎    | 1254790/2372674 [00:06<00:05, 205584.51it/s]Splitting dataset:  54%|█████▍    | 1276100/2372674 [00:06<00:05, 207779.24it/s]Splitting dataset:  55%|█████▍    | 1296905/2372674 [00:06<00:05, 205930.88it/s]Splitting dataset:  56%|█████▌    | 1317989/2372674 [00:06<00:05, 207372.73it/s]Splitting dataset:  56%|█████▋    | 1338743/2372674 [00:06<00:05, 206118.91it/s]Splitting dataset:  57%|█████▋    | 1359367/2372674 [00:06<00:05, 202082.30it/s]Splitting dataset:  58%|█████▊    | 1379597/2372674 [00:06<00:04, 201729.93it/s]Splitting dataset:  59%|█████▉    | 1399784/2372674 [00:07<00:04, 201485.95it/s]Splitting dataset:  60%|█████▉    | 1420348/2372674 [00:07<00:04, 202712.62it/s]Splitting dataset:  61%|██████    | 1440671/2372674 [00:07<00:04, 202863.38it/s]Splitting dataset:  62%|██████▏   | 1461092/2372674 [00:07<00:04, 203252.13it/s]Splitting dataset:  62%|██████▏   | 1481422/2372674 [00:07<00:04, 202884.78it/s]Splitting dataset:  63%|██████▎   | 1501714/2372674 [00:07<00:04, 199830.02it/s]Splitting dataset:  64%|██████▍   | 1523500/2372674 [00:07<00:04, 205161.42it/s]Splitting dataset:  65%|██████▌   | 1545377/2372674 [00:07<00:03, 209199.76it/s]Splitting dataset:  66%|██████▌   | 1567196/2372674 [00:07<00:03, 211875.96it/s]Splitting dataset:  67%|██████▋   | 1588396/2372674 [00:07<00:03, 211290.11it/s]Splitting dataset:  68%|██████▊   | 1609534/2372674 [00:08<00:03, 206611.39it/s]Splitting dataset:  69%|██████▊   | 1630394/2372674 [00:08<00:03, 207184.21it/s]Splitting dataset:  70%|██████▉   | 1651785/2372674 [00:08<00:03, 209170.72it/s]Splitting dataset:  71%|███████   | 1673556/2372674 [00:08<00:03, 211701.03it/s]Splitting dataset:  71%|███████▏  | 1694741/2372674 [00:08<00:03, 208322.50it/s]Splitting dataset:  72%|███████▏  | 1715594/2372674 [00:08<00:03, 206437.84it/s]Splitting dataset:  73%|███████▎  | 1736253/2372674 [00:08<00:03, 204875.69it/s]Splitting dataset:  74%|███████▍  | 1756752/2372674 [00:08<00:03, 204147.58it/s]Splitting dataset:  75%|███████▍  | 1777174/2372674 [00:08<00:02, 203190.54it/s]Splitting dataset:  76%|███████▌  | 1797498/2372674 [00:08<00:02, 194231.75it/s]Splitting dataset:  77%|███████▋  | 1818306/2372674 [00:09<00:02, 198213.25it/s]Splitting dataset:  77%|███████▋  | 1838199/2372674 [00:09<00:02, 194247.73it/s]Splitting dataset:  78%|███████▊  | 1857682/2372674 [00:09<00:02, 189812.73it/s]Splitting dataset:  79%|███████▉  | 1878734/2372674 [00:09<00:02, 195765.48it/s]Splitting dataset:  80%|████████  | 1898733/2372674 [00:09<00:02, 196994.24it/s]Splitting dataset:  81%|████████  | 1918479/2372674 [00:09<00:02, 194668.02it/s]Splitting dataset:  82%|████████▏ | 1938597/2372674 [00:09<00:02, 196574.53it/s]Splitting dataset:  83%|████████▎ | 1958753/2372674 [00:09<00:02, 198042.74it/s]Splitting dataset:  83%|████████▎ | 1978603/2372674 [00:09<00:01, 198174.88it/s]Splitting dataset:  84%|████████▍ | 1999053/2372674 [00:09<00:01, 200054.20it/s]Splitting dataset:  85%|████████▌ | 2019388/2372674 [00:10<00:01, 201032.77it/s]Splitting dataset:  86%|████████▌ | 2039501/2372674 [00:10<00:01, 199020.98it/s]Splitting dataset:  87%|████████▋ | 2059414/2372674 [00:10<00:01, 197269.36it/s]Splitting dataset:  88%|████████▊ | 2079437/2372674 [00:10<00:01, 198142.21it/s]Splitting dataset:  88%|████████▊ | 2099259/2372674 [00:10<00:01, 196608.82it/s]Splitting dataset:  89%|████████▉ | 2118927/2372674 [00:10<00:01, 194577.26it/s]Splitting dataset:  90%|█████████ | 2138392/2372674 [00:10<00:01, 193669.62it/s]Splitting dataset:  91%|█████████ | 2157764/2372674 [00:10<00:01, 193633.79it/s]Splitting dataset:  92%|█████████▏| 2178883/2372674 [00:10<00:00, 198843.61it/s]Splitting dataset:  93%|█████████▎| 2200500/2372674 [00:10<00:00, 203999.72it/s]Splitting dataset:  94%|█████████▎| 2222910/2372674 [00:11<00:00, 209996.28it/s]Splitting dataset:  95%|█████████▍| 2245454/2372674 [00:11<00:00, 214609.19it/s]Splitting dataset:  96%|█████████▌| 2267318/2372674 [00:11<00:00, 215811.44it/s]Splitting dataset:  96%|█████████▋| 2289263/2372674 [00:11<00:00, 216897.25it/s]Splitting dataset:  97%|█████████▋| 2310958/2372674 [00:11<00:00, 214501.43it/s]Splitting dataset:  98%|█████████▊| 2332418/2372674 [00:11<00:00, 206940.19it/s]Splitting dataset:  99%|█████████▉| 2353171/2372674 [00:11<00:00, 199844.31it/s]Splitting dataset: 100%|██████████| 2372674/2372674 [00:11<00:00, 200828.26it/s]
INFO:root:processing dataset...
  0%|          | 0/5453 [00:00<?, ?it/s]  1%|          | 51/5453 [00:00<00:10, 509.41it/s]  2%|▏         | 102/5453 [00:00<00:10, 507.43it/s]  3%|▎         | 156/5453 [00:00<00:10, 519.84it/s]  4%|▍         | 208/5453 [00:00<00:10, 486.21it/s]/data02/gtguo/DEL/pkg/utils/mol_feat.py:71: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matricesor `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484808560/work/aten/src/ATen/native/TensorShape.cpp:2981.)
  return torch.tensor(coo, dtype=torch.long).T
  5%|▍         | 270/5453 [00:00<00:09, 530.54it/s]  6%|▌         | 324/5453 [00:00<00:09, 529.31it/s]  7%|▋         | 379/5453 [00:00<00:09, 534.91it/s]  8%|▊         | 441/5453 [00:00<00:08, 559.87it/s]  9%|▉         | 498/5453 [00:00<00:09, 538.44it/s] 10%|█         | 553/5453 [00:01<00:09, 541.65it/s] 11%|█▏        | 615/5453 [00:01<00:08, 564.45it/s] 12%|█▏        | 672/5453 [00:01<00:09, 530.45it/s] 13%|█▎        | 728/5453 [00:01<00:08, 538.24it/s] 14%|█▍        | 783/5453 [00:01<00:08, 528.43it/s] 15%|█▌        | 839/5453 [00:01<00:08, 536.54it/s] 16%|█▋        | 893/5453 [00:01<00:08, 533.78it/s] 17%|█▋        | 947/5453 [00:01<00:08, 515.81it/s] 19%|█▊        | 1012/5453 [00:01<00:08, 553.30it/s] 20%|█▉        | 1068/5453 [00:02<00:08, 531.69it/s] 21%|██        | 1126/5453 [00:02<00:07, 544.93it/s] 22%|██▏       | 1183/5453 [00:02<00:07, 549.09it/s] 23%|██▎       | 1239/5453 [00:02<00:07, 547.53it/s] 24%|██▎       | 1294/5453 [00:02<00:07, 520.41it/s] 25%|██▍       | 1347/5453 [00:02<00:07, 520.70it/s] 26%|██▌       | 1408/5453 [00:02<00:07, 542.49it/s] 27%|██▋       | 1463/5453 [00:02<00:07, 540.40it/s] 28%|██▊       | 1519/5453 [00:02<00:07, 545.56it/s] 29%|██▉       | 1578/5453 [00:02<00:06, 558.59it/s] 30%|██▉       | 1634/5453 [00:03<00:06, 551.10it/s] 31%|███       | 1692/5453 [00:03<00:06, 557.89it/s] 32%|███▏      | 1748/5453 [00:03<00:06, 541.13it/s] 33%|███▎      | 1803/5453 [00:03<00:06, 524.41it/s] 34%|███▍      | 1856/5453 [00:03<00:06, 516.78it/s] 35%|███▍      | 1908/5453 [00:03<00:07, 481.80it/s] 36%|███▌      | 1957/5453 [00:03<00:07, 482.56it/s] 37%|███▋      | 2011/5453 [00:03<00:06, 497.76it/s] 38%|███▊      | 2062/5453 [00:03<00:07, 461.80it/s] 39%|███▉      | 2116/5453 [00:04<00:06, 481.08it/s] 40%|███▉      | 2168/5453 [00:04<00:06, 491.53it/s] 41%|████      | 2224/5453 [00:04<00:06, 511.03it/s] 42%|████▏     | 2276/5453 [00:04<00:06, 498.28it/s] 43%|████▎     | 2329/5453 [00:04<00:06, 506.99it/s] 44%|████▎     | 2385/5453 [00:04<00:05, 519.10it/s] 45%|████▍     | 2438/5453 [00:04<00:05, 514.00it/s] 46%|████▌     | 2490/5453 [00:04<00:05, 514.04it/s] 47%|████▋     | 2542/5453 [00:04<00:05, 499.16it/s] 48%|████▊     | 2593/5453 [00:04<00:05, 498.25it/s] 49%|████▊     | 2646/5453 [00:05<00:05, 507.17it/s] 49%|████▉     | 2697/5453 [00:05<00:05, 487.39it/s] 50%|█████     | 2746/5453 [00:05<00:05, 482.69it/s] 51%|█████▏    | 2803/5453 [00:05<00:05, 506.95it/s] 52%|█████▏    | 2856/5453 [00:05<00:05, 513.14it/s] 53%|█████▎    | 2908/5453 [00:05<00:05, 492.23it/s] 54%|█████▍    | 2958/5453 [00:05<00:05, 458.45it/s] 55%|█████▌    | 3010/5453 [00:05<00:05, 474.61it/s] 56%|█████▌    | 3060/5453 [00:05<00:04, 481.06it/s] 57%|█████▋    | 3109/5453 [00:06<00:04, 475.14it/s] 58%|█████▊    | 3157/5453 [00:06<00:04, 471.72it/s] 59%|█████▉    | 3206/5453 [00:06<00:04, 475.49it/s] 60%|█████▉    | 3256/5453 [00:06<00:04, 480.85it/s] 61%|██████    | 3305/5453 [00:06<00:04, 474.30it/s] 61%|██████▏   | 3353/5453 [00:06<00:04, 472.39it/s] 62%|██████▏   | 3401/5453 [00:06<00:04, 473.96it/s] 63%|██████▎   | 3451/5453 [00:06<00:04, 480.23it/s] 64%|██████▍   | 3504/5453 [00:06<00:03, 493.47it/s] 65%|██████▌   | 3556/5453 [00:06<00:03, 501.28it/s] 66%|██████▌   | 3607/5453 [00:07<00:03, 500.28it/s] 67%|██████▋   | 3658/5453 [00:07<00:03, 501.40it/s] 68%|██████▊   | 3709/5453 [00:07<00:03, 499.48it/s] 69%|██████▉   | 3759/5453 [00:07<00:04, 341.80it/s] 70%|██████▉   | 3811/5453 [00:07<00:04, 380.35it/s] 71%|███████   | 3861/5453 [00:07<00:03, 408.46it/s] 72%|███████▏  | 3912/5453 [00:07<00:03, 433.81it/s] 73%|███████▎  | 3960/5453 [00:07<00:03, 444.33it/s] 74%|███████▎  | 4013/5453 [00:08<00:03, 466.11it/s] 75%|███████▍  | 4063/5453 [00:08<00:02, 474.07it/s] 75%|███████▌  | 4113/5453 [00:08<00:02, 479.05it/s] 76%|███████▋  | 4163/5453 [00:08<00:02, 482.59it/s] 77%|███████▋  | 4213/5453 [00:08<00:02, 482.56it/s] 78%|███████▊  | 4262/5453 [00:08<00:02, 483.41it/s] 79%|███████▉  | 4312/5453 [00:08<00:02, 486.19it/s] 80%|███████▉  | 4361/5453 [00:08<00:02, 480.27it/s] 81%|████████  | 4410/5453 [00:08<00:02, 481.82it/s] 82%|████████▏ | 4459/5453 [00:08<00:02, 473.92it/s] 83%|████████▎ | 4507/5453 [00:09<00:01, 473.94it/s] 84%|████████▎ | 4555/5453 [00:09<00:01, 467.67it/s] 84%|████████▍ | 4602/5453 [00:09<00:01, 464.02it/s] 85%|████████▌ | 4649/5453 [00:09<00:01, 464.87it/s] 86%|████████▌ | 4696/5453 [00:09<00:01, 458.82it/s] 87%|████████▋ | 4743/5453 [00:09<00:01, 461.26it/s] 88%|████████▊ | 4791/5453 [00:09<00:01, 466.11it/s] 89%|████████▊ | 4839/5453 [00:09<00:01, 468.03it/s] 90%|████████▉ | 4886/5453 [00:09<00:01, 465.50it/s] 90%|█████████ | 4933/5453 [00:09<00:01, 466.58it/s] 91%|█████████▏| 4980/5453 [00:10<00:01, 462.80it/s] 92%|█████████▏| 5027/5453 [00:10<00:00, 455.29it/s] 93%|█████████▎| 5073/5453 [00:10<00:00, 454.59it/s] 94%|█████████▍| 5125/5453 [00:10<00:00, 470.95it/s] 95%|█████████▍| 5173/5453 [00:10<00:00, 473.10it/s] 96%|█████████▌| 5222/5453 [00:10<00:00, 475.55it/s] 97%|█████████▋| 5270/5453 [00:10<00:00, 468.99it/s] 98%|█████████▊| 5318/5453 [00:10<00:00, 471.03it/s] 98%|█████████▊| 5366/5453 [00:10<00:00, 469.86it/s] 99%|█████████▉| 5414/5453 [00:10<00:00, 469.83it/s]100%|██████████| 5453/5453 [00:11<00:00, 490.60it/s]
INFO:__main__:Dataset has 5453 samples
INFO:__main__:Dataset data example: {'pyg_data': Data(x=[22, 19], edge_index=[2, 50], edge_attr=[50, 2]), 'activity': 5.028260409112222}
INFO:__main__:Train loader has 2 data
INFO:__main__:Model loaded from /data02/gtguo/DEL/data/weights/refnet/ca9_large/model.pt
3408 2045
2726 682
/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([2048])) that is different to the input size (torch.Size([2048, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([678])) that is different to the input size (torch.Size([678, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([682])) that is different to the input size (torch.Size([682, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:__main__:Epoch 0, Train Loss 52.702510833740234
INFO:__main__:Epoch 0, Valid Loss 53.105743408203125
INFO:__main__:Saving model at epoch 0
INFO:__main__:Epoch 1, Train Loss 52.61284255981445
INFO:__main__:Epoch 1, Valid Loss 52.4749870300293
INFO:__main__:Epoch 2, Train Loss 51.627105712890625
INFO:__main__:Epoch 2, Valid Loss 51.906803131103516
INFO:__main__:Epoch 3, Train Loss 50.85492706298828
INFO:__main__:Epoch 3, Valid Loss 51.38379669189453
INFO:__main__:Epoch 4, Train Loss 51.01506042480469
INFO:__main__:Epoch 4, Valid Loss 50.94043731689453
INFO:__main__:Epoch 5, Train Loss 50.35273742675781
INFO:__main__:Epoch 5, Valid Loss 50.582275390625
INFO:__main__:Saving model at epoch 5
INFO:__main__:Epoch 6, Train Loss 50.47136688232422
INFO:__main__:Epoch 6, Valid Loss 50.25898361206055
INFO:__main__:Epoch 7, Train Loss 49.59912872314453
INFO:__main__:Epoch 7, Valid Loss 49.930908203125
INFO:__main__:Epoch 8, Train Loss 48.735504150390625
INFO:__main__:Epoch 8, Valid Loss 49.62932205200195
INFO:__main__:Epoch 9, Train Loss 48.83196258544922
INFO:__main__:Epoch 9, Valid Loss 49.30720901489258
INFO:__main__:Epoch 10, Train Loss 48.05553436279297
INFO:__main__:Epoch 10, Valid Loss 48.94982147216797
INFO:__main__:Saving model at epoch 10
INFO:__main__:Epoch 11, Train Loss 48.0068244934082
INFO:__main__:Epoch 11, Valid Loss 48.550472259521484
INFO:__main__:Epoch 12, Train Loss 47.75196075439453
INFO:__main__:Epoch 12, Valid Loss 48.11312484741211
INFO:__main__:Epoch 13, Train Loss 48.04844665527344
INFO:__main__:Epoch 13, Valid Loss 47.64936828613281
INFO:__main__:Epoch 14, Train Loss 46.6760139465332
INFO:__main__:Epoch 14, Valid Loss 47.160343170166016
INFO:__main__:Epoch 15, Train Loss 46.31805419921875
INFO:__main__:Epoch 15, Valid Loss 46.638668060302734
INFO:__main__:Saving model at epoch 15
/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([2048])) that is different to the input size (torch.Size([2047, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:__main__:Epoch 16, Train Loss 46.12553024291992
INFO:__main__:Epoch 16, Valid Loss 46.07089614868164
INFO:__main__:Epoch 17, Train Loss 44.93805694580078
INFO:__main__:Epoch 17, Valid Loss 45.45600891113281
INFO:__main__:Epoch 18, Train Loss 44.783897399902344
INFO:__main__:Epoch 18, Valid Loss 44.81779479980469
INFO:__main__:Epoch 19, Train Loss 43.881927490234375
INFO:__main__:Epoch 19, Valid Loss 44.13747787475586
INFO:__main__:Epoch 20, Train Loss 44.18025207519531
INFO:__main__:Epoch 20, Valid Loss 43.39249801635742
INFO:__main__:Saving model at epoch 20
INFO:__main__:Epoch 21, Train Loss 42.35432052612305
INFO:__main__:Epoch 21, Valid Loss 42.58578872680664
INFO:__main__:Epoch 22, Train Loss 42.192996978759766
INFO:__main__:Epoch 22, Valid Loss 41.73133850097656
INFO:__main__:Epoch 23, Train Loss 40.47334671020508
INFO:__main__:Epoch 23, Valid Loss 40.79656219482422
INFO:__main__:Epoch 24, Train Loss 39.3155632019043
INFO:__main__:Epoch 24, Valid Loss 39.762245178222656
INFO:__main__:Epoch 25, Train Loss 39.14796447753906
INFO:__main__:Epoch 25, Valid Loss 38.64086151123047
INFO:__main__:Saving model at epoch 25
INFO:__main__:Epoch 26, Train Loss 37.34728240966797
INFO:__main__:Epoch 26, Valid Loss 37.43916702270508
INFO:__main__:Epoch 27, Train Loss 36.76280975341797
INFO:__main__:Epoch 27, Valid Loss 36.15858840942383
INFO:__main__:Epoch 28, Train Loss 34.65123748779297
INFO:__main__:Epoch 28, Valid Loss 34.79743576049805
INFO:__main__:Epoch 29, Train Loss 34.03900146484375
INFO:__main__:Epoch 29, Valid Loss 33.347450256347656
INFO:__main__:Epoch 30, Train Loss 32.98700714111328
INFO:__main__:Epoch 30, Valid Loss 31.805517196655273
INFO:__main__:Saving model at epoch 30
INFO:__main__:Epoch 31, Train Loss 30.922937393188477
INFO:__main__:Epoch 31, Valid Loss 30.172317504882812
INFO:__main__:Epoch 32, Train Loss 29.005720138549805
INFO:__main__:Epoch 32, Valid Loss 28.453405380249023
INFO:__main__:Epoch 33, Train Loss 26.503353118896484
INFO:__main__:Epoch 33, Valid Loss 26.655038833618164
INFO:__main__:Epoch 34, Train Loss 24.934444427490234
INFO:__main__:Epoch 34, Valid Loss 24.77781867980957
INFO:__main__:Epoch 35, Train Loss 23.809768676757812
INFO:__main__:Epoch 35, Valid Loss 22.83917808532715
INFO:__main__:Saving model at epoch 35
INFO:__main__:Epoch 36, Train Loss 21.485301971435547
INFO:__main__:Epoch 36, Valid Loss 20.84718132019043
INFO:__main__:Epoch 37, Train Loss 19.55100440979004
INFO:__main__:Epoch 37, Valid Loss 18.818647384643555
INFO:__main__:Epoch 38, Train Loss 17.844018936157227
INFO:__main__:Epoch 38, Valid Loss 16.781490325927734
INFO:__main__:Epoch 39, Train Loss 15.55868148803711
INFO:__main__:Epoch 39, Valid Loss 14.75631046295166
INFO:__main__:Epoch 40, Train Loss 13.110503196716309
INFO:__main__:Epoch 40, Valid Loss 12.76398754119873
INFO:__main__:Saving model at epoch 40
INFO:__main__:Epoch 41, Train Loss 11.435890197753906
INFO:__main__:Epoch 41, Valid Loss 10.837331771850586
INFO:__main__:Epoch 42, Train Loss 9.920736312866211
INFO:__main__:Epoch 42, Valid Loss 9.009020805358887
INFO:__main__:Epoch 43, Train Loss 7.925150394439697
INFO:__main__:Epoch 43, Valid Loss 7.313906669616699
INFO:__main__:Epoch 44, Train Loss 6.480047225952148
INFO:__main__:Epoch 44, Valid Loss 5.787683486938477
INFO:__main__:Epoch 45, Train Loss 4.955413341522217
INFO:__main__:Epoch 45, Valid Loss 4.464505195617676
INFO:__main__:Saving model at epoch 45
INFO:__main__:Epoch 46, Train Loss 3.941232204437256
INFO:__main__:Epoch 46, Valid Loss 3.370656728744507
INFO:__main__:Epoch 47, Train Loss 3.0517935752868652
INFO:__main__:Epoch 47, Valid Loss 2.5215656757354736
INFO:__main__:Epoch 48, Train Loss 2.3904290199279785
INFO:__main__:Epoch 48, Valid Loss 1.9188830852508545
INFO:__main__:Epoch 49, Train Loss 1.8400324583053589
INFO:__main__:Epoch 49, Valid Loss 1.546739101409912
INFO:__main__:Epoch 50, Train Loss 1.7672683000564575
INFO:__main__:Epoch 50, Valid Loss 1.3719712495803833
INFO:__main__:Saving model at epoch 50
INFO:__main__:Epoch 51, Train Loss 1.8027498722076416
INFO:__main__:Epoch 51, Valid Loss 1.3456227779388428
INFO:__main__:Epoch 52, Train Loss 1.7427983283996582
INFO:__main__:Epoch 52, Valid Loss 1.4092657566070557
INFO:__main__:Epoch 53, Train Loss 1.465123176574707
INFO:__main__:Epoch 53, Valid Loss 1.5059610605239868
INFO:__main__:Epoch 54, Train Loss 2.021651029586792
INFO:__main__:Epoch 54, Valid Loss 1.5903327465057373
INFO:__main__:Epoch 55, Train Loss 1.4453890323638916
INFO:__main__:Epoch 55, Valid Loss 1.635633945465088
INFO:__main__:Epoch 56, Train Loss 1.67483389377594
INFO:__main__:Epoch 56, Valid Loss 1.6359546184539795
INFO:__main__:Epoch 57, Train Loss 2.307478904724121
INFO:__main__:Epoch 57, Valid Loss 1.5960662364959717
INFO:__main__:Epoch 58, Train Loss 1.6953401565551758
INFO:__main__:Epoch 58, Valid Loss 1.529708981513977
INFO:__main__:Epoch 59, Train Loss 1.6155431270599365
INFO:__main__:Epoch 59, Valid Loss 1.455020546913147
INFO:__main__:Epoch 60, Train Loss 1.8966162204742432
INFO:__main__:Epoch 60, Valid Loss 1.3874071836471558
INFO:__main__:Epoch 61, Train Loss 1.7565644979476929
INFO:__main__:Epoch 61, Valid Loss 1.3450863361358643
INFO:__main__:Epoch 62, Train Loss 1.4902753829956055
INFO:__main__:Epoch 62, Valid Loss 1.3529701232910156
INFO:__main__:Epoch 63, Train Loss 1.6192536354064941
INFO:__main__:Epoch 63, Valid Loss 1.3969483375549316
INFO:__main__:Epoch 64, Train Loss 1.5808919668197632
INFO:__main__:Epoch 64, Valid Loss 1.4082778692245483
INFO:__main__:Epoch 65, Train Loss 1.6409796476364136
INFO:__main__:Epoch 65, Valid Loss 1.3865853548049927
INFO:__main__:Epoch 66, Train Loss 1.2736928462982178
INFO:__main__:Epoch 66, Valid Loss 1.36446213722229
INFO:__main__:Epoch 67, Train Loss 1.519801378250122
INFO:__main__:Epoch 67, Valid Loss 1.3514474630355835
/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch_geometric/data/collate.py:204: UserWarning: An output with one or more elements was resized since it had shape [17293], which does not match the required output shape [17293, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484808560/work/aten/src/ATen/native/Resize.cpp:17.)
  value = torch.cat(values, dim=cat_dim or 0, out=out)
/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch_geometric/data/collate.py:204: UserWarning: An output with one or more elements was resized since it had shape [36924], which does not match the required output shape [2, 36924]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484808560/work/aten/src/ATen/native/Resize.cpp:17.)
  value = torch.cat(values, dim=cat_dim or 0, out=out)
/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch_geometric/data/collate.py:204: UserWarning: An output with one or more elements was resized since it had shape [36924], which does not match the required output shape [36924, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484808560/work/aten/src/ATen/native/Resize.cpp:17.)
  value = torch.cat(values, dim=cat_dim or 0, out=out)
INFO:__main__:Epoch 68, Train Loss 1.6635717153549194
INFO:__main__:Epoch 68, Valid Loss 1.3454129695892334
INFO:__main__:Epoch 69, Train Loss 1.5563733577728271
INFO:__main__:Epoch 69, Valid Loss 1.34293532371521
INFO:__main__:Epoch 70, Train Loss 1.4173299074172974
INFO:__main__:Epoch 70, Valid Loss 1.3419941663742065
INFO:__main__:Saving model at epoch 70
INFO:__main__:Epoch 71, Train Loss 1.8326891660690308
INFO:__main__:Epoch 71, Valid Loss 1.3416075706481934
INFO:__main__:Epoch 72, Train Loss 1.5001258850097656
INFO:__main__:Epoch 72, Valid Loss 1.3415147066116333
INFO:__main__:Epoch 73, Train Loss 1.166250467300415
INFO:__main__:Epoch 73, Valid Loss 1.3415546417236328
INFO:__main__:Epoch 74, Train Loss 1.3634363412857056
INFO:__main__:Epoch 74, Valid Loss 1.3416303396224976
INFO:__main__:Epoch 75, Train Loss 1.667065143585205
INFO:__main__:Epoch 75, Valid Loss 1.3417774438858032
INFO:__main__:Epoch 76, Train Loss 1.7443937063217163
INFO:__main__:Epoch 76, Valid Loss 1.3420112133026123
INFO:__main__:Epoch 77, Train Loss 1.4875435829162598
INFO:__main__:Epoch 77, Valid Loss 1.3424280881881714
INFO:__main__:Epoch 78, Train Loss 1.5464802980422974
INFO:__main__:Epoch 78, Valid Loss 1.343031883239746
INFO:__main__:Epoch 79, Train Loss 1.6000540256500244
INFO:__main__:Epoch 79, Valid Loss 1.343672752380371
INFO:__main__:Epoch 80, Train Loss 1.7269185781478882
INFO:__main__:Epoch 80, Valid Loss 1.3443236351013184
INFO:__main__:Epoch 81, Train Loss 1.5822393894195557
INFO:__main__:Epoch 81, Valid Loss 1.344939947128296
INFO:__main__:Epoch 82, Train Loss 1.4274852275848389
INFO:__main__:Epoch 82, Valid Loss 1.3454152345657349
INFO:__main__:Epoch 83, Train Loss 1.5687593221664429
INFO:__main__:Epoch 83, Valid Loss 1.345489263534546
INFO:__main__:Epoch 84, Train Loss 1.3885706663131714
INFO:__main__:Epoch 84, Valid Loss 1.3452519178390503
INFO:__main__:Epoch 85, Train Loss 1.4206568002700806
INFO:__main__:Epoch 85, Valid Loss 1.344831943511963
INFO:__main__:Epoch 86, Train Loss 1.557758092880249
INFO:__main__:Epoch 86, Valid Loss 1.3443340063095093
INFO:__main__:Epoch 87, Train Loss 1.651464581489563
INFO:__main__:Epoch 87, Valid Loss 1.3439158201217651
INFO:__main__:Epoch 88, Train Loss 1.645203709602356
INFO:__main__:Epoch 88, Valid Loss 1.343687653541565
INFO:__main__:Epoch 89, Train Loss 1.4424922466278076
INFO:__main__:Epoch 89, Valid Loss 1.3435345888137817
INFO:__main__:Epoch 90, Train Loss 1.607025146484375
INFO:__main__:Epoch 90, Valid Loss 1.3433541059494019
INFO:__main__:Epoch 91, Train Loss 1.3575793504714966
INFO:__main__:Epoch 91, Valid Loss 1.3432841300964355
INFO:__main__:Epoch 92, Train Loss 1.6337454319000244
INFO:__main__:Epoch 92, Valid Loss 1.3432302474975586
INFO:__main__:Epoch 93, Train Loss 1.415781021118164
INFO:__main__:Epoch 93, Valid Loss 1.3431313037872314
INFO:__main__:Epoch 94, Train Loss 1.6657453775405884
INFO:__main__:Epoch 94, Valid Loss 1.3430603742599487
INFO:__main__:Epoch 95, Train Loss 1.7049468755722046
INFO:__main__:Epoch 95, Valid Loss 1.3430794477462769
INFO:__main__:Epoch 96, Train Loss 1.2742332220077515
INFO:__main__:Epoch 96, Valid Loss 1.3431856632232666
INFO:__main__:Epoch 97, Train Loss 1.5223246812820435
INFO:__main__:Epoch 97, Valid Loss 1.343198537826538
INFO:__main__:Epoch 98, Train Loss 1.6499103307724
INFO:__main__:Epoch 98, Valid Loss 1.3432810306549072
INFO:__main__:Epoch 99, Train Loss 1.4777191877365112
INFO:__main__:Epoch 99, Valid Loss 1.3434444665908813
INFO:__main__:Epoch 100, Train Loss 1.4037561416625977
INFO:__main__:Epoch 100, Valid Loss 1.3435708284378052
INFO:__main__:Epoch 101, Train Loss 1.5941548347473145
INFO:__main__:Epoch 101, Valid Loss 1.343685507774353
INFO:__main__:Epoch 102, Train Loss 1.6777437925338745
INFO:__main__:Epoch 102, Valid Loss 1.3438103199005127
INFO:__main__:Epoch 103, Train Loss 1.586881399154663
INFO:__main__:Epoch 103, Valid Loss 1.3438373804092407
INFO:__main__:Epoch 104, Train Loss 1.6197229623794556
INFO:__main__:Epoch 104, Valid Loss 1.3438053131103516
INFO:__main__:Epoch 105, Train Loss 1.2948142290115356
INFO:__main__:Epoch 105, Valid Loss 1.3437572717666626
INFO:__main__:Epoch 106, Train Loss 1.607465147972107
INFO:__main__:Epoch 106, Valid Loss 1.3436260223388672
INFO:__main__:Epoch 107, Train Loss 1.6663191318511963
INFO:__main__:Epoch 107, Valid Loss 1.3435612916946411
INFO:__main__:Epoch 108, Train Loss 1.5183875560760498
INFO:__main__:Epoch 108, Valid Loss 1.3435412645339966
INFO:__main__:Epoch 109, Train Loss 1.583314299583435
INFO:__main__:Epoch 109, Valid Loss 1.343518853187561
INFO:__main__:Epoch 110, Train Loss 1.6174347400665283
INFO:__main__:Epoch 110, Valid Loss 1.3434851169586182
INFO:__main__:Epoch 111, Train Loss 1.421473741531372
INFO:__main__:Epoch 111, Valid Loss 1.3434466123580933
INFO:__main__:Epoch 112, Train Loss 1.5530916452407837
INFO:__main__:Epoch 112, Valid Loss 1.3434120416641235
INFO:__main__:Epoch 113, Train Loss 1.5574629306793213
INFO:__main__:Epoch 113, Valid Loss 1.343395471572876
INFO:__main__:Epoch 114, Train Loss 1.312049388885498
INFO:__main__:Epoch 114, Valid Loss 1.3433740139007568
INFO:__main__:Epoch 115, Train Loss 1.5224981307983398
INFO:__main__:Epoch 115, Valid Loss 1.3432244062423706
INFO:__main__:Epoch 116, Train Loss 1.4382206201553345
INFO:__main__:Epoch 116, Valid Loss 1.3431627750396729
INFO:__main__:Epoch 117, Train Loss 1.579421877861023
INFO:__main__:Epoch 117, Valid Loss 1.3432533740997314
INFO:__main__:Epoch 118, Train Loss 1.658219814300537
INFO:__main__:Epoch 118, Valid Loss 1.3434284925460815
INFO:__main__:Epoch 119, Train Loss 1.6962437629699707
INFO:__main__:Epoch 119, Valid Loss 1.343520164489746
INFO:__main__:Epoch 120, Train Loss 1.6594185829162598
INFO:__main__:Epoch 120, Valid Loss 1.3434804677963257
INFO:__main__:Epoch 121, Train Loss 1.3799419403076172
INFO:__main__:Epoch 121, Valid Loss 1.3433847427368164
INFO:__main__:Epoch 122, Train Loss 1.2319165468215942
INFO:__main__:Epoch 122, Valid Loss 1.3431482315063477
INFO:__main__:Epoch 123, Train Loss 1.6060614585876465
INFO:__main__:Epoch 123, Valid Loss 1.3427284955978394
INFO:__main__:Epoch 124, Train Loss 1.5622345209121704
INFO:__main__:Epoch 124, Valid Loss 1.3422011137008667
INFO:__main__:Epoch 125, Train Loss 1.4619148969650269
INFO:__main__:Epoch 125, Valid Loss 1.3417489528656006
INFO:__main__:Epoch 126, Train Loss 1.6745857000350952
INFO:__main__:Epoch 126, Valid Loss 1.3414409160614014
INFO:__main__:Epoch 127, Train Loss 1.4546664953231812
INFO:__main__:Epoch 127, Valid Loss 1.3412894010543823
INFO:__main__:Epoch 128, Train Loss 1.3790185451507568
INFO:__main__:Epoch 128, Valid Loss 1.3413487672805786
INFO:__main__:Epoch 129, Train Loss 1.6489651203155518
INFO:__main__:Epoch 129, Valid Loss 1.3415567874908447
INFO:__main__:Epoch 130, Train Loss 1.3571109771728516
INFO:__main__:Epoch 130, Valid Loss 1.3418241739273071
INFO:__main__:Epoch 131, Train Loss 1.4835790395736694
INFO:__main__:Epoch 131, Valid Loss 1.342121958732605
INFO:__main__:Epoch 132, Train Loss 1.4222720861434937
INFO:__main__:Epoch 132, Valid Loss 1.3424961566925049
INFO:__main__:Epoch 133, Train Loss 1.5532493591308594
INFO:__main__:Epoch 133, Valid Loss 1.3428603410720825
INFO:__main__:Epoch 134, Train Loss 1.3783223628997803
INFO:__main__:Epoch 134, Valid Loss 1.3432307243347168
INFO:__main__:Epoch 135, Train Loss 1.4816267490386963
INFO:__main__:Epoch 135, Valid Loss 1.3434867858886719
INFO:__main__:Epoch 136, Train Loss 1.4521348476409912
INFO:__main__:Epoch 136, Valid Loss 1.3436907529830933
INFO:__main__:Epoch 137, Train Loss 1.435760498046875
INFO:__main__:Epoch 137, Valid Loss 1.3436638116836548
INFO:__main__:Epoch 138, Train Loss 1.4599727392196655
INFO:__main__:Epoch 138, Valid Loss 1.34344482421875
INFO:__main__:Epoch 139, Train Loss 1.5373109579086304
INFO:__main__:Epoch 139, Valid Loss 1.343156099319458
INFO:__main__:Epoch 140, Train Loss 1.7024593353271484
INFO:__main__:Epoch 140, Valid Loss 1.3428806066513062
INFO:__main__:Epoch 141, Train Loss 1.6643977165222168
INFO:__main__:Epoch 141, Valid Loss 1.3426164388656616
INFO:__main__:Epoch 142, Train Loss 1.559337854385376
INFO:__main__:Epoch 142, Valid Loss 1.3425211906433105
INFO:__main__:Epoch 143, Train Loss 1.503328561782837
INFO:__main__:Epoch 143, Valid Loss 1.3425190448760986
INFO:__main__:Epoch 144, Train Loss 1.3661431074142456
INFO:__main__:Epoch 144, Valid Loss 1.342567801475525
INFO:__main__:Epoch 145, Train Loss 1.325942873954773
INFO:__main__:Epoch 145, Valid Loss 1.3424700498580933
INFO:__main__:Epoch 146, Train Loss 1.346205234527588
INFO:__main__:Epoch 146, Valid Loss 1.3422307968139648
INFO:__main__:Epoch 147, Train Loss 1.252407431602478
INFO:__main__:Epoch 147, Valid Loss 1.3419708013534546
INFO:__main__:Epoch 148, Train Loss 1.6850215196609497
INFO:__main__:Epoch 148, Valid Loss 1.341679334640503
INFO:__main__:Epoch 149, Train Loss 1.7435204982757568
INFO:__main__:Epoch 149, Valid Loss 1.3416446447372437
INFO:__main__:Epoch 150, Train Loss 1.5648584365844727
INFO:__main__:Epoch 150, Valid Loss 1.341640591621399
INFO:__main__:Epoch 151, Train Loss 1.3425790071487427
INFO:__main__:Epoch 151, Valid Loss 1.3415197134017944
INFO:__main__:Epoch 152, Train Loss 1.4768632650375366
INFO:__main__:Epoch 152, Valid Loss 1.3412024974822998
INFO:__main__:Epoch 153, Train Loss 1.671435832977295
INFO:__main__:Epoch 153, Valid Loss 1.3408870697021484
INFO:__main__:Epoch 154, Train Loss 1.3647565841674805
INFO:__main__:Epoch 154, Valid Loss 1.3408889770507812
INFO:__main__:Epoch 155, Train Loss 1.4291852712631226
INFO:__main__:Epoch 155, Valid Loss 1.3410471677780151
INFO:__main__:Epoch 156, Train Loss 1.4458465576171875
INFO:__main__:Epoch 156, Valid Loss 1.341275930404663
INFO:__main__:Epoch 157, Train Loss 1.3484140634536743
INFO:__main__:Epoch 157, Valid Loss 1.3416237831115723
INFO:__main__:Epoch 158, Train Loss 1.3075847625732422
INFO:__main__:Epoch 158, Valid Loss 1.3420615196228027
INFO:__main__:Epoch 159, Train Loss 1.7830702066421509
INFO:__main__:Epoch 159, Valid Loss 1.3424046039581299
INFO:__main__:Epoch 160, Train Loss 1.5299712419509888
INFO:__main__:Epoch 160, Valid Loss 1.3427777290344238
INFO:__main__:Epoch 161, Train Loss 1.4243361949920654
INFO:__main__:Epoch 161, Valid Loss 1.3429757356643677
INFO:__main__:Epoch 162, Train Loss 1.4861328601837158
INFO:__main__:Epoch 162, Valid Loss 1.3429864645004272
INFO:__main__:Epoch 163, Train Loss 1.3251051902770996
INFO:__main__:Epoch 163, Valid Loss 1.3429120779037476
INFO:__main__:Epoch 164, Train Loss 1.5804189443588257
INFO:__main__:Epoch 164, Valid Loss 1.342673897743225
INFO:__main__:Epoch 165, Train Loss 1.3799668550491333
INFO:__main__:Epoch 165, Valid Loss 1.3423833847045898
INFO:__main__:Epoch 166, Train Loss 1.4462347030639648
INFO:__main__:Epoch 166, Valid Loss 1.3420345783233643
INFO:__main__:Epoch 167, Train Loss 1.38170325756073
INFO:__main__:Epoch 167, Valid Loss 1.3416495323181152
INFO:__main__:Epoch 168, Train Loss 1.3874961137771606
INFO:__main__:Epoch 168, Valid Loss 1.3411896228790283
INFO:__main__:Epoch 169, Train Loss 1.2988022565841675
INFO:__main__:Epoch 169, Valid Loss 1.3408242464065552
INFO:__main__:Epoch 170, Train Loss 1.4213073253631592
INFO:__main__:Epoch 170, Valid Loss 1.340559959411621
INFO:__main__:Saving model at epoch 170
INFO:__main__:Epoch 171, Train Loss 1.552841067314148
INFO:__main__:Epoch 171, Valid Loss 1.3404161930084229
INFO:__main__:Epoch 172, Train Loss 1.351927638053894
INFO:__main__:Epoch 172, Valid Loss 1.3403266668319702
INFO:__main__:Epoch 173, Train Loss 1.3684958219528198
INFO:__main__:Epoch 173, Valid Loss 1.3402736186981201
INFO:__main__:Epoch 174, Train Loss 1.521257996559143
INFO:__main__:Epoch 174, Valid Loss 1.3403170108795166
INFO:__main__:Epoch 175, Train Loss 1.3055084943771362
INFO:__main__:Epoch 175, Valid Loss 1.3404598236083984
INFO:__main__:Epoch 176, Train Loss 1.521448016166687
INFO:__main__:Epoch 176, Valid Loss 1.3405512571334839
INFO:__main__:Epoch 177, Train Loss 1.4098109006881714
INFO:__main__:Epoch 177, Valid Loss 1.3406938314437866
INFO:__main__:Epoch 178, Train Loss 1.3656326532363892
INFO:__main__:Epoch 178, Valid Loss 1.3409558534622192
INFO:__main__:Epoch 179, Train Loss 1.3430769443511963
INFO:__main__:Epoch 179, Valid Loss 1.3413044214248657
INFO:__main__:Epoch 180, Train Loss 1.4064408540725708
INFO:__main__:Epoch 180, Valid Loss 1.3415954113006592
INFO:__main__:Epoch 181, Train Loss 1.5469554662704468
INFO:__main__:Epoch 181, Valid Loss 1.3419255018234253
INFO:__main__:Epoch 182, Train Loss 1.3455898761749268
INFO:__main__:Epoch 182, Valid Loss 1.3421846628189087
INFO:__main__:Epoch 183, Train Loss 1.4557349681854248
INFO:__main__:Epoch 183, Valid Loss 1.3421812057495117
INFO:__main__:Epoch 184, Train Loss 1.9162828922271729
INFO:__main__:Epoch 184, Valid Loss 1.3420829772949219
INFO:__main__:Epoch 185, Train Loss 1.427681565284729
INFO:__main__:Epoch 185, Valid Loss 1.3419638872146606
INFO:__main__:Epoch 186, Train Loss 1.4980562925338745
INFO:__main__:Epoch 186, Valid Loss 1.3417491912841797
INFO:__main__:Epoch 187, Train Loss 1.3299895524978638
INFO:__main__:Epoch 187, Valid Loss 1.341680884361267
INFO:__main__:Epoch 188, Train Loss 1.4161934852600098
INFO:__main__:Epoch 188, Valid Loss 1.3415278196334839
INFO:__main__:Epoch 189, Train Loss 1.7330302000045776
INFO:__main__:Epoch 189, Valid Loss 1.3412071466445923
/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch_geometric/data/collate.py:204: UserWarning: An output with one or more elements was resized since it had shape [52169], which does not match the required output shape [52169, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484808560/work/aten/src/ATen/native/Resize.cpp:17.)
  value = torch.cat(values, dim=cat_dim or 0, out=out)
/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch_geometric/data/collate.py:204: UserWarning: An output with one or more elements was resized since it had shape [111376], which does not match the required output shape [2, 111376]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484808560/work/aten/src/ATen/native/Resize.cpp:17.)
  value = torch.cat(values, dim=cat_dim or 0, out=out)
/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch_geometric/data/collate.py:204: UserWarning: An output with one or more elements was resized since it had shape [111376], which does not match the required output shape [111376, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484808560/work/aten/src/ATen/native/Resize.cpp:17.)
  value = torch.cat(values, dim=cat_dim or 0, out=out)
INFO:__main__:Epoch 190, Train Loss 1.5594027042388916
INFO:__main__:Epoch 190, Valid Loss 1.340917706489563
INFO:__main__:Epoch 191, Train Loss 1.569260597229004
INFO:__main__:Epoch 191, Valid Loss 1.3407553434371948
INFO:__main__:Epoch 192, Train Loss 1.4970368146896362
INFO:__main__:Epoch 192, Valid Loss 1.3406572341918945
INFO:__main__:Epoch 193, Train Loss 1.678788185119629
INFO:__main__:Epoch 193, Valid Loss 1.3407220840454102
INFO:__main__:Epoch 194, Train Loss 1.5271152257919312
INFO:__main__:Epoch 194, Valid Loss 1.3410305976867676
INFO:__main__:Epoch 195, Train Loss 1.541116714477539
INFO:__main__:Epoch 195, Valid Loss 1.3411171436309814
INFO:__main__:Epoch 196, Train Loss 1.7612512111663818
INFO:__main__:Epoch 196, Valid Loss 1.3412009477615356
INFO:__main__:Epoch 197, Train Loss 1.3837575912475586
INFO:__main__:Epoch 197, Valid Loss 1.3414031267166138
INFO:__main__:Epoch 198, Train Loss 1.5172491073608398
INFO:__main__:Epoch 198, Valid Loss 1.3415262699127197
INFO:__main__:Epoch 199, Train Loss 1.8129456043243408
INFO:__main__:Epoch 199, Valid Loss 1.3415435552597046
INFO:__main__:R2: 0.2142086889197259
INFO:__main__:Loss: 1.1110323667526245
INFO:__main__:R2: 0.06719175346964035
INFO:__main__:Loss: 1.3188999891281128
nohup: ignoring input
INFO:__main__:name: ca9_50_init
INFO:__main__:base_fname: ca9_large
INFO:__main__:seed: 4
INFO:__main__:device: cuda:2
INFO:__main__:batch_size: 2048
INFO:__main__:epochs: 200
INFO:__main__:lr: 0.0003
INFO:__main__:log_interval: 1
INFO:__main__:save_interval: 5
INFO:__main__:save_path: /data02/gtguo/DEL/data/weights/refnet_ca9_transfer/
INFO:__main__:load_path: /data02/gtguo/DEL/data/weights/refnet/
INFO:__main__:target_name: ca9
INFO:__main__:fp_size: 2048
INFO:__main__:transfer_learning: False
INFO:__main__:transfer_learning_ratio: 0.625
INFO:__main__:num_workers: 8
INFO:__main__:enc_node_feat_dim: 19
INFO:__main__:enc_edge_feat_dim: 2
INFO:__main__:enc_node_embedding_size: 64
INFO:__main__:enc_edge_embedding_size: 64
INFO:__main__:enc_n_layers: 5
INFO:__main__:enc_gat_n_heads: 4
INFO:__main__:enc_gat_ffn_ratio: 4
INFO:__main__:enc_fp_embedding_size: 32
INFO:__main__:enc_fp_ffn_size: 128
INFO:__main__:enc_fp_gated: False
INFO:__main__:enc_fp_n_heads: 4
INFO:__main__:enc_fp_size: 256
INFO:__main__:enc_fp_to_gat_feedback: add
INFO:__main__:enc_gat_to_fp_pooling: mean
INFO:__main__:dec_node_input_size: 64
INFO:__main__:dec_node_emb_size: 64
INFO:__main__:dec_fp_input_size: 32
INFO:__main__:dec_fp_emb_size: 64
INFO:__main__:dec_output_size: 2
INFO:__main__:reg_input_size: 64
INFO:__main__:reg_hidden_size: 64
INFO:__main__:reg_output_size: 1
INFO:__main__:record_path: /data02/gtguo/DEL/data/records/refnet_ca9_transfer/
INFO:__main__:Model has 408772 parameters
INFO:__main__:Head has 4225 parameters
Splitting dataset:   0%|          | 0/2372674 [00:00<?, ?it/s]Splitting dataset:   1%|          | 16440/2372674 [00:00<00:14, 164369.59it/s]Splitting dataset:   1%|▏         | 34054/2372674 [00:00<00:13, 171289.81it/s]Splitting dataset:   2%|▏         | 52895/2372674 [00:00<00:12, 179101.66it/s]Splitting dataset:   3%|▎         | 71707/2372674 [00:00<00:12, 182657.60it/s]Splitting dataset:   4%|▍         | 90963/2372674 [00:00<00:12, 186224.37it/s]Splitting dataset:   5%|▍         | 109586/2372674 [00:00<00:13, 171211.88it/s]Splitting dataset:   5%|▌         | 126911/2372674 [00:00<00:14, 159095.68it/s]Splitting dataset:   6%|▌         | 143394/2372674 [00:00<00:13, 160754.19it/s]Splitting dataset:   7%|▋         | 161244/2372674 [00:00<00:13, 165947.63it/s]Splitting dataset:   8%|▊         | 179405/2372674 [00:01<00:12, 170561.63it/s]Splitting dataset:   8%|▊         | 197383/2372674 [00:01<00:12, 173291.40it/s]Splitting dataset:   9%|▉         | 215030/2372674 [00:01<00:12, 174234.12it/s]Splitting dataset:  10%|▉         | 233942/2372674 [00:01<00:11, 178671.13it/s]Splitting dataset:  11%|█         | 252464/2372674 [00:01<00:11, 180618.88it/s]Splitting dataset:  11%|█▏        | 270569/2372674 [00:01<00:11, 180740.28it/s]Splitting dataset:  12%|█▏        | 288911/2372674 [00:01<00:11, 181540.12it/s]Splitting dataset:  13%|█▎        | 308961/2372674 [00:01<00:11, 187217.15it/s]Splitting dataset:  14%|█▍        | 327700/2372674 [00:01<00:10, 186292.12it/s]Splitting dataset:  15%|█▍        | 346548/2372674 [00:01<00:10, 186940.74it/s]Splitting dataset:  15%|█▌        | 365252/2372674 [00:02<00:10, 186955.01it/s]Splitting dataset:  16%|█▌        | 383954/2372674 [00:02<00:10, 185784.41it/s]Splitting dataset:  17%|█▋        | 402539/2372674 [00:02<00:10, 184243.81it/s]Splitting dataset:  18%|█▊        | 421447/2372674 [00:02<00:10, 185678.89it/s]Splitting dataset:  19%|█▊        | 440021/2372674 [00:02<00:10, 185054.23it/s]Splitting dataset:  19%|█▉        | 458888/2372674 [00:02<00:10, 186127.69it/s]Splitting dataset:  20%|██        | 478450/2372674 [00:02<00:10, 188957.26it/s]Splitting dataset:  21%|██        | 498583/2372674 [00:02<00:09, 192651.58it/s]Splitting dataset:  22%|██▏       | 517853/2372674 [00:02<00:09, 191842.62it/s]Splitting dataset:  23%|██▎       | 537041/2372674 [00:02<00:09, 185974.78it/s]Splitting dataset:  23%|██▎       | 555678/2372674 [00:03<00:09, 184078.92it/s]Splitting dataset:  24%|██▍       | 574114/2372674 [00:03<00:09, 180258.57it/s]Splitting dataset:  25%|██▍       | 592553/2372674 [00:03<00:09, 181453.88it/s]Splitting dataset:  26%|██▌       | 611245/2372674 [00:03<00:09, 183047.38it/s]Splitting dataset:  27%|██▋       | 629571/2372674 [00:03<00:09, 179401.40it/s]Splitting dataset:  27%|██▋       | 647537/2372674 [00:03<00:09, 177905.82it/s]Splitting dataset:  28%|██▊       | 665345/2372674 [00:03<00:09, 177434.59it/s]Splitting dataset:  29%|██▉       | 684616/2372674 [00:03<00:09, 181928.82it/s]Splitting dataset:  30%|██▉       | 703787/2372674 [00:03<00:09, 184821.77it/s]Splitting dataset:  30%|███       | 722719/2372674 [00:03<00:08, 186157.11it/s]Splitting dataset:  31%|███▏      | 741783/2372674 [00:04<00:08, 187489.52it/s]Splitting dataset:  32%|███▏      | 760542/2372674 [00:04<00:08, 187250.69it/s]Splitting dataset:  33%|███▎      | 779953/2372674 [00:04<00:08, 189297.19it/s]Splitting dataset:  34%|███▎      | 798889/2372674 [00:04<00:08, 188446.57it/s]Splitting dataset:  34%|███▍      | 817739/2372674 [00:04<00:08, 188416.99it/s]Splitting dataset:  35%|███▌      | 837183/2372674 [00:04<00:08, 190214.33it/s]Splitting dataset:  36%|███▌      | 856892/2372674 [00:04<00:07, 192268.00it/s]Splitting dataset:  37%|███▋      | 876858/2372674 [00:04<00:07, 194479.29it/s]Splitting dataset:  38%|███▊      | 896309/2372674 [00:04<00:07, 193343.82it/s]Splitting dataset:  39%|███▊      | 915647/2372674 [00:04<00:07, 192721.75it/s]Splitting dataset:  39%|███▉      | 935118/2372674 [00:05<00:07, 193312.18it/s]Splitting dataset:  40%|████      | 955213/2372674 [00:05<00:07, 195590.96it/s]Splitting dataset:  41%|████      | 974938/2372674 [00:05<00:07, 196083.79it/s]Splitting dataset:  42%|████▏     | 995218/2372674 [00:05<00:06, 198092.62it/s]Splitting dataset:  43%|████▎     | 1015120/2372674 [00:05<00:06, 198368.37it/s]Splitting dataset:  44%|████▎     | 1034959/2372674 [00:05<00:06, 194554.35it/s]Splitting dataset:  44%|████▍     | 1054432/2372674 [00:05<00:06, 193231.70it/s]Splitting dataset:  45%|████▌     | 1074463/2372674 [00:05<00:06, 195319.64it/s]Splitting dataset:  46%|████▌     | 1094007/2372674 [00:05<00:06, 194565.72it/s]Splitting dataset:  47%|████▋     | 1113472/2372674 [00:06<00:06, 189780.74it/s]Splitting dataset:  48%|████▊     | 1132478/2372674 [00:06<00:06, 189522.02it/s]Splitting dataset:  49%|████▊     | 1152702/2372674 [00:06<00:06, 193264.28it/s]Splitting dataset:  49%|████▉     | 1172671/2372674 [00:06<00:06, 195158.88it/s]Splitting dataset:  50%|█████     | 1192204/2372674 [00:06<00:06, 193495.06it/s]Splitting dataset:  51%|█████     | 1212248/2372674 [00:06<00:05, 195549.71it/s]Splitting dataset:  52%|█████▏    | 1232060/2372674 [00:06<00:05, 196311.37it/s]Splitting dataset:  53%|█████▎    | 1251720/2372674 [00:06<00:05, 196393.85it/s]Splitting dataset:  54%|█████▎    | 1272169/2372674 [00:06<00:05, 198808.99it/s]Splitting dataset:  54%|█████▍    | 1292056/2372674 [00:06<00:05, 196408.50it/s]Splitting dataset:  55%|█████▌    | 1311727/2372674 [00:07<00:05, 196491.37it/s]Splitting dataset:  56%|█████▌    | 1331384/2372674 [00:07<00:05, 195341.99it/s]Splitting dataset:  57%|█████▋    | 1351257/2372674 [00:07<00:05, 196347.03it/s]Splitting dataset:  58%|█████▊    | 1370897/2372674 [00:07<00:05, 194619.88it/s]Splitting dataset:  59%|█████▊    | 1390571/2372674 [00:07<00:05, 195247.26it/s]Splitting dataset:  59%|█████▉    | 1410101/2372674 [00:07<00:04, 193961.60it/s]Splitting dataset:  60%|██████    | 1429502/2372674 [00:07<00:04, 193121.09it/s]Splitting dataset:  61%|██████    | 1448818/2372674 [00:07<00:04, 192792.13it/s]Splitting dataset:  62%|██████▏   | 1468351/2372674 [00:07<00:04, 193542.81it/s]Splitting dataset:  63%|██████▎   | 1487708/2372674 [00:07<00:04, 192124.74it/s]Splitting dataset:  64%|██████▎   | 1507699/2372674 [00:08<00:04, 194435.97it/s]Splitting dataset:  64%|██████▍   | 1528604/2372674 [00:08<00:04, 198788.00it/s]Splitting dataset:  65%|██████▌   | 1549493/2372674 [00:08<00:04, 201802.18it/s]Splitting dataset:  66%|██████▌   | 1570318/2372674 [00:08<00:03, 203728.76it/s]Splitting dataset:  67%|██████▋   | 1590696/2372674 [00:08<00:03, 202544.44it/s]Splitting dataset:  68%|██████▊   | 1610955/2372674 [00:08<00:03, 202053.86it/s]Splitting dataset:  69%|██████▉   | 1631801/2372674 [00:08<00:03, 203962.99it/s]Splitting dataset:  70%|██████▉   | 1653057/2372674 [00:08<00:03, 206527.92it/s]Splitting dataset:  71%|███████   | 1674196/2372674 [00:08<00:03, 207979.94it/s]Splitting dataset:  71%|███████▏  | 1694997/2372674 [00:08<00:03, 206433.49it/s]Splitting dataset:  72%|███████▏  | 1715645/2372674 [00:09<00:03, 200601.88it/s]Splitting dataset:  73%|███████▎  | 1735741/2372674 [00:09<00:03, 199729.37it/s]Splitting dataset:  74%|███████▍  | 1755867/2372674 [00:09<00:03, 200176.01it/s]Splitting dataset:  75%|███████▍  | 1776058/2372674 [00:09<00:02, 200678.63it/s]Splitting dataset:  76%|███████▌  | 1796139/2372674 [00:09<00:02, 199728.28it/s]Splitting dataset:  77%|███████▋  | 1816211/2372674 [00:09<00:02, 200018.57it/s]Splitting dataset:  77%|███████▋  | 1836220/2372674 [00:09<00:02, 193028.47it/s]Splitting dataset:  78%|███████▊  | 1855577/2372674 [00:09<00:02, 189578.36it/s]Splitting dataset:  79%|███████▉  | 1875631/2372674 [00:09<00:02, 192751.62it/s]Splitting dataset:  80%|███████▉  | 1894947/2372674 [00:09<00:02, 190433.41it/s]Splitting dataset:  81%|████████  | 1914021/2372674 [00:10<00:02, 185588.09it/s]Splitting dataset:  81%|████████▏ | 1932944/2372674 [00:10<00:02, 186638.60it/s]Splitting dataset:  82%|████████▏ | 1952230/2372674 [00:10<00:02, 188452.77it/s]Splitting dataset:  83%|████████▎ | 1971102/2372674 [00:10<00:02, 188371.33it/s]Splitting dataset:  84%|████████▍ | 1990051/2372674 [00:10<00:02, 188699.97it/s]Splitting dataset:  85%|████████▍ | 2009623/2372674 [00:10<00:01, 190784.07it/s]Splitting dataset:  86%|████████▌ | 2029033/2372674 [00:10<00:01, 191769.47it/s]Splitting dataset:  86%|████████▋ | 2048689/2372674 [00:10<00:01, 193196.98it/s]Splitting dataset:  87%|████████▋ | 2068045/2372674 [00:10<00:01, 193302.97it/s]Splitting dataset:  88%|████████▊ | 2087380/2372674 [00:10<00:01, 190943.46it/s]Splitting dataset:  89%|████████▉ | 2106484/2372674 [00:11<00:01, 187906.45it/s]Splitting dataset:  90%|████████▉ | 2125290/2372674 [00:11<00:01, 186335.94it/s]Splitting dataset:  90%|█████████ | 2143935/2372674 [00:11<00:01, 184360.01it/s]Splitting dataset:  91%|█████████ | 2162380/2372674 [00:11<00:01, 184332.66it/s]Splitting dataset:  92%|█████████▏| 2183237/2372674 [00:11<00:00, 191492.23it/s]Splitting dataset:  93%|█████████▎| 2204053/2372674 [00:11<00:00, 196438.79it/s]Splitting dataset:  94%|█████████▍| 2225303/2372674 [00:11<00:00, 201220.28it/s]Splitting dataset:  95%|█████████▍| 2246523/2372674 [00:11<00:00, 204486.73it/s]Splitting dataset:  96%|█████████▌| 2268052/2372674 [00:11<00:00, 207711.90it/s]Splitting dataset:  96%|█████████▋| 2289202/2372674 [00:12<00:00, 208842.03it/s]Splitting dataset:  97%|█████████▋| 2310094/2372674 [00:12<00:00, 203643.81it/s]Splitting dataset:  98%|█████████▊| 2330492/2372674 [00:12<00:00, 190663.27it/s]Splitting dataset:  99%|█████████▉| 2349733/2372674 [00:12<00:00, 185290.51it/s]Splitting dataset: 100%|█████████▉| 2368392/2372674 [00:12<00:00, 181953.39it/s]Splitting dataset: 100%|██████████| 2372674/2372674 [00:12<00:00, 190135.33it/s]
INFO:root:processing dataset...
  0%|          | 0/5453 [00:00<?, ?it/s]  1%|          | 53/5453 [00:00<00:10, 523.75it/s]  2%|▏         | 106/5453 [00:00<00:10, 521.61it/s]  3%|▎         | 167/5453 [00:00<00:09, 558.41it/s]  4%|▍         | 223/5453 [00:00<00:09, 545.63it/s]/data02/gtguo/DEL/pkg/utils/mol_feat.py:71: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matricesor `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484808560/work/aten/src/ATen/native/TensorShape.cpp:2981.)
  return torch.tensor(coo, dtype=torch.long).T
  5%|▌         | 292/5453 [00:00<00:08, 596.07it/s]  6%|▋         | 352/5453 [00:00<00:08, 567.58it/s]  8%|▊         | 418/5453 [00:00<00:08, 595.85it/s]  9%|▉         | 478/5453 [00:00<00:08, 566.57it/s] 10%|▉         | 536/5453 [00:00<00:08, 557.11it/s] 11%|█         | 595/5453 [00:01<00:08, 564.02it/s] 12%|█▏        | 652/5453 [00:01<00:08, 540.99it/s] 13%|█▎        | 707/5453 [00:01<00:08, 531.38it/s] 14%|█▍        | 762/5453 [00:01<00:08, 536.61it/s] 15%|█▍        | 816/5453 [00:01<00:08, 532.24it/s] 16%|█▌        | 870/5453 [00:01<00:08, 517.66it/s] 17%|█▋        | 924/5453 [00:01<00:08, 519.62it/s] 18%|█▊        | 977/5453 [00:01<00:08, 517.14it/s] 19%|█▉        | 1029/5453 [00:01<00:08, 500.09it/s] 20%|█▉        | 1080/5453 [00:02<00:09, 458.41it/s] 21%|██        | 1127/5453 [00:02<00:09, 449.06it/s] 22%|██▏       | 1179/5453 [00:02<00:09, 467.22it/s] 23%|██▎       | 1231/5453 [00:02<00:08, 480.45it/s] 24%|██▎       | 1284/5453 [00:02<00:08, 493.80it/s] 24%|██▍       | 1334/5453 [00:02<00:08, 483.44it/s] 26%|██▌       | 1397/5453 [00:02<00:07, 522.39it/s] 27%|██▋       | 1453/5453 [00:02<00:07, 531.47it/s] 28%|██▊       | 1507/5453 [00:02<00:07, 532.52it/s] 29%|██▊       | 1563/5453 [00:02<00:07, 540.13it/s] 30%|██▉       | 1621/5453 [00:03<00:06, 548.94it/s] 31%|███       | 1676/5453 [00:03<00:06, 542.84it/s] 32%|███▏      | 1736/5453 [00:03<00:06, 558.23it/s] 33%|███▎      | 1792/5453 [00:03<00:06, 540.41it/s] 34%|███▍      | 1847/5453 [00:03<00:06, 526.60it/s] 35%|███▍      | 1900/5453 [00:03<00:07, 500.87it/s] 36%|███▌      | 1951/5453 [00:03<00:07, 497.42it/s] 37%|███▋      | 2006/5453 [00:03<00:06, 511.89it/s] 38%|███▊      | 2059/5453 [00:03<00:06, 515.61it/s] 39%|███▊      | 2113/5453 [00:04<00:06, 522.23it/s] 40%|███▉      | 2166/5453 [00:04<00:06, 523.26it/s] 41%|████      | 2224/5453 [00:04<00:06, 537.99it/s] 42%|████▏     | 2278/5453 [00:04<00:06, 516.31it/s] 43%|████▎     | 2332/5453 [00:04<00:05, 521.13it/s] 44%|████▍     | 2388/5453 [00:04<00:05, 532.31it/s] 45%|████▍     | 2443/5453 [00:04<00:05, 537.31it/s] 46%|████▌     | 2497/5453 [00:04<00:05, 532.05it/s] 47%|████▋     | 2551/5453 [00:04<00:05, 506.35it/s] 48%|████▊     | 2603/5453 [00:04<00:05, 509.76it/s] 49%|████▊     | 2656/5453 [00:05<00:05, 514.29it/s] 50%|████▉     | 2708/5453 [00:05<00:05, 493.80it/s] 51%|█████     | 2758/5453 [00:05<00:05, 489.94it/s] 52%|█████▏    | 2813/5453 [00:05<00:05, 506.95it/s] 53%|█████▎    | 2864/5453 [00:05<00:05, 507.74it/s] 53%|█████▎    | 2915/5453 [00:05<00:05, 482.61it/s] 54%|█████▍    | 2964/5453 [00:05<00:05, 449.80it/s] 55%|█████▌    | 3017/5453 [00:05<00:05, 469.65it/s] 56%|█████▌    | 3065/5453 [00:05<00:05, 469.73it/s] 57%|█████▋    | 3115/5453 [00:06<00:04, 475.20it/s] 58%|█████▊    | 3164/5453 [00:06<00:04, 478.84it/s] 59%|█████▉    | 3213/5453 [00:06<00:04, 467.02it/s] 60%|█████▉    | 3262/5453 [00:06<00:04, 473.42it/s] 61%|██████    | 3312/5453 [00:06<00:04, 480.47it/s] 62%|██████▏   | 3362/5453 [00:06<00:04, 485.77it/s] 63%|██████▎   | 3411/5453 [00:06<00:04, 484.53it/s] 63%|██████▎   | 3460/5453 [00:06<00:04, 485.37it/s] 64%|██████▍   | 3509/5453 [00:06<00:04, 481.28it/s] 65%|██████▌   | 3560/5453 [00:06<00:03, 488.63it/s] 66%|██████▌   | 3610/5453 [00:07<00:03, 490.04it/s] 67%|██████▋   | 3660/5453 [00:07<00:03, 488.63it/s] 68%|██████▊   | 3711/5453 [00:07<00:03, 494.92it/s] 69%|██████▉   | 3761/5453 [00:07<00:05, 307.58it/s] 70%|██████▉   | 3808/5453 [00:07<00:04, 339.67it/s] 71%|███████   | 3856/5453 [00:07<00:04, 370.85it/s] 72%|███████▏  | 3907/5453 [00:07<00:03, 404.75it/s] 73%|███████▎  | 3955/5453 [00:07<00:03, 423.68it/s] 73%|███████▎  | 4007/5453 [00:08<00:03, 449.58it/s] 74%|███████▍  | 4056/5453 [00:08<00:03, 459.59it/s] 75%|███████▌  | 4107/5453 [00:08<00:02, 472.40it/s] 76%|███████▌  | 4156/5453 [00:08<00:02, 473.31it/s] 77%|███████▋  | 4207/5453 [00:08<00:02, 482.53it/s] 78%|███████▊  | 4257/5453 [00:08<00:02, 459.50it/s] 79%|███████▉  | 4305/5453 [00:08<00:02, 464.78it/s] 80%|███████▉  | 4354/5453 [00:08<00:02, 471.07it/s] 81%|████████  | 4404/5453 [00:08<00:02, 478.42it/s] 82%|████████▏ | 4453/5453 [00:09<00:02, 466.73it/s] 83%|████████▎ | 4501/5453 [00:09<00:02, 468.14it/s] 83%|████████▎ | 4549/5453 [00:09<00:01, 466.00it/s] 84%|████████▍ | 4597/5453 [00:09<00:01, 469.04it/s] 85%|████████▌ | 4645/5453 [00:09<00:01, 467.70it/s] 86%|████████▌ | 4692/5453 [00:09<00:01, 456.13it/s] 87%|████████▋ | 4738/5453 [00:09<00:01, 454.14it/s] 88%|████████▊ | 4784/5453 [00:09<00:01, 448.39it/s] 89%|████████▊ | 4833/5453 [00:09<00:01, 459.07it/s] 90%|████████▉ | 4882/5453 [00:09<00:01, 466.80it/s] 90%|█████████ | 4932/5453 [00:10<00:01, 474.26it/s] 91%|█████████▏| 4981/5453 [00:10<00:00, 478.64it/s] 92%|█████████▏| 5029/5453 [00:10<00:00, 468.98it/s] 93%|█████████▎| 5077/5453 [00:10<00:00, 471.59it/s] 94%|█████████▍| 5126/5453 [00:10<00:00, 475.09it/s] 95%|█████████▍| 5175/5453 [00:10<00:00, 477.19it/s] 96%|█████████▌| 5224/5453 [00:10<00:00, 479.25it/s] 97%|█████████▋| 5272/5453 [00:10<00:00, 471.96it/s] 98%|█████████▊| 5320/5453 [00:10<00:00, 473.55it/s] 98%|█████████▊| 5368/5453 [00:10<00:00, 470.47it/s] 99%|█████████▉| 5416/5453 [00:11<00:00, 457.30it/s]100%|██████████| 5453/5453 [00:11<00:00, 486.85it/s]
INFO:__main__:Dataset has 5453 samples
INFO:__main__:Dataset data example: {'pyg_data': Data(x=[22, 19], edge_index=[2, 50], edge_attr=[50, 2]), 'activity': 5.028260409112222}
INFO:__main__:Train loader has 2 data
3408 2045
2726 682
/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([2048])) that is different to the input size (torch.Size([2048, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([678])) that is different to the input size (torch.Size([678, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([682])) that is different to the input size (torch.Size([682, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:__main__:Epoch 0, Train Loss 50.71177673339844
INFO:__main__:Epoch 0, Valid Loss 51.00627899169922
INFO:__main__:Saving model at epoch 0
INFO:__main__:Epoch 1, Train Loss 50.59336853027344
INFO:__main__:Epoch 1, Valid Loss 50.5920295715332
INFO:__main__:Epoch 2, Train Loss 49.890933990478516
INFO:__main__:Epoch 2, Valid Loss 50.24348831176758
INFO:__main__:Epoch 3, Train Loss 49.26649475097656
INFO:__main__:Epoch 3, Valid Loss 49.835994720458984
INFO:__main__:Epoch 4, Train Loss 49.51306915283203
INFO:__main__:Epoch 4, Valid Loss 49.44705581665039
INFO:__main__:Epoch 5, Train Loss 48.84146499633789
INFO:__main__:Epoch 5, Valid Loss 49.004432678222656
INFO:__main__:Saving model at epoch 5
INFO:__main__:Epoch 6, Train Loss 48.803367614746094
INFO:__main__:Epoch 6, Valid Loss 48.486793518066406
INFO:__main__:Epoch 7, Train Loss 47.751747131347656
INFO:__main__:Epoch 7, Valid Loss 47.97610855102539
INFO:__main__:Epoch 8, Train Loss 46.680389404296875
INFO:__main__:Epoch 8, Valid Loss 47.40644454956055
INFO:__main__:Epoch 9, Train Loss 46.49229431152344
INFO:__main__:Epoch 9, Valid Loss 46.801395416259766
INFO:__main__:Epoch 10, Train Loss 45.4331169128418
INFO:__main__:Epoch 10, Valid Loss 46.1204833984375
INFO:__main__:Saving model at epoch 10
INFO:__main__:Epoch 11, Train Loss 45.0544548034668
INFO:__main__:Epoch 11, Valid Loss 45.395164489746094
INFO:__main__:Epoch 12, Train Loss 44.4674072265625
INFO:__main__:Epoch 12, Valid Loss 44.630653381347656
INFO:__main__:Epoch 13, Train Loss 44.38210678100586
INFO:__main__:Epoch 13, Valid Loss 43.79936218261719
INFO:__main__:Epoch 14, Train Loss 42.6744270324707
INFO:__main__:Epoch 14, Valid Loss 42.905025482177734
INFO:__main__:Epoch 15, Train Loss 41.886756896972656
INFO:__main__:Epoch 15, Valid Loss 41.94276809692383
INFO:__main__:Saving model at epoch 15
/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([2048])) that is different to the input size (torch.Size([2047, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:__main__:Epoch 16, Train Loss 41.21464920043945
INFO:__main__:Epoch 16, Valid Loss 40.92275619506836
INFO:__main__:Epoch 17, Train Loss 39.620460510253906
INFO:__main__:Epoch 17, Valid Loss 39.84153747558594
INFO:__main__:Epoch 18, Train Loss 38.955238342285156
INFO:__main__:Epoch 18, Valid Loss 38.68915939331055
INFO:__main__:Epoch 19, Train Loss 37.54118728637695
INFO:__main__:Epoch 19, Valid Loss 37.45791244506836
INFO:__main__:Epoch 20, Train Loss 37.196895599365234
INFO:__main__:Epoch 20, Valid Loss 36.138973236083984
INFO:__main__:Saving model at epoch 20
INFO:__main__:Epoch 21, Train Loss 34.87608337402344
INFO:__main__:Epoch 21, Valid Loss 34.72764205932617
INFO:__main__:Epoch 22, Train Loss 34.03922653198242
INFO:__main__:Epoch 22, Valid Loss 33.23155212402344
INFO:__main__:Epoch 23, Train Loss 31.747390747070312
INFO:__main__:Epoch 23, Valid Loss 31.633764266967773
INFO:__main__:Epoch 24, Train Loss 29.98186492919922
INFO:__main__:Epoch 24, Valid Loss 29.94985008239746
INFO:__main__:Epoch 25, Train Loss 29.053390502929688
INFO:__main__:Epoch 25, Valid Loss 28.171890258789062
INFO:__main__:Saving model at epoch 25
INFO:__main__:Epoch 26, Train Loss 26.668733596801758
INFO:__main__:Epoch 26, Valid Loss 26.307058334350586
INFO:__main__:Epoch 27, Train Loss 25.351545333862305
INFO:__main__:Epoch 27, Valid Loss 24.35439109802246
INFO:__main__:Epoch 28, Train Loss 22.703285217285156
INFO:__main__:Epoch 28, Valid Loss 22.322811126708984
INFO:__main__:Epoch 29, Train Loss 21.282819747924805
INFO:__main__:Epoch 29, Valid Loss 20.24201774597168
INFO:__main__:Epoch 30, Train Loss 19.500900268554688
INFO:__main__:Epoch 30, Valid Loss 18.120607376098633
INFO:__main__:Saving model at epoch 30
INFO:__main__:Epoch 31, Train Loss 17.037273406982422
INFO:__main__:Epoch 31, Valid Loss 15.974177360534668
INFO:__main__:Epoch 32, Train Loss 14.81799602508545
INFO:__main__:Epoch 32, Valid Loss 13.828961372375488
INFO:__main__:Epoch 33, Train Loss 12.082521438598633
INFO:__main__:Epoch 33, Valid Loss 11.722908973693848
INFO:__main__:Epoch 34, Train Loss 10.384893417358398
INFO:__main__:Epoch 34, Valid Loss 9.700799942016602
INFO:__main__:Epoch 35, Train Loss 8.821464538574219
INFO:__main__:Epoch 35, Valid Loss 7.8057661056518555
INFO:__main__:Saving model at epoch 35
INFO:__main__:Epoch 36, Train Loss 6.829573154449463
INFO:__main__:Epoch 36, Valid Loss 6.083117485046387
INFO:__main__:Epoch 37, Train Loss 5.250802993774414
INFO:__main__:Epoch 37, Valid Loss 4.581681728363037
INFO:__main__:Epoch 38, Train Loss 4.155769348144531
INFO:__main__:Epoch 38, Valid Loss 3.3479201793670654
INFO:__main__:Epoch 39, Train Loss 2.898890256881714
INFO:__main__:Epoch 39, Valid Loss 2.409388780593872
INFO:__main__:Epoch 40, Train Loss 2.0260422229766846
INFO:__main__:Epoch 40, Valid Loss 1.7789350748062134
INFO:__main__:Saving model at epoch 40
INFO:__main__:Epoch 41, Train Loss 1.9236502647399902
INFO:__main__:Epoch 41, Valid Loss 1.439947485923767
INFO:__main__:Epoch 42, Train Loss 1.6723134517669678
INFO:__main__:Epoch 42, Valid Loss 1.3421260118484497
INFO:__main__:Epoch 43, Train Loss 1.4926010370254517
INFO:__main__:Epoch 43, Valid Loss 1.408750295639038
INFO:__main__:Epoch 44, Train Loss 1.9325684309005737
INFO:__main__:Epoch 44, Valid Loss 1.5498385429382324
INFO:__main__:Epoch 45, Train Loss 2.1325693130493164
INFO:__main__:Epoch 45, Valid Loss 1.6801844835281372
INFO:__main__:Epoch 46, Train Loss 2.047140598297119
INFO:__main__:Epoch 46, Valid Loss 1.7408466339111328
INFO:__main__:Epoch 47, Train Loss 2.2068259716033936
INFO:__main__:Epoch 47, Valid Loss 1.709760069847107
INFO:__main__:Epoch 48, Train Loss 2.035848617553711
INFO:__main__:Epoch 48, Valid Loss 1.5827823877334595
INFO:__main__:Epoch 49, Train Loss 1.6898177862167358
INFO:__main__:Epoch 49, Valid Loss 1.3967057466506958
INFO:__main__:Epoch 50, Train Loss 1.7144724130630493
INFO:__main__:Epoch 50, Valid Loss 1.379690170288086
INFO:__main__:Epoch 51, Train Loss 1.909430742263794
INFO:__main__:Epoch 51, Valid Loss 1.5241611003875732
INFO:__main__:Epoch 52, Train Loss 1.8036954402923584
INFO:__main__:Epoch 52, Valid Loss 1.4188833236694336
INFO:__main__:Epoch 53, Train Loss 1.3414982557296753
INFO:__main__:Epoch 53, Valid Loss 1.3476358652114868
INFO:__main__:Epoch 54, Train Loss 1.7267085313796997
INFO:__main__:Epoch 54, Valid Loss 1.3437994718551636
INFO:__main__:Epoch 55, Train Loss 1.20509934425354
INFO:__main__:Epoch 55, Valid Loss 1.3490675687789917
INFO:__main__:Epoch 56, Train Loss 1.3452166318893433
INFO:__main__:Epoch 56, Valid Loss 1.3488999605178833
INFO:__main__:Epoch 57, Train Loss 1.8919575214385986
INFO:__main__:Epoch 57, Valid Loss 1.3445148468017578
INFO:__main__:Epoch 58, Train Loss 1.4040558338165283
INFO:__main__:Epoch 58, Valid Loss 1.3420401811599731
INFO:__main__:Epoch 59, Train Loss 1.3904460668563843
INFO:__main__:Epoch 59, Valid Loss 1.345987319946289
INFO:__main__:Epoch 60, Train Loss 1.7288234233856201
INFO:__main__:Epoch 60, Valid Loss 1.3549892902374268
INFO:__main__:Epoch 61, Train Loss 1.7334543466567993
INFO:__main__:Epoch 61, Valid Loss 1.363339900970459
INFO:__main__:Epoch 62, Train Loss 1.50461745262146
INFO:__main__:Epoch 62, Valid Loss 1.3641505241394043
INFO:__main__:Epoch 63, Train Loss 1.6115673780441284
INFO:__main__:Epoch 63, Valid Loss 1.3583229780197144
INFO:__main__:Epoch 64, Train Loss 1.5502777099609375
INFO:__main__:Epoch 64, Valid Loss 1.3510764837265015
INFO:__main__:Epoch 65, Train Loss 1.603916049003601
INFO:__main__:Epoch 65, Valid Loss 1.345847249031067
INFO:__main__:Epoch 66, Train Loss 1.2447274923324585
INFO:__main__:Epoch 66, Valid Loss 1.3430631160736084
INFO:__main__:Epoch 67, Train Loss 1.5077852010726929
INFO:__main__:Epoch 67, Valid Loss 1.3421014547348022
/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch_geometric/data/collate.py:204: UserWarning: An output with one or more elements was resized since it had shape [17293], which does not match the required output shape [17293, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484808560/work/aten/src/ATen/native/Resize.cpp:17.)
  value = torch.cat(values, dim=cat_dim or 0, out=out)
/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch_geometric/data/collate.py:204: UserWarning: An output with one or more elements was resized since it had shape [36924], which does not match the required output shape [2, 36924]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484808560/work/aten/src/ATen/native/Resize.cpp:17.)
  value = torch.cat(values, dim=cat_dim or 0, out=out)
/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch_geometric/data/collate.py:204: UserWarning: An output with one or more elements was resized since it had shape [36924], which does not match the required output shape [36924, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484808560/work/aten/src/ATen/native/Resize.cpp:17.)
  value = torch.cat(values, dim=cat_dim or 0, out=out)
INFO:__main__:Epoch 68, Train Loss 1.6664923429489136
INFO:__main__:Epoch 68, Valid Loss 1.3419862985610962
INFO:__main__:Epoch 69, Train Loss 1.5679283142089844
INFO:__main__:Epoch 69, Valid Loss 1.3419814109802246
INFO:__main__:Epoch 70, Train Loss 1.4163955450057983
INFO:__main__:Epoch 70, Valid Loss 1.3421435356140137
INFO:__main__:Epoch 71, Train Loss 1.8361784219741821
INFO:__main__:Epoch 71, Valid Loss 1.3428107500076294
INFO:__main__:Epoch 72, Train Loss 1.4992350339889526
INFO:__main__:Epoch 72, Valid Loss 1.3442316055297852
INFO:__main__:Epoch 73, Train Loss 1.1715362071990967
INFO:__main__:Epoch 73, Valid Loss 1.345871090888977
INFO:__main__:Epoch 74, Train Loss 1.3664096593856812
INFO:__main__:Epoch 74, Valid Loss 1.3465688228607178
INFO:__main__:Epoch 75, Train Loss 1.6724233627319336
INFO:__main__:Epoch 75, Valid Loss 1.346461534500122
INFO:__main__:Epoch 76, Train Loss 1.7444491386413574
INFO:__main__:Epoch 76, Valid Loss 1.3459110260009766
INFO:__main__:Epoch 77, Train Loss 1.4835911989212036
INFO:__main__:Epoch 77, Valid Loss 1.3456754684448242
INFO:__main__:Epoch 78, Train Loss 1.5485944747924805
INFO:__main__:Epoch 78, Valid Loss 1.3457387685775757
INFO:__main__:Epoch 79, Train Loss 1.6028374433517456
INFO:__main__:Epoch 79, Valid Loss 1.3456265926361084
INFO:__main__:Epoch 80, Train Loss 1.7310720682144165
INFO:__main__:Epoch 80, Valid Loss 1.3455235958099365
INFO:__main__:Epoch 81, Train Loss 1.5845640897750854
INFO:__main__:Epoch 81, Valid Loss 1.3455036878585815
INFO:__main__:Epoch 82, Train Loss 1.4267821311950684
INFO:__main__:Epoch 82, Valid Loss 1.3454293012619019
INFO:__main__:Epoch 83, Train Loss 1.5697942972183228
INFO:__main__:Epoch 83, Valid Loss 1.344824194908142
INFO:__main__:Epoch 84, Train Loss 1.3873685598373413
INFO:__main__:Epoch 84, Valid Loss 1.344090223312378
INFO:__main__:Epoch 85, Train Loss 1.4204550981521606
INFO:__main__:Epoch 85, Valid Loss 1.3435208797454834
INFO:__main__:Epoch 86, Train Loss 1.5598357915878296
INFO:__main__:Epoch 86, Valid Loss 1.3432013988494873
INFO:__main__:Epoch 87, Train Loss 1.6550638675689697
INFO:__main__:Epoch 87, Valid Loss 1.3432568311691284
INFO:__main__:Epoch 88, Train Loss 1.648951768875122
INFO:__main__:Epoch 88, Valid Loss 1.343839168548584
INFO:__main__:Epoch 89, Train Loss 1.4445197582244873
INFO:__main__:Epoch 89, Valid Loss 1.3447009325027466
INFO:__main__:Epoch 90, Train Loss 1.6092309951782227
INFO:__main__:Epoch 90, Valid Loss 1.3453593254089355
INFO:__main__:Epoch 91, Train Loss 1.3586937189102173
INFO:__main__:Epoch 91, Valid Loss 1.3460208177566528
INFO:__main__:Epoch 92, Train Loss 1.6386672258377075
INFO:__main__:Epoch 92, Valid Loss 1.3462305068969727
INFO:__main__:Epoch 93, Train Loss 1.4161920547485352
INFO:__main__:Epoch 93, Valid Loss 1.3457757234573364
INFO:__main__:Epoch 94, Train Loss 1.668221116065979
INFO:__main__:Epoch 94, Valid Loss 1.345137119293213
INFO:__main__:Epoch 95, Train Loss 1.7056879997253418
INFO:__main__:Epoch 95, Valid Loss 1.3447250127792358
INFO:__main__:Epoch 96, Train Loss 1.2745835781097412
INFO:__main__:Epoch 96, Valid Loss 1.3445875644683838
INFO:__main__:Epoch 97, Train Loss 1.5236420631408691
INFO:__main__:Epoch 97, Valid Loss 1.3442531824111938
INFO:__main__:Epoch 98, Train Loss 1.652873158454895
INFO:__main__:Epoch 98, Valid Loss 1.3442420959472656
INFO:__main__:Epoch 99, Train Loss 1.4790319204330444
INFO:__main__:Epoch 99, Valid Loss 1.3445680141448975
INFO:__main__:Epoch 100, Train Loss 1.4050806760787964
INFO:__main__:Epoch 100, Valid Loss 1.344873070716858
INFO:__main__:Epoch 101, Train Loss 1.5954867601394653
INFO:__main__:Epoch 101, Valid Loss 1.3451783657073975
INFO:__main__:Epoch 102, Train Loss 1.6805135011672974
INFO:__main__:Epoch 102, Valid Loss 1.345496654510498
INFO:__main__:Epoch 103, Train Loss 1.5897223949432373
INFO:__main__:Epoch 103, Valid Loss 1.3454686403274536
INFO:__main__:Epoch 104, Train Loss 1.622426152229309
INFO:__main__:Epoch 104, Valid Loss 1.3452417850494385
INFO:__main__:Epoch 105, Train Loss 1.2948342561721802
INFO:__main__:Epoch 105, Valid Loss 1.3449965715408325
INFO:__main__:Epoch 106, Train Loss 1.611780047416687
INFO:__main__:Epoch 106, Valid Loss 1.3446046113967896
INFO:__main__:Epoch 107, Train Loss 1.6707624197006226
INFO:__main__:Epoch 107, Valid Loss 1.344514012336731
INFO:__main__:Epoch 108, Train Loss 1.5212959051132202
INFO:__main__:Epoch 108, Valid Loss 1.3446438312530518
INFO:__main__:Epoch 109, Train Loss 1.587642788887024
INFO:__main__:Epoch 109, Valid Loss 1.344825267791748
INFO:__main__:Epoch 110, Train Loss 1.6203429698944092
INFO:__main__:Epoch 110, Valid Loss 1.3449808359146118
INFO:__main__:Epoch 111, Train Loss 1.4229332208633423
INFO:__main__:Epoch 111, Valid Loss 1.3450822830200195
INFO:__main__:Epoch 112, Train Loss 1.5560545921325684
INFO:__main__:Epoch 112, Valid Loss 1.3451275825500488
INFO:__main__:Epoch 113, Train Loss 1.5588722229003906
INFO:__main__:Epoch 113, Valid Loss 1.3451560735702515
INFO:__main__:Epoch 114, Train Loss 1.3139443397521973
INFO:__main__:Epoch 114, Valid Loss 1.3450952768325806
INFO:__main__:Epoch 115, Train Loss 1.5255422592163086
INFO:__main__:Epoch 115, Valid Loss 1.3446366786956787
INFO:__main__:Epoch 116, Train Loss 1.439996361732483
INFO:__main__:Epoch 116, Valid Loss 1.3444836139678955
INFO:__main__:Epoch 117, Train Loss 1.582706332206726
INFO:__main__:Epoch 117, Valid Loss 1.3448127508163452
INFO:__main__:Epoch 118, Train Loss 1.6613401174545288
INFO:__main__:Epoch 118, Valid Loss 1.3454004526138306
INFO:__main__:Epoch 119, Train Loss 1.6997543573379517
INFO:__main__:Epoch 119, Valid Loss 1.345649003982544
INFO:__main__:Epoch 120, Train Loss 1.661028265953064
INFO:__main__:Epoch 120, Valid Loss 1.345379114151001
INFO:__main__:Epoch 121, Train Loss 1.3816518783569336
INFO:__main__:Epoch 121, Valid Loss 1.3449219465255737
INFO:__main__:Epoch 122, Train Loss 1.23167884349823
INFO:__main__:Epoch 122, Valid Loss 1.3441834449768066
INFO:__main__:Epoch 123, Train Loss 1.60979425907135
INFO:__main__:Epoch 123, Valid Loss 1.343264102935791
INFO:__main__:Epoch 124, Train Loss 1.5659421682357788
INFO:__main__:Epoch 124, Valid Loss 1.3425060510635376
INFO:__main__:Epoch 125, Train Loss 1.4633889198303223
INFO:__main__:Epoch 125, Valid Loss 1.3421770334243774
INFO:__main__:Epoch 126, Train Loss 1.6778072118759155
INFO:__main__:Epoch 126, Valid Loss 1.3421889543533325
INFO:__main__:Epoch 127, Train Loss 1.4560409784317017
INFO:__main__:Epoch 127, Valid Loss 1.3425575494766235
INFO:__main__:Epoch 128, Train Loss 1.380760669708252
INFO:__main__:Epoch 128, Valid Loss 1.3436243534088135
INFO:__main__:Epoch 129, Train Loss 1.652190923690796
INFO:__main__:Epoch 129, Valid Loss 1.345292091369629
INFO:__main__:Epoch 130, Train Loss 1.3583735227584839
INFO:__main__:Epoch 130, Valid Loss 1.3467003107070923
INFO:__main__:Epoch 131, Train Loss 1.484062671661377
INFO:__main__:Epoch 131, Valid Loss 1.3472001552581787
INFO:__main__:Epoch 132, Train Loss 1.425010323524475
INFO:__main__:Epoch 132, Valid Loss 1.3470109701156616
INFO:__main__:Epoch 133, Train Loss 1.554126501083374
INFO:__main__:Epoch 133, Valid Loss 1.3461673259735107
INFO:__main__:Epoch 134, Train Loss 1.380508303642273
INFO:__main__:Epoch 134, Valid Loss 1.34529447555542
INFO:__main__:Epoch 135, Train Loss 1.483619213104248
INFO:__main__:Epoch 135, Valid Loss 1.34444260597229
INFO:__main__:Epoch 136, Train Loss 1.453450322151184
INFO:__main__:Epoch 136, Valid Loss 1.344003438949585
INFO:__main__:Epoch 137, Train Loss 1.4367434978485107
INFO:__main__:Epoch 137, Valid Loss 1.343613862991333
INFO:__main__:Epoch 138, Train Loss 1.4611879587173462
INFO:__main__:Epoch 138, Valid Loss 1.343348741531372
INFO:__main__:Epoch 139, Train Loss 1.5415658950805664
INFO:__main__:Epoch 139, Valid Loss 1.3433420658111572
INFO:__main__:Epoch 140, Train Loss 1.709275484085083
INFO:__main__:Epoch 140, Valid Loss 1.343620777130127
INFO:__main__:Epoch 141, Train Loss 1.6707371473312378
INFO:__main__:Epoch 141, Valid Loss 1.3440508842468262
INFO:__main__:Epoch 142, Train Loss 1.5612136125564575
INFO:__main__:Epoch 142, Valid Loss 1.3449043035507202
INFO:__main__:Epoch 143, Train Loss 1.5066734552383423
INFO:__main__:Epoch 143, Valid Loss 1.3458125591278076
INFO:__main__:Epoch 144, Train Loss 1.369045615196228
INFO:__main__:Epoch 144, Valid Loss 1.346373438835144
INFO:__main__:Epoch 145, Train Loss 1.3302083015441895
INFO:__main__:Epoch 145, Valid Loss 1.3457645177841187
INFO:__main__:Epoch 146, Train Loss 1.3463153839111328
INFO:__main__:Epoch 146, Valid Loss 1.3444374799728394
INFO:__main__:Epoch 147, Train Loss 1.2523442506790161
INFO:__main__:Epoch 147, Valid Loss 1.3433129787445068
INFO:__main__:Epoch 148, Train Loss 1.692720651626587
INFO:__main__:Epoch 148, Valid Loss 1.3425654172897339
INFO:__main__:Epoch 149, Train Loss 1.7498127222061157
INFO:__main__:Epoch 149, Valid Loss 1.342653512954712
INFO:__main__:Epoch 150, Train Loss 1.568237066268921
INFO:__main__:Epoch 150, Valid Loss 1.3430278301239014
INFO:__main__:Epoch 151, Train Loss 1.342020034790039
INFO:__main__:Epoch 151, Valid Loss 1.343239665031433
INFO:__main__:Epoch 152, Train Loss 1.4807261228561401
INFO:__main__:Epoch 152, Valid Loss 1.3429166078567505
INFO:__main__:Epoch 153, Train Loss 1.6785582304000854
INFO:__main__:Epoch 153, Valid Loss 1.3425791263580322
INFO:__main__:Epoch 154, Train Loss 1.3670310974121094
INFO:__main__:Epoch 154, Valid Loss 1.3430428504943848
INFO:__main__:Epoch 155, Train Loss 1.4312695264816284
INFO:__main__:Epoch 155, Valid Loss 1.3439804315567017
INFO:__main__:Epoch 156, Train Loss 1.447660207748413
INFO:__main__:Epoch 156, Valid Loss 1.3449631929397583
INFO:__main__:Epoch 157, Train Loss 1.3469812870025635
INFO:__main__:Epoch 157, Valid Loss 1.3459062576293945
INFO:__main__:Epoch 158, Train Loss 1.3095271587371826
INFO:__main__:Epoch 158, Valid Loss 1.346470832824707
INFO:__main__:Epoch 159, Train Loss 1.7884631156921387
INFO:__main__:Epoch 159, Valid Loss 1.3460314273834229
INFO:__main__:Epoch 160, Train Loss 1.5344923734664917
INFO:__main__:Epoch 160, Valid Loss 1.345482349395752
INFO:__main__:Epoch 161, Train Loss 1.4265344142913818
INFO:__main__:Epoch 161, Valid Loss 1.3445887565612793
INFO:__main__:Epoch 162, Train Loss 1.488542079925537
INFO:__main__:Epoch 162, Valid Loss 1.3437482118606567
INFO:__main__:Epoch 163, Train Loss 1.3263992071151733
INFO:__main__:Epoch 163, Valid Loss 1.3433233499526978
INFO:__main__:Epoch 164, Train Loss 1.5869616270065308
INFO:__main__:Epoch 164, Valid Loss 1.3430813550949097
INFO:__main__:Epoch 165, Train Loss 1.3814048767089844
INFO:__main__:Epoch 165, Valid Loss 1.3431154489517212
INFO:__main__:Epoch 166, Train Loss 1.4504053592681885
INFO:__main__:Epoch 166, Valid Loss 1.3432579040527344
INFO:__main__:Epoch 167, Train Loss 1.3830759525299072
INFO:__main__:Epoch 167, Valid Loss 1.3433648347854614
INFO:__main__:Epoch 168, Train Loss 1.3873258829116821
INFO:__main__:Epoch 168, Valid Loss 1.3431642055511475
INFO:__main__:Epoch 169, Train Loss 1.2987377643585205
INFO:__main__:Epoch 169, Valid Loss 1.343015193939209
INFO:__main__:Epoch 170, Train Loss 1.4235771894454956
INFO:__main__:Epoch 170, Valid Loss 1.3429402112960815
INFO:__main__:Epoch 171, Train Loss 1.5552922487258911
INFO:__main__:Epoch 171, Valid Loss 1.3430192470550537
INFO:__main__:Epoch 172, Train Loss 1.3524078130722046
INFO:__main__:Epoch 172, Valid Loss 1.3430577516555786
INFO:__main__:Epoch 173, Train Loss 1.37106192111969
INFO:__main__:Epoch 173, Valid Loss 1.3430095911026
INFO:__main__:Epoch 174, Train Loss 1.525560975074768
INFO:__main__:Epoch 174, Valid Loss 1.3431177139282227
INFO:__main__:Epoch 175, Train Loss 1.3065240383148193
INFO:__main__:Epoch 175, Valid Loss 1.3433917760849
INFO:__main__:Epoch 176, Train Loss 1.5240331888198853
INFO:__main__:Epoch 176, Valid Loss 1.3433259725570679
INFO:__main__:Epoch 177, Train Loss 1.4097213745117188
INFO:__main__:Epoch 177, Valid Loss 1.3433356285095215
INFO:__main__:Epoch 178, Train Loss 1.3654495477676392
INFO:__main__:Epoch 178, Valid Loss 1.343665599822998
INFO:__main__:Epoch 179, Train Loss 1.343387484550476
INFO:__main__:Epoch 179, Valid Loss 1.3441953659057617
INFO:__main__:Epoch 180, Train Loss 1.4110658168792725
INFO:__main__:Epoch 180, Valid Loss 1.344454050064087
INFO:__main__:Epoch 181, Train Loss 1.5492242574691772
INFO:__main__:Epoch 181, Valid Loss 1.3447656631469727
INFO:__main__:Epoch 182, Train Loss 1.3484653234481812
INFO:__main__:Epoch 182, Valid Loss 1.3448158502578735
INFO:__main__:Epoch 183, Train Loss 1.4581419229507446
INFO:__main__:Epoch 183, Valid Loss 1.3441967964172363
INFO:__main__:Epoch 184, Train Loss 1.9262596368789673
INFO:__main__:Epoch 184, Valid Loss 1.3436386585235596
INFO:__main__:Epoch 185, Train Loss 1.4269007444381714
INFO:__main__:Epoch 185, Valid Loss 1.3433783054351807
INFO:__main__:Epoch 186, Train Loss 1.504047155380249
INFO:__main__:Epoch 186, Valid Loss 1.3432022333145142
INFO:__main__:Epoch 187, Train Loss 1.3324100971221924
INFO:__main__:Epoch 187, Valid Loss 1.3435935974121094
INFO:__main__:Epoch 188, Train Loss 1.4209771156311035
INFO:__main__:Epoch 188, Valid Loss 1.3438565731048584
INFO:__main__:Epoch 189, Train Loss 1.7430171966552734
INFO:__main__:Epoch 189, Valid Loss 1.3435912132263184
/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch_geometric/data/collate.py:204: UserWarning: An output with one or more elements was resized since it had shape [52169], which does not match the required output shape [52169, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484808560/work/aten/src/ATen/native/Resize.cpp:17.)
  value = torch.cat(values, dim=cat_dim or 0, out=out)
/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch_geometric/data/collate.py:204: UserWarning: An output with one or more elements was resized since it had shape [111376], which does not match the required output shape [2, 111376]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484808560/work/aten/src/ATen/native/Resize.cpp:17.)
  value = torch.cat(values, dim=cat_dim or 0, out=out)
/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch_geometric/data/collate.py:204: UserWarning: An output with one or more elements was resized since it had shape [111376], which does not match the required output shape [111376, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484808560/work/aten/src/ATen/native/Resize.cpp:17.)
  value = torch.cat(values, dim=cat_dim or 0, out=out)
INFO:__main__:Epoch 190, Train Loss 1.5644514560699463
INFO:__main__:Epoch 190, Valid Loss 1.3433232307434082
INFO:__main__:Epoch 191, Train Loss 1.5743170976638794
INFO:__main__:Epoch 191, Valid Loss 1.3433265686035156
INFO:__main__:Epoch 192, Train Loss 1.4996229410171509
INFO:__main__:Epoch 192, Valid Loss 1.34341299533844
INFO:__main__:Epoch 193, Train Loss 1.6860748529434204
INFO:__main__:Epoch 193, Valid Loss 1.3438752889633179
INFO:__main__:Epoch 194, Train Loss 1.5314136743545532
INFO:__main__:Epoch 194, Valid Loss 1.3449180126190186
INFO:__main__:Epoch 195, Train Loss 1.544356346130371
INFO:__main__:Epoch 195, Valid Loss 1.3447723388671875
INFO:__main__:Epoch 196, Train Loss 1.7711193561553955
INFO:__main__:Epoch 196, Valid Loss 1.344354510307312
INFO:__main__:Epoch 197, Train Loss 1.38638174533844
INFO:__main__:Epoch 197, Valid Loss 1.3442506790161133
INFO:__main__:Epoch 198, Train Loss 1.525264024734497
INFO:__main__:Epoch 198, Valid Loss 1.3439725637435913
INFO:__main__:Epoch 199, Train Loss 1.8212183713912964
INFO:__main__:Epoch 199, Valid Loss 1.3436212539672852
INFO:__main__:R2: 0.27160802618176294
INFO:__main__:Loss: 1.0298752784729004
INFO:__main__:R2: 0.0672599154092971
INFO:__main__:Loss: 1.3188036680221558
nohup: ignoring input
INFO:__main__:name: ca9_50_init
INFO:__main__:base_fname: ca9_large_corr
INFO:__main__:seed: 4
INFO:__main__:device: cuda:1
INFO:__main__:batch_size: 2048
INFO:__main__:epochs: 100
INFO:__main__:lr: 0.0003
INFO:__main__:log_interval: 1
INFO:__main__:save_interval: 5
INFO:__main__:save_path: /data02/gtguo/DEL/data/weights/refnet_ca9_transfer/
INFO:__main__:load_path: /data02/gtguo/DEL/data/weights/refnet/
INFO:__main__:target_name: ca9
INFO:__main__:fp_size: 2048
INFO:__main__:transfer_learning: True
INFO:__main__:transfer_learning_ratio: 0.625
INFO:__main__:num_workers: 8
INFO:__main__:enc_node_feat_dim: 19
INFO:__main__:enc_edge_feat_dim: 2
INFO:__main__:enc_node_embedding_size: 64
INFO:__main__:enc_edge_embedding_size: 64
INFO:__main__:enc_n_layers: 5
INFO:__main__:enc_gat_n_heads: 4
INFO:__main__:enc_gat_ffn_ratio: 4
INFO:__main__:enc_fp_embedding_size: 32
INFO:__main__:enc_fp_ffn_size: 128
INFO:__main__:enc_fp_gated: False
INFO:__main__:enc_fp_n_heads: 4
INFO:__main__:enc_fp_size: 256
INFO:__main__:enc_fp_to_gat_feedback: add
INFO:__main__:enc_gat_to_fp_pooling: mean
INFO:__main__:dec_node_input_size: 64
INFO:__main__:dec_node_emb_size: 64
INFO:__main__:dec_fp_input_size: 32
INFO:__main__:dec_fp_emb_size: 64
INFO:__main__:dec_output_size: 2
INFO:__main__:reg_input_size: 64
INFO:__main__:reg_hidden_size: 64
INFO:__main__:reg_output_size: 1
INFO:__main__:record_path: /data02/gtguo/DEL/data/records/refnet_ca9_transfer/
INFO:__main__:Model has 408772 parameters
INFO:__main__:Head has 4225 parameters
Splitting dataset:   0%|          | 0/2372674 [00:00<?, ?it/s]Splitting dataset:   1%|          | 15614/2372674 [00:00<00:15, 156123.40it/s]Splitting dataset:   1%|▏         | 32949/2372674 [00:00<00:14, 166250.05it/s]Splitting dataset:   2%|▏         | 50706/2372674 [00:00<00:13, 171415.46it/s]Splitting dataset:   3%|▎         | 68418/2372674 [00:00<00:13, 173663.28it/s]Splitting dataset:   4%|▎         | 85785/2372674 [00:00<00:13, 173384.33it/s]Splitting dataset:   4%|▍         | 103839/2372674 [00:00<00:12, 175811.10it/s]Splitting dataset:   5%|▌         | 121421/2372674 [00:00<00:13, 173008.17it/s]Splitting dataset:   6%|▌         | 139055/2372674 [00:00<00:12, 174054.56it/s]Splitting dataset:   7%|▋         | 156991/2372674 [00:00<00:12, 175696.95it/s]Splitting dataset:   7%|▋         | 175116/2372674 [00:01<00:12, 177399.08it/s]Splitting dataset:   8%|▊         | 192862/2372674 [00:01<00:12, 174754.94it/s]Splitting dataset:   9%|▉         | 210721/2372674 [00:01<00:12, 175903.05it/s]Splitting dataset:  10%|▉         | 229061/2372674 [00:01<00:12, 178149.71it/s]Splitting dataset:  10%|█         | 247831/2372674 [00:01<00:11, 181013.06it/s]Splitting dataset:  11%|█         | 266314/2372674 [00:01<00:11, 182156.75it/s]Splitting dataset:  12%|█▏        | 284536/2372674 [00:01<00:11, 180331.57it/s]Splitting dataset:  13%|█▎        | 303696/2372674 [00:01<00:11, 183686.38it/s]Splitting dataset:  14%|█▎        | 322798/2372674 [00:01<00:11, 185872.78it/s]Splitting dataset:  14%|█▍        | 341393/2372674 [00:01<00:11, 183325.23it/s]Splitting dataset:  15%|█▌        | 359738/2372674 [00:02<00:11, 181969.91it/s]Splitting dataset:  16%|█▌        | 377945/2372674 [00:02<00:11, 181087.18it/s]Splitting dataset:  17%|█▋        | 396060/2372674 [00:02<00:11, 178591.74it/s]Splitting dataset:  17%|█▋        | 413929/2372674 [00:02<00:11, 176820.69it/s]Splitting dataset:  18%|█▊        | 431858/2372674 [00:02<00:10, 177544.69it/s]Splitting dataset:  19%|█▉        | 449619/2372674 [00:02<00:10, 177406.72it/s]Splitting dataset:  20%|█▉        | 467391/2372674 [00:02<00:10, 177496.48it/s]Splitting dataset:  21%|██        | 487011/2372674 [00:02<00:10, 183061.13it/s]Splitting dataset:  21%|██▏       | 505505/2372674 [00:02<00:10, 183617.80it/s]Splitting dataset:  22%|██▏       | 523872/2372674 [00:02<00:10, 181542.54it/s]Splitting dataset:  23%|██▎       | 542195/2372674 [00:03<00:10, 182041.27it/s]Splitting dataset:  24%|██▎       | 560868/2372674 [00:03<00:09, 183434.85it/s]Splitting dataset:  24%|██▍       | 579217/2372674 [00:03<00:09, 183074.91it/s]Splitting dataset:  25%|██▌       | 598771/2372674 [00:03<00:09, 186791.94it/s]Splitting dataset:  26%|██▌       | 617717/2372674 [00:03<00:09, 187586.06it/s]Splitting dataset:  27%|██▋       | 636480/2372674 [00:03<00:09, 183261.59it/s]Splitting dataset:  28%|██▊       | 654901/2372674 [00:03<00:09, 183537.03it/s]Splitting dataset:  28%|██▊       | 673512/2372674 [00:03<00:09, 184296.42it/s]Splitting dataset:  29%|██▉       | 691955/2372674 [00:03<00:09, 184221.40it/s]Splitting dataset:  30%|██▉       | 710649/2372674 [00:03<00:08, 185029.06it/s]Splitting dataset:  31%|███       | 729159/2372674 [00:04<00:08, 183044.76it/s]Splitting dataset:  32%|███▏      | 747473/2372674 [00:04<00:09, 176918.96it/s]Splitting dataset:  32%|███▏      | 765210/2372674 [00:04<00:09, 176043.08it/s]Splitting dataset:  33%|███▎      | 783569/2372674 [00:04<00:08, 178246.20it/s]Splitting dataset:  34%|███▍      | 801883/2372674 [00:04<00:08, 179682.20it/s]Splitting dataset:  35%|███▍      | 819873/2372674 [00:04<00:08, 175066.88it/s]Splitting dataset:  35%|███▌      | 837907/2372674 [00:04<00:08, 176602.23it/s]Splitting dataset:  36%|███▌      | 855597/2372674 [00:04<00:08, 174726.88it/s]Splitting dataset:  37%|███▋      | 873092/2372674 [00:04<00:08, 169161.13it/s]Splitting dataset:  38%|███▊      | 890053/2372674 [00:04<00:08, 168924.15it/s]Splitting dataset:  38%|███▊      | 908133/2372674 [00:05<00:08, 172383.27it/s]Splitting dataset:  39%|███▉      | 927221/2372674 [00:05<00:08, 177817.96it/s]Splitting dataset:  40%|███▉      | 945995/2372674 [00:05<00:07, 180748.47it/s]Splitting dataset:  41%|████      | 964097/2372674 [00:05<00:08, 170544.54it/s]Splitting dataset:  41%|████▏     | 983355/2372674 [00:05<00:07, 176839.53it/s]Splitting dataset:  42%|████▏     | 1002412/2372674 [00:05<00:07, 180816.31it/s]Splitting dataset:  43%|████▎     | 1020594/2372674 [00:05<00:07, 178314.06it/s]Splitting dataset:  44%|████▍     | 1038499/2372674 [00:05<00:07, 171319.88it/s]Splitting dataset:  45%|████▍     | 1057188/2372674 [00:05<00:07, 175763.28it/s]Splitting dataset:  45%|████▌     | 1077139/2372674 [00:06<00:07, 182640.75it/s]Splitting dataset:  46%|████▌     | 1096928/2372674 [00:06<00:06, 187102.97it/s]Splitting dataset:  47%|████▋     | 1115710/2372674 [00:06<00:06, 186183.52it/s]Splitting dataset:  48%|████▊     | 1134574/2372674 [00:06<00:06, 186906.14it/s]Splitting dataset:  49%|████▊     | 1154299/2372674 [00:06<00:06, 189971.74it/s]Splitting dataset:  49%|████▉     | 1173325/2372674 [00:06<00:06, 188079.69it/s]Splitting dataset:  50%|█████     | 1192157/2372674 [00:06<00:06, 185125.31it/s]Splitting dataset:  51%|█████     | 1211538/2372674 [00:06<00:06, 187673.80it/s]Splitting dataset:  52%|█████▏    | 1230328/2372674 [00:06<00:06, 187736.64it/s]Splitting dataset:  53%|█████▎    | 1249117/2372674 [00:06<00:06, 186493.99it/s]Splitting dataset:  53%|█████▎    | 1268302/2372674 [00:07<00:05, 188078.98it/s]Splitting dataset:  54%|█████▍    | 1287120/2372674 [00:07<00:05, 187185.29it/s]Splitting dataset:  55%|█████▌    | 1306517/2372674 [00:07<00:05, 189197.69it/s]Splitting dataset:  56%|█████▌    | 1325444/2372674 [00:07<00:05, 187575.46it/s]Splitting dataset:  57%|█████▋    | 1344209/2372674 [00:07<00:05, 186920.94it/s]Splitting dataset:  57%|█████▋    | 1362906/2372674 [00:07<00:05, 185781.59it/s]Splitting dataset:  58%|█████▊    | 1381489/2372674 [00:07<00:05, 185562.56it/s]Splitting dataset:  59%|█████▉    | 1400048/2372674 [00:07<00:05, 185207.07it/s]Splitting dataset:  60%|█████▉    | 1418571/2372674 [00:07<00:05, 185133.92it/s]Splitting dataset:  61%|██████    | 1437904/2372674 [00:07<00:04, 187577.13it/s]Splitting dataset:  61%|██████▏   | 1457207/2372674 [00:08<00:04, 189203.36it/s]Splitting dataset:  62%|██████▏   | 1476130/2372674 [00:08<00:04, 187759.92it/s]Splitting dataset:  63%|██████▎   | 1494938/2372674 [00:08<00:04, 187852.49it/s]Splitting dataset:  64%|██████▍   | 1514568/2372674 [00:08<00:04, 190371.46it/s]Splitting dataset:  65%|██████▍   | 1534278/2372674 [00:08<00:04, 192378.66it/s]Splitting dataset:  65%|██████▌   | 1553950/2372674 [00:08<00:04, 193673.93it/s]Splitting dataset:  66%|██████▋   | 1573883/2372674 [00:08<00:04, 195362.87it/s]Splitting dataset:  67%|██████▋   | 1593422/2372674 [00:08<00:04, 194361.68it/s]Splitting dataset:  68%|██████▊   | 1612861/2372674 [00:08<00:03, 192252.52it/s]Splitting dataset:  69%|██████▉   | 1632857/2372674 [00:08<00:03, 194536.82it/s]Splitting dataset:  70%|██████▉   | 1653503/2372674 [00:09<00:03, 198081.88it/s]Splitting dataset:  71%|███████   | 1674116/2372674 [00:09<00:03, 200478.04it/s]Splitting dataset:  71%|███████▏  | 1694171/2372674 [00:09<00:03, 199271.65it/s]Splitting dataset:  72%|███████▏  | 1714104/2372674 [00:09<00:03, 194454.30it/s]Splitting dataset:  73%|███████▎  | 1733577/2372674 [00:09<00:03, 190008.98it/s]Splitting dataset:  74%|███████▍  | 1752611/2372674 [00:09<00:03, 189722.69it/s]Splitting dataset:  75%|███████▍  | 1771606/2372674 [00:09<00:03, 188632.89it/s]Splitting dataset:  75%|███████▌  | 1790606/2372674 [00:09<00:03, 189031.55it/s]Splitting dataset:  76%|███████▋  | 1810232/2372674 [00:09<00:02, 191165.87it/s]Splitting dataset:  77%|███████▋  | 1829360/2372674 [00:09<00:02, 186315.69it/s]Splitting dataset:  78%|███████▊  | 1848023/2372674 [00:10<00:02, 180972.53it/s]Splitting dataset:  79%|███████▊  | 1866164/2372674 [00:10<00:02, 179066.72it/s]Splitting dataset:  79%|███████▉  | 1884582/2372674 [00:10<00:02, 180543.63it/s]Splitting dataset:  80%|████████  | 1902662/2372674 [00:10<00:02, 177679.45it/s]Splitting dataset:  81%|████████  | 1920452/2372674 [00:10<00:02, 176942.28it/s]Splitting dataset:  82%|████████▏ | 1938160/2372674 [00:10<00:02, 176116.60it/s]Splitting dataset:  82%|████████▏ | 1955781/2372674 [00:10<00:02, 171713.29it/s]Splitting dataset:  83%|████████▎ | 1973234/2372674 [00:10<00:02, 172529.14it/s]Splitting dataset:  84%|████████▍ | 1990762/2372674 [00:10<00:02, 173327.65it/s]Splitting dataset:  85%|████████▍ | 2009056/2372674 [00:11<00:02, 176163.42it/s]Splitting dataset:  85%|████████▌ | 2026755/2372674 [00:11<00:01, 176405.95it/s]Splitting dataset:  86%|████████▌ | 2044539/2372674 [00:11<00:01, 176827.33it/s]Splitting dataset:  87%|████████▋ | 2062862/2372674 [00:11<00:01, 178735.58it/s]Splitting dataset:  88%|████████▊ | 2080743/2372674 [00:11<00:01, 178161.39it/s]Splitting dataset:  88%|████████▊ | 2098564/2372674 [00:11<00:01, 169679.35it/s]Splitting dataset:  89%|████████▉ | 2115617/2372674 [00:11<00:01, 169623.62it/s]Splitting dataset:  90%|████████▉ | 2132639/2372674 [00:11<00:01, 168695.14it/s]Splitting dataset:  91%|█████████ | 2150183/2372674 [00:11<00:01, 170667.42it/s]Splitting dataset:  91%|█████████▏| 2168211/2372674 [00:11<00:01, 173498.96it/s]Splitting dataset:  92%|█████████▏| 2188722/2372674 [00:12<00:01, 182867.06it/s]Splitting dataset:  93%|█████████▎| 2209368/2372674 [00:12<00:00, 189883.03it/s]Splitting dataset:  94%|█████████▍| 2229752/2372674 [00:12<00:00, 194041.89it/s]Splitting dataset:  95%|█████████▍| 2250143/2372674 [00:12<00:00, 196985.32it/s]Splitting dataset:  96%|█████████▌| 2270836/2372674 [00:12<00:00, 199956.35it/s]Splitting dataset:  97%|█████████▋| 2291009/2372674 [00:12<00:00, 200483.54it/s]Splitting dataset:  97%|█████████▋| 2311067/2372674 [00:12<00:00, 196334.09it/s]Splitting dataset:  98%|█████████▊| 2330726/2372674 [00:12<00:00, 186661.79it/s]Splitting dataset:  99%|█████████▉| 2349499/2372674 [00:12<00:00, 183407.94it/s]Splitting dataset: 100%|█████████▉| 2367914/2372674 [00:12<00:00, 181288.84it/s]Splitting dataset: 100%|██████████| 2372674/2372674 [00:13<00:00, 182484.59it/s]
INFO:root:processing dataset...
  0%|          | 0/5453 [00:00<?, ?it/s]  1%|          | 45/5453 [00:00<00:12, 446.10it/s]  2%|▏         | 98/5453 [00:00<00:10, 492.34it/s]  3%|▎         | 154/5453 [00:00<00:10, 521.57it/s]  4%|▍         | 207/5453 [00:00<00:10, 504.64it/s]/data02/gtguo/DEL/pkg/utils/mol_feat.py:71: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matricesor `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484808560/work/aten/src/ATen/native/TensorShape.cpp:2981.)
  return torch.tensor(coo, dtype=torch.long).T
  5%|▍         | 268/5453 [00:00<00:09, 538.76it/s]  6%|▌         | 322/5453 [00:00<00:09, 533.32it/s]  7%|▋         | 376/5453 [00:00<00:09, 522.77it/s]  8%|▊         | 432/5453 [00:00<00:09, 533.54it/s]  9%|▉         | 486/5453 [00:00<00:09, 508.62it/s] 10%|▉         | 538/5453 [00:01<00:09, 503.45it/s] 11%|█         | 592/5453 [00:01<00:09, 512.31it/s] 12%|█▏        | 644/5453 [00:01<00:09, 493.13it/s] 13%|█▎        | 694/5453 [00:01<00:10, 467.73it/s] 14%|█▎        | 742/5453 [00:01<00:10, 470.88it/s] 14%|█▍        | 790/5453 [00:01<00:10, 461.78it/s] 15%|█▌        | 840/5453 [00:01<00:09, 470.04it/s] 16%|█▋        | 888/5453 [00:01<00:09, 466.10it/s] 17%|█▋        | 935/5453 [00:01<00:09, 457.61it/s] 18%|█▊        | 991/5453 [00:02<00:09, 486.07it/s] 19%|█▉        | 1040/5453 [00:02<00:09, 485.93it/s] 20%|██        | 1092/5453 [00:02<00:08, 494.76it/s] 21%|██        | 1145/5453 [00:02<00:08, 504.28it/s] 22%|██▏       | 1196/5453 [00:02<00:08, 493.10it/s] 23%|██▎       | 1246/5453 [00:02<00:08, 486.27it/s] 24%|██▎       | 1295/5453 [00:02<00:08, 470.38it/s] 25%|██▍       | 1343/5453 [00:02<00:08, 470.43it/s] 26%|██▌       | 1400/5453 [00:02<00:08, 496.54it/s] 27%|██▋       | 1450/5453 [00:02<00:08, 496.73it/s] 28%|██▊       | 1500/5453 [00:03<00:08, 485.49it/s] 28%|██▊       | 1551/5453 [00:03<00:07, 492.22it/s] 29%|██▉       | 1604/5453 [00:03<00:07, 502.12it/s] 30%|███       | 1655/5453 [00:03<00:07, 491.50it/s] 31%|███▏      | 1710/5453 [00:03<00:07, 508.47it/s] 32%|███▏      | 1762/5453 [00:03<00:07, 508.87it/s] 33%|███▎      | 1813/5453 [00:03<00:07, 498.28it/s] 34%|███▍      | 1863/5453 [00:03<00:07, 485.70it/s] 35%|███▌      | 1912/5453 [00:03<00:07, 448.19it/s] 36%|███▌      | 1961/5453 [00:04<00:07, 457.03it/s] 37%|███▋      | 2010/5453 [00:04<00:07, 464.80it/s] 38%|███▊      | 2059/5453 [00:04<00:07, 471.20it/s] 39%|███▊      | 2109/5453 [00:04<00:06, 478.88it/s] 40%|███▉      | 2158/5453 [00:04<00:06, 479.30it/s] 41%|████      | 2213/5453 [00:04<00:06, 498.13it/s] 42%|████▏     | 2263/5453 [00:04<00:06, 475.94it/s] 42%|████▏     | 2314/5453 [00:04<00:06, 483.58it/s] 43%|████▎     | 2363/5453 [00:04<00:06, 474.97it/s] 44%|████▍     | 2421/5453 [00:04<00:06, 501.86it/s] 45%|████▌     | 2472/5453 [00:05<00:06, 492.68it/s] 46%|████▌     | 2522/5453 [00:05<00:06, 477.06it/s] 47%|████▋     | 2570/5453 [00:05<00:06, 463.80it/s] 48%|████▊     | 2623/5453 [00:05<00:05, 481.91it/s] 49%|████▉     | 2672/5453 [00:05<00:06, 452.55it/s] 50%|████▉     | 2718/5453 [00:05<00:06, 454.17it/s] 51%|█████     | 2768/5453 [00:05<00:05, 465.78it/s] 52%|█████▏    | 2819/5453 [00:05<00:05, 478.18it/s] 53%|█████▎    | 2870/5453 [00:05<00:05, 486.90it/s] 54%|█████▎    | 2919/5453 [00:06<00:05, 465.57it/s] 54%|█████▍    | 2966/5453 [00:06<00:05, 425.41it/s] 55%|█████▌    | 3014/5453 [00:06<00:05, 438.73it/s] 56%|█████▌    | 3059/5453 [00:06<00:05, 437.54it/s] 57%|█████▋    | 3104/5453 [00:06<00:05, 439.84it/s] 58%|█████▊    | 3149/5453 [00:06<00:05, 441.60it/s] 59%|█████▊    | 3194/5453 [00:06<00:05, 443.98it/s] 59%|█████▉    | 3239/5453 [00:06<00:05, 442.18it/s] 60%|██████    | 3287/5453 [00:06<00:04, 451.23it/s] 61%|██████    | 3333/5453 [00:06<00:04, 451.42it/s] 62%|██████▏   | 3379/5453 [00:07<00:04, 448.12it/s] 63%|██████▎   | 3424/5453 [00:07<00:04, 448.02it/s] 64%|██████▎   | 3472/5453 [00:07<00:04, 457.36it/s] 65%|██████▍   | 3518/5453 [00:07<00:04, 457.54it/s] 65%|██████▌   | 3564/5453 [00:07<00:04, 454.65it/s] 66%|██████▌   | 3610/5453 [00:07<00:04, 444.97it/s] 67%|██████▋   | 3655/5453 [00:07<00:04, 423.62it/s] 68%|██████▊   | 3698/5453 [00:07<00:04, 419.08it/s] 69%|██████▊   | 3741/5453 [00:08<00:06, 284.11it/s] 69%|██████▉   | 3788/5453 [00:08<00:05, 322.97it/s] 70%|███████   | 3834/5453 [00:08<00:04, 353.73it/s] 71%|███████   | 3879/5453 [00:08<00:04, 376.99it/s] 72%|███████▏  | 3924/5453 [00:08<00:03, 395.94it/s] 73%|███████▎  | 3968/5453 [00:08<00:03, 407.49it/s] 74%|███████▎  | 4016/5453 [00:08<00:03, 425.25it/s] 74%|███████▍  | 4062/5453 [00:08<00:03, 434.03it/s] 75%|███████▌  | 4109/5453 [00:08<00:03, 441.78it/s] 76%|███████▌  | 4155/5453 [00:08<00:02, 446.12it/s] 77%|███████▋  | 4203/5453 [00:09<00:02, 453.40it/s] 78%|███████▊  | 4249/5453 [00:09<00:02, 448.51it/s] 79%|███████▉  | 4296/5453 [00:09<00:02, 451.34it/s] 80%|███████▉  | 4342/5453 [00:09<00:02, 445.21it/s] 80%|████████  | 4388/5453 [00:09<00:02, 447.15it/s] 81%|████████▏ | 4433/5453 [00:09<00:02, 423.72it/s] 82%|████████▏ | 4476/5453 [00:09<00:02, 420.75it/s] 83%|████████▎ | 4520/5453 [00:09<00:02, 425.67it/s] 84%|████████▎ | 4563/5453 [00:09<00:02, 422.97it/s] 84%|████████▍ | 4606/5453 [00:10<00:01, 424.91it/s] 85%|████████▌ | 4649/5453 [00:10<00:01, 425.35it/s] 86%|████████▌ | 4692/5453 [00:10<00:01, 414.12it/s] 87%|████████▋ | 4734/5453 [00:10<00:01, 414.69it/s] 88%|████████▊ | 4776/5453 [00:10<00:01, 412.40it/s] 88%|████████▊ | 4821/5453 [00:10<00:01, 422.82it/s] 89%|████████▉ | 4864/5453 [00:10<00:01, 423.84it/s] 90%|█████████ | 4909/5453 [00:10<00:01, 428.43it/s] 91%|█████████ | 4955/5453 [00:10<00:01, 436.09it/s] 92%|█████████▏| 5000/5453 [00:10<00:01, 438.06it/s] 93%|█████████▎| 5045/5453 [00:11<00:00, 441.07it/s] 93%|█████████▎| 5090/5453 [00:11<00:00, 443.22it/s] 94%|█████████▍| 5136/5453 [00:11<00:00, 445.81it/s] 95%|█████████▌| 5181/5453 [00:11<00:00, 444.81it/s] 96%|█████████▌| 5226/5453 [00:11<00:00, 439.88it/s] 97%|█████████▋| 5271/5453 [00:11<00:00, 436.88it/s] 98%|█████████▊| 5317/5453 [00:11<00:00, 441.35it/s] 98%|█████████▊| 5362/5453 [00:11<00:00, 442.37it/s] 99%|█████████▉| 5407/5453 [00:11<00:00, 440.47it/s]100%|█████████▉| 5452/5453 [00:12<00:00, 396.38it/s]100%|██████████| 5453/5453 [00:12<00:00, 453.61it/s]
INFO:__main__:Dataset has 5453 samples
INFO:__main__:Dataset data example: {'pyg_data': Data(x=[22, 19], edge_index=[2, 50], edge_attr=[50, 2]), 'activity': 5.028260409112222}
INFO:__main__:Train loader has 2 data
INFO:__main__:Model loaded from /data02/gtguo/DEL/data/weights/refnet/ca9_large_corr/model.pt
3408 2045
2726 682
/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([2048])) that is different to the input size (torch.Size([2048, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([678])) that is different to the input size (torch.Size([678, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([682])) that is different to the input size (torch.Size([682, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:__main__:Epoch 0, Train Loss 51.516910552978516
INFO:__main__:Epoch 0, Valid Loss 51.82040786743164
INFO:__main__:Saving model at epoch 0
INFO:__main__:Epoch 1, Train Loss 51.399654388427734
INFO:__main__:Epoch 1, Valid Loss 51.44489669799805
INFO:__main__:Epoch 2, Train Loss 50.76346969604492
INFO:__main__:Epoch 2, Valid Loss 51.18822479248047
INFO:__main__:Epoch 3, Train Loss 50.28723907470703
INFO:__main__:Epoch 3, Valid Loss 50.93914031982422
INFO:__main__:Epoch 4, Train Loss 50.692996978759766
INFO:__main__:Epoch 4, Valid Loss 50.696876525878906
INFO:__main__:Epoch 5, Train Loss 50.16653823852539
INFO:__main__:Epoch 5, Valid Loss 50.4356575012207
INFO:__main__:Saving model at epoch 5
INFO:__main__:Epoch 6, Train Loss 50.35300827026367
INFO:__main__:Epoch 6, Valid Loss 50.158992767333984
INFO:__main__:Epoch 7, Train Loss 49.52098846435547
INFO:__main__:Epoch 7, Valid Loss 49.88273620605469
INFO:__main__:Epoch 8, Train Loss 48.699485778808594
INFO:__main__:Epoch 8, Valid Loss 49.59101104736328
INFO:__main__:Epoch 9, Train Loss 48.79093551635742
INFO:__main__:Epoch 9, Valid Loss 49.2666130065918
INFO:__main__:Epoch 10, Train Loss 48.02568817138672
INFO:__main__:Epoch 10, Valid Loss 48.93211364746094
INFO:__main__:Saving model at epoch 10
INFO:__main__:Epoch 11, Train Loss 47.994834899902344
INFO:__main__:Epoch 11, Valid Loss 48.534034729003906
INFO:__main__:Epoch 12, Train Loss 47.73416519165039
INFO:__main__:Epoch 12, Valid Loss 48.09850311279297
INFO:__main__:Epoch 13, Train Loss 48.03470993041992
INFO:__main__:Epoch 13, Valid Loss 47.631256103515625
INFO:__main__:Epoch 14, Train Loss 46.658382415771484
INFO:__main__:Epoch 14, Valid Loss 47.13629150390625
INFO:__main__:Epoch 15, Train Loss 46.293052673339844
INFO:__main__:Epoch 15, Valid Loss 46.62387466430664
INFO:__main__:Saving model at epoch 15
/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch/nn/modules/loss.py:530: UserWarning: Using a target size (torch.Size([2048])) that is different to the input size (torch.Size([2047, 1])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.
  return F.mse_loss(input, target, reduction=self.reduction)
INFO:__main__:Epoch 16, Train Loss 46.121177673339844
INFO:__main__:Epoch 16, Valid Loss 46.07823181152344
INFO:__main__:Epoch 17, Train Loss 44.95750045776367
INFO:__main__:Epoch 17, Valid Loss 45.490074157714844
INFO:__main__:Epoch 18, Train Loss 44.832393646240234
INFO:__main__:Epoch 18, Valid Loss 44.87116241455078
INFO:__main__:Epoch 19, Train Loss 43.93810272216797
INFO:__main__:Epoch 19, Valid Loss 44.18783950805664
INFO:__main__:Epoch 20, Train Loss 44.22984313964844
INFO:__main__:Epoch 20, Valid Loss 43.44070053100586
INFO:__main__:Saving model at epoch 20
INFO:__main__:Epoch 21, Train Loss 42.402103424072266
INFO:__main__:Epoch 21, Valid Loss 42.63029098510742
INFO:__main__:Epoch 22, Train Loss 42.23426818847656
INFO:__main__:Epoch 22, Valid Loss 41.76247787475586
INFO:__main__:Epoch 23, Train Loss 40.50273513793945
INFO:__main__:Epoch 23, Valid Loss 40.83467483520508
INFO:__main__:Epoch 24, Train Loss 39.36924362182617
INFO:__main__:Epoch 24, Valid Loss 39.835792541503906
INFO:__main__:Epoch 25, Train Loss 39.24488067626953
INFO:__main__:Epoch 25, Valid Loss 38.7665901184082
INFO:__main__:Saving model at epoch 25
INFO:__main__:Epoch 26, Train Loss 37.499881744384766
INFO:__main__:Epoch 26, Valid Loss 37.6221809387207
INFO:__main__:Epoch 27, Train Loss 36.98170852661133
INFO:__main__:Epoch 27, Valid Loss 36.413970947265625
INFO:__main__:Epoch 28, Train Loss 34.93283462524414
INFO:__main__:Epoch 28, Valid Loss 35.11566925048828
INFO:__main__:Epoch 29, Train Loss 34.38698959350586
INFO:__main__:Epoch 29, Valid Loss 33.727787017822266
INFO:__main__:Epoch 30, Train Loss 33.39967346191406
INFO:__main__:Epoch 30, Valid Loss 32.25341796875
INFO:__main__:Saving model at epoch 30
INFO:__main__:Epoch 31, Train Loss 31.399965286254883
INFO:__main__:Epoch 31, Valid Loss 30.68242835998535
INFO:__main__:Epoch 32, Train Loss 29.543834686279297
INFO:__main__:Epoch 32, Valid Loss 29.028310775756836
INFO:__main__:Epoch 33, Train Loss 27.089982986450195
INFO:__main__:Epoch 33, Valid Loss 27.275070190429688
INFO:__main__:Epoch 34, Train Loss 25.56062126159668
INFO:__main__:Epoch 34, Valid Loss 25.439687728881836
INFO:__main__:Epoch 35, Train Loss 24.485740661621094
INFO:__main__:Epoch 35, Valid Loss 23.53091812133789
INFO:__main__:Saving model at epoch 35
INFO:__main__:Epoch 36, Train Loss 22.174592971801758
INFO:__main__:Epoch 36, Valid Loss 21.553936004638672
INFO:__main__:Epoch 37, Train Loss 20.249666213989258
INFO:__main__:Epoch 37, Valid Loss 19.521818161010742
INFO:__main__:Epoch 38, Train Loss 18.529781341552734
INFO:__main__:Epoch 38, Valid Loss 17.450138092041016
INFO:__main__:Epoch 39, Train Loss 16.191171646118164
INFO:__main__:Epoch 39, Valid Loss 15.3659086227417
INFO:__main__:Epoch 40, Train Loss 13.664529800415039
INFO:__main__:Epoch 40, Valid Loss 13.295297622680664
INFO:__main__:Saving model at epoch 40
INFO:__main__:Epoch 41, Train Loss 11.902695655822754
INFO:__main__:Epoch 41, Valid Loss 11.270598411560059
INFO:__main__:Epoch 42, Train Loss 10.28631591796875
INFO:__main__:Epoch 42, Valid Loss 9.316871643066406
INFO:__main__:Epoch 43, Train Loss 8.160938262939453
INFO:__main__:Epoch 43, Valid Loss 7.489015579223633
INFO:__main__:Epoch 44, Train Loss 6.586197853088379
INFO:__main__:Epoch 44, Valid Loss 5.837157249450684
INFO:__main__:Epoch 45, Train Loss 4.948536396026611
INFO:__main__:Epoch 45, Valid Loss 4.406455993652344
INFO:__main__:Saving model at epoch 45
INFO:__main__:Epoch 46, Train Loss 3.8446733951568604
INFO:__main__:Epoch 46, Valid Loss 3.2358529567718506
INFO:__main__:Epoch 47, Train Loss 2.9047276973724365
INFO:__main__:Epoch 47, Valid Loss 2.3495025634765625
INFO:__main__:Epoch 48, Train Loss 2.229381799697876
INFO:__main__:Epoch 48, Valid Loss 1.7558238506317139
INFO:__main__:Epoch 49, Train Loss 1.707051396369934
INFO:__main__:Epoch 49, Valid Loss 1.4356451034545898
INFO:__main__:Epoch 50, Train Loss 1.7133846282958984
INFO:__main__:Epoch 50, Valid Loss 1.3416848182678223
INFO:__main__:Saving model at epoch 50
INFO:__main__:Epoch 51, Train Loss 1.8345353603363037
INFO:__main__:Epoch 51, Valid Loss 1.4018479585647583
INFO:__main__:Epoch 52, Train Loss 1.8535809516906738
INFO:__main__:Epoch 52, Valid Loss 1.5304431915283203
INFO:__main__:Epoch 53, Train Loss 1.6148751974105835
INFO:__main__:Epoch 53, Valid Loss 1.6477911472320557
INFO:__main__:Epoch 54, Train Loss 2.1768364906311035
INFO:__main__:Epoch 54, Valid Loss 1.6949609518051147
INFO:__main__:Epoch 55, Train Loss 1.5014303922653198
INFO:__main__:Epoch 55, Valid Loss 1.634011149406433
INFO:__main__:Epoch 56, Train Loss 1.5862528085708618
INFO:__main__:Epoch 56, Valid Loss 1.4642772674560547
INFO:__main__:Epoch 57, Train Loss 1.9573513269424438
INFO:__main__:Epoch 57, Valid Loss 1.3423480987548828
INFO:__main__:Epoch 58, Train Loss 1.4250978231430054
INFO:__main__:Epoch 58, Valid Loss 1.5150965452194214
INFO:__main__:Epoch 59, Train Loss 1.5216797590255737
INFO:__main__:Epoch 59, Valid Loss 1.5044260025024414
INFO:__main__:Epoch 60, Train Loss 1.7512191534042358
INFO:__main__:Epoch 60, Valid Loss 1.3743391036987305
INFO:__main__:Epoch 61, Train Loss 1.72467839717865
INFO:__main__:Epoch 61, Valid Loss 1.3416699171066284
INFO:__main__:Epoch 62, Train Loss 1.5026379823684692
INFO:__main__:Epoch 62, Valid Loss 1.3551254272460938
INFO:__main__:Epoch 63, Train Loss 1.6395578384399414
INFO:__main__:Epoch 63, Valid Loss 1.3612011671066284
INFO:__main__:Epoch 64, Train Loss 1.5918233394622803
INFO:__main__:Epoch 64, Valid Loss 1.3525352478027344
INFO:__main__:Epoch 65, Train Loss 1.6126608848571777
INFO:__main__:Epoch 65, Valid Loss 1.3425590991973877
INFO:__main__:Epoch 66, Train Loss 1.2432061433792114
INFO:__main__:Epoch 66, Valid Loss 1.343796730041504
INFO:__main__:Epoch 67, Train Loss 1.5125120878219604
INFO:__main__:Epoch 67, Valid Loss 1.3540308475494385
/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch_geometric/data/collate.py:204: UserWarning: An output with one or more elements was resized since it had shape [17293], which does not match the required output shape [17293, 19]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484808560/work/aten/src/ATen/native/Resize.cpp:17.)
  value = torch.cat(values, dim=cat_dim or 0, out=out)
/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch_geometric/data/collate.py:204: UserWarning: An output with one or more elements was resized since it had shape [36924], which does not match the required output shape [2, 36924]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484808560/work/aten/src/ATen/native/Resize.cpp:17.)
  value = torch.cat(values, dim=cat_dim or 0, out=out)
/work01/home/gtguo/miniconda3/envs/DEL/lib/python3.10/site-packages/torch_geometric/data/collate.py:204: UserWarning: An output with one or more elements was resized since it had shape [36924], which does not match the required output shape [36924, 2]. This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  /opt/conda/conda-bld/pytorch_1659484808560/work/aten/src/ATen/native/Resize.cpp:17.)
  value = torch.cat(values, dim=cat_dim or 0, out=out)
INFO:__main__:Epoch 68, Train Loss 1.6700669527053833
INFO:__main__:Epoch 68, Valid Loss 1.3617948293685913
INFO:__main__:Epoch 69, Train Loss 1.554788589477539
INFO:__main__:Epoch 69, Valid Loss 1.3613613843917847
INFO:__main__:Epoch 70, Train Loss 1.4325226545333862
INFO:__main__:Epoch 70, Valid Loss 1.3549277782440186
INFO:__main__:Epoch 71, Train Loss 1.8332843780517578
INFO:__main__:Epoch 71, Valid Loss 1.347090482711792
INFO:__main__:Epoch 72, Train Loss 1.4987448453903198
INFO:__main__:Epoch 72, Valid Loss 1.3428856134414673
INFO:__main__:Epoch 73, Train Loss 1.166521668434143
INFO:__main__:Epoch 73, Valid Loss 1.3415312767028809
INFO:__main__:Epoch 74, Train Loss 1.3642934560775757
INFO:__main__:Epoch 74, Valid Loss 1.3414301872253418
INFO:__main__:Epoch 75, Train Loss 1.6691337823867798
INFO:__main__:Epoch 75, Valid Loss 1.3414438962936401
INFO:__main__:Epoch 76, Train Loss 1.7511727809906006
INFO:__main__:Epoch 76, Valid Loss 1.341406226158142
INFO:__main__:Epoch 77, Train Loss 1.4915378093719482
INFO:__main__:Epoch 77, Valid Loss 1.342434048652649
INFO:__main__:Epoch 78, Train Loss 1.5473295450210571
INFO:__main__:Epoch 78, Valid Loss 1.3456096649169922
INFO:__main__:Epoch 79, Train Loss 1.6009525060653687
INFO:__main__:Epoch 79, Valid Loss 1.3493183851242065
INFO:__main__:Epoch 80, Train Loss 1.7289994955062866
INFO:__main__:Epoch 80, Valid Loss 1.3515207767486572
INFO:__main__:Epoch 81, Train Loss 1.5822303295135498
INFO:__main__:Epoch 81, Valid Loss 1.3512463569641113
INFO:__main__:Epoch 82, Train Loss 1.4323809146881104
INFO:__main__:Epoch 82, Valid Loss 1.3488261699676514
INFO:__main__:Epoch 83, Train Loss 1.5707385540008545
INFO:__main__:Epoch 83, Valid Loss 1.3451647758483887
INFO:__main__:Epoch 84, Train Loss 1.387297511100769
INFO:__main__:Epoch 84, Valid Loss 1.3426134586334229
INFO:__main__:Epoch 85, Train Loss 1.4194419384002686
INFO:__main__:Epoch 85, Valid Loss 1.341590404510498
INFO:__main__:Epoch 86, Train Loss 1.5595958232879639
INFO:__main__:Epoch 86, Valid Loss 1.3413679599761963
INFO:__main__:Epoch 87, Train Loss 1.6583008766174316
INFO:__main__:Epoch 87, Valid Loss 1.3415024280548096
INFO:__main__:Epoch 88, Train Loss 1.6496278047561646
INFO:__main__:Epoch 88, Valid Loss 1.342422604560852
INFO:__main__:Epoch 89, Train Loss 1.4429993629455566
INFO:__main__:Epoch 89, Valid Loss 1.3443869352340698
INFO:__main__:Epoch 90, Train Loss 1.6075279712677002
INFO:__main__:Epoch 90, Valid Loss 1.3463294506072998
INFO:__main__:Epoch 91, Train Loss 1.358280062675476
INFO:__main__:Epoch 91, Valid Loss 1.347801685333252
INFO:__main__:Epoch 92, Train Loss 1.63853919506073
INFO:__main__:Epoch 92, Valid Loss 1.3477072715759277
INFO:__main__:Epoch 93, Train Loss 1.4168128967285156
INFO:__main__:Epoch 93, Valid Loss 1.346055030822754
INFO:__main__:Epoch 94, Train Loss 1.6668791770935059
INFO:__main__:Epoch 94, Valid Loss 1.3442823886871338
INFO:__main__:Epoch 95, Train Loss 1.7056440114974976
INFO:__main__:Epoch 95, Valid Loss 1.3432461023330688
INFO:__main__:Epoch 96, Train Loss 1.2736977338790894
INFO:__main__:Epoch 96, Valid Loss 1.342905044555664
INFO:__main__:Epoch 97, Train Loss 1.5233436822891235
INFO:__main__:Epoch 97, Valid Loss 1.3426553010940552
INFO:__main__:Epoch 98, Train Loss 1.652417540550232
INFO:__main__:Epoch 98, Valid Loss 1.3429476022720337
INFO:__main__:Epoch 99, Train Loss 1.478333830833435
INFO:__main__:Epoch 99, Valid Loss 1.3437925577163696
INFO:__main__:R2: 0.2528842265494773
INFO:__main__:Loss: 1.0563489198684692
INFO:__main__:R2: 0.06731122719256333
INFO:__main__:Loss: 1.3187311887741089
